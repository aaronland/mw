<html><!-- InstanceBegin template="/Templates/mw2004-papers.dwt" codeOutsideHTMLIsLocked="false" --><!-- Mirrored from www.museumsandtheweb.com/mw2004/papers/wakkary/wakkary.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:44:39 GMT --><head>
<!-- #BeginEditable "doctitle" -->
<title>Museums and the Web 2004 : Papers : Wakkary et al., Interactive Audio Content ...</title>
<!-- #EndEditable --> 
<meta name="keywords" content="Museums and the Web 2004, Archives &amp; Museum Informatics, museums online, on-line, cultural heritage online, museum digitization, internet, conference, symposium, workshop, meeting, international, papers, presentations, multimedia, interactive, education, exhibits, evaluation, virtual reality, digitization, information architecture, information design, interface design, digital library, digital libraries "/>
<!-- #BeginEditable "page keywords" -->
<meta name="keywords" content="augmented reality, audio, adaptive information retrieval, museum guide"/>
<!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset="/>
<!-- #BeginEditable "script" --><!-- #EndEditable --> 
<link rel="stylesheet" href="../../../mw2003/Library/mw2003.css" type="text/css"/>
<script language="JavaScript">
<!--
function MM_swapImgRestore() { //v3.0
  var i,x,a=document.MM_sr; for(i=0;a&&i<a.length&&(x=a[i])&&x.oSrc;i++) x.src=x.oSrc;
}

function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.0
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && document.getElementById) x=document.getElementById(n); return x;
}

function MM_swapImage() { //v3.0
  var i,j=0,x,a=MM_swapImage.arguments; document.MM_sr=new Array; for(i=0;i<(a.length-2);i+=3)
   if ((x=MM_findObj(a[i]))!=null){document.MM_sr[j++]=x; if(!x.oSrc) x.oSrc=x.src; x.src=a[i+2];}
}
//-->
</script>
</head>

<body bgcolor="#FFFFFF" background="../../../mw2002/images/mw2002.bg.gif" text="#000000" link="#660099" vlink="#000066" onload="MM_preloadImages(&#39;../../images/nav/register_on.gif&#39;,&#39;../../images/nav/workshops_on.gif&#39;,&#39;../../images/nav/sessions_on.gif&#39;,&#39;../../images/nav/speakers_on.gif&#39;,&#39;../../images/nav/interactions_on.gif&#39;,&#39;../../images/nav/demos_on.gif&#39;,&#39;../../images/nav/exhibits_on.gif&#39;,&#39;../../images/nav/events_on.gif&#39;,&#39;../../images/nav/best_on.gif&#39;,&#39;../../images/nav/dates_on.gif&#39;,&#39;../../images/nav/local_on.gif&#39;,&#39;../../images/nav/sponsors_on.gif&#39;)">
<table width="600" border="0" cellspacing="2" cellpadding="5">
  <tbody><tr> 
    <td width="145" align="LEFT" valign="TOP"> 
      <p><a href="../../index.html"><img src="../../images/mw.gif" width="112" height="155" border="0" alt="/mw/"/></a></p>
      <p> <a href="../../register/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;register&#39;,&#39;&#39;,&#39;../../images/nav/register_on.gif&#39;,1)"><img name="register" border="0" src="../../images/nav/register_off.gif" width="112" height="18"/></a><br/>
        <a href="../../workshops/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;workshops&#39;,&#39;&#39;,&#39;../../images/nav/workshops_on.gif&#39;,1)"><img name="workshops" border="0" src="../../images/nav/workshops_off.gif" width="112" height="18"/></a><br/>
        <a href="../../sessions/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;sessions&#39;,&#39;&#39;,&#39;../../images/nav/sessions_on.gif&#39;,1)"><img name="sessions" border="0" src="../../images/nav/sessions_off.gif" width="112" height="18"/></a><br/>
        <a href="../../speakers/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;speakers&#39;,&#39;&#39;,&#39;../../images/nav/speakers_on.gif&#39;,1)"><img name="speakers" border="0" src="../../images/nav/speakers_off.gif" width="112" height="18"/></a><br/>
        <a href="../../interact/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;interactions&#39;,&#39;&#39;,&#39;../../images/nav/interactions_on.gif&#39;,1)"><img name="interactions" border="0" src="../../images/nav/interactions_off.gif" width="112" height="18"/></a><br/>
        <a href="../../demos/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;demonstrations&#39;,&#39;&#39;,&#39;../../images/nav/demos_on.gif&#39;,1)"><img name="demonstrations" border="0" src="../../images/nav/demos_off.gif" width="112" height="18"/></a><br/>
        <a href="../../exhibit/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;exhibits&#39;,&#39;&#39;,&#39;../../images/nav/exhibits_on.gif&#39;,1)"><img name="exhibits" border="0" src="../../images/nav/exhibits_off.gif" width="112" height="18"/></a><br/>
        <a href="../../events/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;events&#39;,&#39;&#39;,&#39;../../images/nav/events_on.gif&#39;,1)"><img name="events" border="0" src="../../images/nav/events_off.gif" width="112" height="18"/></a><br/>
        <a href="../../best/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;best&#39;,&#39;&#39;,&#39;../../images/nav/best_on.gif&#39;,1)"><img name="best" border="0" src="../../images/nav/best_off.gif" width="112" height="18"/></a><br/>
        <a href="../../dates/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;dates&#39;,&#39;&#39;,&#39;../../images/nav/dates_on.gif&#39;,1)"><img name="dates" border="0" src="../../images/nav/dates_off.gif" width="112" height="18"/></a><br/>
        <a href="../../local/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;local&#39;,&#39;&#39;,&#39;../../images/nav/local_on.gif&#39;,1)"><img name="local" border="0" src="../../images/nav/local_off.gif" width="112" height="18"/></a><br/>
        <a href="../../sponsor/index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage(&#39;sponsors&#39;,&#39;&#39;,&#39;../../images/nav/sponsors_on.gif&#39;,1)"><img name="sponsors" border="0" src="../../images/nav/sponsors_off.gif" width="112" height="18"/></a> 
        <br/>
        <br/>
        <a href="http://www.archimuse.com/" target="_top"><img src="../../images/nav/nav_ami.gif" width="135" height="35" border="0" alt="A&amp;MI home"/></a> 
        <br/>
      </p>
      <table width="81%" border="0" cellspacing="0" cellpadding="0">
        <tbody><tr> 
          <td align="left" valign="top"> 
            <p align="right"><span class="verysmall">158 Lee Avenue<br/>
              Toronto Ontario<br/>
              M4E 2P3 Canada</span></p>
            <p class="verysmall" align="right">ph: +1 416-691-2516<br/>
              fx: +1 416-352-6025</p>
            <p align="right"><span class="verysmall">info @ archimuse.com<br/>
              <a href="http://www.archimuse.com/">www.archimuse.com</a></span></p>
            <div align="right"> 
              <table width="74">
                <tbody><tr> 
                  <td> <a href="http://search.museumsandtheweb.com/search" target="_top"> 
                    <img src="../../images/search.gif" width="24" height="25" alt="Search" border="0" name="Search"/></a> 
                  </td>
                  <td valign="MIDDLE"> <a href="http://search.museumsandtheweb.com/search"> 
                    <span class="verysmall">Search<br/>
                    A&amp;MI</span></a> </td>
                </tr>
              </tbody></table>
            </div>
            <p align="right"><font face="Arial, Helvetica, sans-serif" class="verysmall"><span class="verysmall">Join 
              our <a href="http://search.museumsandtheweb.com/mailinglist/"> Mailing 
              List</a>. <br/>
              <a href="http://search.museumsandtheweb.com/terms-of-use-privacy/"> Privacy</a>.</span></font></p>
            <p align="right"> </p>
            <p align="right">  </p>
            <p align="right"><span class="verysmall">published: March 2004<br/>
analytic scripts updated:  
              <!-- #BeginDate format:Am1 -->October 28, 2010<!-- #EndDate -->
              </span></p>
            <p align="right"><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img src="http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0  License" width="88" height="31" style="border-width:0"/></a> </p>
          </td>
        </tr>
      </tbody></table>
      <p>  </p>
    </td>
    <td width="455" align="LEFT" valign="TOP" class="normal"><a href="../../speakers/index.html"><img src="../../images/papers.gif" width="390" height="55" border="0" alt="Museums and the Web 2003 Papers"/></a> 
      <br/>
      <!-- #BeginEditable "Body of Page" -->
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252"/>
  <p class="PaperTitle">Interactive Audio Content: An Approach to Audio Content 
      for a Dynamic Museum Experience through Augmented Audio Reality and Adaptive 
      Information Retrieval</p>
  <p class="Author">Ron Wakkary, Kenneth Newby, Marek Hatala, Dale 
      Evernden and Milena Droumeva, Simon Fraser University, 
      Canada</p> <p class="AbstractTitle">Abstract</p>
   <p class="AbstractText">ec(h)o is an audio augmented reality interface utilizing 
      spatialized soundscapes and a semantic Web approach to information. The paper 
      discusses our approach to conceptualizing museum content and its creation 
      as audio objects in order to satisfy the requirements of the ec(h)o system. 
      This includes the conceptualizing of information relevant to an existing exhibition 
      design (an exhibition from the Canadian Museum of Nature in Ottawa). We discuss 
      the process of acquiring, designing and developing information relevant to 
      the exhibition and its mapping to the requirements of adaptive information 
      retrieval and the interaction model. The development of the audio objects 
      is based on an audio display model that addresses issues of psychoacoustics, 
      composition and cognition. The paper outlines the challenges and identifies 
      the limitations of our approach.</p>
  <p class="keywords">Keywords: augmented reality, audio, adaptive 
      information retrieval, museum guide</p>
  <h1>1. Introduction</h1>
  <p>Museums are natural laboratories for examining the complex nature of constructing 
      meaning - learning from and enjoying objects and environments through interaction. 
      Museum visits have been described as interactive, situational, social, subjective 
      and inter-connected with the physical environment (Leinhardt and Crowley 1998; 
      Lehn, Heath et al. 2001). Research and commercial practice in the development 
      of electronic museum guides have typically focused on the use of portable 
      computing devices for interaction, data storage and audio delivery (Proctor 
      and Tellis 2003). The growth of such systems continues despite the known 
      limits to this approach. These include the cognitive and learning difficulties 
      of using a new graphical interface, competition for attention between the 
      device and its surroundings, and the ergonomic problems of weight and operation 
      (Woodruff, Aoki et al. 2001; Proctor and Tellis 2003). Often portable computing 
      based systems deliver content in ways familiar to computing but not familiar 
      to museum visitors (Leinhardt and Crowley 1998; Lehn, Heath et al. 2001; Woodruff, 
      Szymanski et al. 2001). An arguably more important limitation in current practice 
      is the approach to digital content. Typically content for museum guides has 
      been developed much like CD-ROM content, interactive but finite and limited 
      structurally in terms of associations and linkages.</p>
  <p>Our goal is to design a system that fits with the interactions and everyday 
      competencies of the museum visitor, such that it amplifies and strengthens 
      the visitor&#39;s ability to explore, learn from and construct the meaning of 
      exhibitions.</p>
  <p>The paper discusses our approach to conceptualizing the content and its creation 
      as audio objects in order to satisfy the requirements of ec(h)o. This includes 
      the conceptualizing of information relevant to an existing exhibition design 
      (an exhibition from the Canadian Museum of Nature in Ottawa). First, we provide 
      an overview of the ec(h)o system and interaction model, followed by a discussion 
      of challenges in relation to adaptive information retrieval and interactive 
      audio. Next, we discuss the process of acquiring, designing and developing 
      information relevant to the exhibition and its mapping to the requirements 
      of adaptive information retrieval and the interaction model. The development 
      of audio objects is based on an audio display model that addresses issues 
      of psychoacoustics, composition and cognition. The paper outlines the challenges 
      and identifies the limitations of our approach.</p>
  <h1>2. Context</h1>
  <h2>2.1 Overview of ec(h)o</h2>
  <p>The platform for ec(h)o is an integrated audio, vision and location tracking 
      system installed as an augmentation of an existing exhibition installation. 
      The platform is designed to create a museum experience that consists of a 
      physical installation and an interactive layer of three-dimensional <i>soundscapes</i> 
      that are physically mapped to museum displays and the overall exhibition installation. 
  </p>
  <p>Each soundscape consists of zones of ambient sound and <i>soundmarks</i> 
      generated by dynamic audio data that relates to the artifacts the visitor 
      is experiencing. The soundscapes change based on the position of the visitors 
      in the space, their past history with viewing the artifacts, and their individual 
      interests in relation to the museum collection. By way of a gesture-based 
      interaction, visitors can interact with a single artifact or multiple artifacts 
      in order to listen to related audio information. The audio delivery is dynamic 
      and generated by agent-assisted searches inferred by past interactions, histories 
      and individual interests. </p>
  <p>The source for the audio-data is digital objects. Our original sample set 
      of digital objects was developed using content that originated from our partner 
      museum, the Canadian Museum of Nature. In the ec(h)o context, digital objects 
      populate a network of repositories linked across different museums. The networked 
      nature of these repositories makes it possible for visitors in the context 
      of one museum to access data from another. For example, a visitor at the Canadian 
      Museum of Nature can access content from the local repository as well as repositories 
      of other museums or on-line resources. </p>
  <p>The ec(h)o architecture consists of four main system components: <b>position 
      tracking</b>, <b>vision system</b>, <b>wireless audio delivery</b>, and <b>reasoning</b>. 
      Two main types of events trigger the communication between the components: 
      a user&#39;s movement through the exhibition space, and a user&#39;s explicit selection 
      of the sound objects. A more detailed description and analysis of the technical 
      and information retrieval aspects can be found in our previous writing (Hatala, 
      Kalantari et al.).</p>
  <h2>2.2 Interaction Model</h2>
  <h3>2.2.1 Conversation structure</h3>
  <p>Similar to the work of Woodruff, Aoki, Hurst and Szymanski (Woodruff, Aoki 
      et al. 2001; Woodruff, Szymanski et al. 2001; Aoki, Grinter et al. 2002), 
      we have adopted the storytelling structure based on Sacks&#39; conversation analysis 
      theory (Sacks 1974). In our case, we modeled the system and interaction on 
      this conversation structure. ec(h)o offers the visitor three short audio pieces 
      that we refer to as <i>audible icons</i>. These audible icons serve as <i>prefaces</i>. 
      They are in effect offering three turn-taking possibilities to the visitor. 
      The visitor selects one and the system delivers the related audio object. 
      This turn-taking represents the <i>telling</i> phase. After the delivery of 
      the object, the system again offers three audible icons. It is at this stage 
      that the <i>response</i> phase occurs. The visitor&#39;s <i>response</i> is expressed 
      through the gesture selection with the interaction object. Additionally, the 
      system may be met by no response, because the visitor does not wish to engage 
      the system. It will then enter into a silent mode. The visitor may also have 
      moved away, and the system will initiate a soundscape and prepare for the 
      next conversational encounter. </p>
  <h3>2.2.2 Navigational model </h3>
  <p>It is important to explain the navigational model, both for its novelty and 
      simplicity, and of course its support of the interaction. The audio objects 
      are semantically tagged to a range of topics. At the beginning of each interaction 
      cycle or &#34;conversation&#34;, three topics are inferred to be more relevant than 
      others to the visitor based on the user model, location and interaction history. 
      Audio objects are cued representing each of the three chosen topics. Audible 
      icons or <i>prefaces</i> related to the objects are presented to the visitor 
      (each audible icon is differently spatialized in the audio display for differentiation). 
      The visitor chooses one of the <i>prefaces</i> and listens to an object representative 
      of the topic chosen. The topics are not explicit to the visitor; rather, the 
      consistency and content logic is kept in the background. After listening to 
      the object, the visitor is offered a new <i>preface </i>based on the previous 
      topic selection. The two previous <i>prefaces</i> that were not selected are 
      offered once again. If three offerings of the same <i>preface </i>and topic 
      have transpired without selection, that topic is replaced. A more detailed 
      description and analysis of the interaction model and design process can be 
      found in a previous writing (Wakkary, Hatala et al.).</p>
  <p class="caption"><img src="wakkary_figure_1.gif" alt="Navigational Model" width="380" height="232"/></p>
  <p class="caption">Figure 1. 1-2-4 navigation model</p>
  <h1>3. Challenges</h1>
  <p>In related works (Bederson 1995; Sarini and Strapparava 1998; Andolesk and 
      Freedman. 2001), the relationship of the digital content to the artifacts 
      is either pre-planned and fixed, or the digital content is not networked and 
      is limited to the local device; in some cases both limits are true. ec(h)o 
      employs a semantic Web approach to the museum&#39;s digital content; thus it is 
      networked, dynamic and user-driven. The interface of ec(h)o does not rely 
      on portable computing devices, rather it utilizes a combination of gesture 
      and object manipulation recognized by a vision system. </p>
  <p>The dynamic and user-driven nature of ec(h)o requires a highly responsive 
      retrieval mechanism with criteria defined by psychoacoustics, content, and 
      composition domains. The retrieval mechanism is based on a user model that 
      is continually updated as the user moves through the exhibition and listens 
      to the audio objects. The criteria are represented by rules operating on the 
      ontological descriptions of sound objects, museum artifacts and user interests. 
  </p>
  <p>Capturing user interests is at the center of the research of several disciplines 
      such as information retrieval, information filtering and user modeling (Wahlster 
      and Kobsa 1989). Most of the systems were developed for retrieval of documents 
      where document content is analyzed and explicit user feedback is solicited 
      to learn or infer the user interests. In the context of ec(h)o there is no 
      direct feedback from the user. ec(h)o can be categorized as a personalized 
      system as it observes the user&#39;s behavior and makes generalizations and predictions 
      about the individual user based on those interactions (Goecks and Shavlik 
      2000; Kobsa and Fink 2002).</p>
  <p>Particular challenges in relation to the use of audio in ec(h)o include: 
      the designing and preparing of the audio objects for dynamic and personal 
      delivery; the information management aspects of developing classifications 
      and relationships; and the ultimate need to create an audio display and user 
      experience that is coherent, consistent and pleasurably exploratory in relation 
      to an existing exhibition. The following section focuses on how we addressed 
      these challenges.</p>
  <h1>4. Audio Content Design and Development </h1>
  <p>The design and production of the audio content is covered in four stages: 
  </p>
  <ol>
    <li>Our expert-based system approach to data collection, describing how 
          we acquired the raw information related to the exhibition and artifacts; </li>
    <li>Concept mapping and audio object design, discussing the initial knowledge 
          management design of the information and the design and development of objects; </li>
    <li>Design of audio objects, describing the audio display and acoustical 
          experience issues related to the objects as audio; </li>
    <li>User scenarios and inference rules, discussing our development of user 
          scenarios as a design approach to developing the inference rules. This set 
          of descriptions outlines the entire process of the design and development 
          of the audio objects, the ontologies, and inference rules.</li>
  </ol>
  <h2>4.1 Stage One - Expert Based System Approach for Data Collection</h2>
  <p>In order to acquire the relevant information 
      for the audio objects we devised two modes of interviews with researchers 
      at the Nature Museum of Canada. The interview sessions took place in the museum 
      over the course of several days. Our goal was to develop an information gathering 
      process that paralleled our conversation approach to the interaction model. 
      The aim was that the interview processes would provide us with audio material 
      that could be used directly and would create the experience for the museum 
      visitor of an interactive guide to the museum with a group of different experts 
      (in the end we used the interview texts to create a script, and so we did 
      not use the recordings directly). In keeping with the our conversation model, 
      we hoped to emulate the experience of experts conversing (both with themselves 
      and the visitor), each taking turns contributing bits of information based 
      on their particular interests and area of expertise.</p>
  <p>We organized interviews with members of the museum 
      research staff. These individuals were chosen based on their expertise in 
      a number of different knowledge domains related to the exhibition: Zoology, 
      Ichthyology, Botany, Vascular Plants, Invertebrates, conservation, etc. The 
      interviews were conducted in two parts: part one introduced the interviewee 
      to the ec(h)o project and asked each to comment or provide contextual information 
      from a perspective and area of expertise related to the exhibit; phase two 
      involved a video walk-through of the exhibit space in which the interviewer 
      and expert engaged in a discussion of the artifacts and collections on display. 
      Here interviewees were asked to provide discipline-specific information about 
      the exhibit&#39;s themes and sub-themes, as well as relevant information about 
      specific artifacts within each of the exhibits. Here is a sample set of questions: 
  </p>
  <ul>
    <li>Each display tells a story. What is that story? </li>
    <li>Can you discuss the different groupings of the artifacts and 
          explain how and why they are clustered? </li>
    <li>Can you describe the significance of each artifact or group 
          of artifacts? </li>
    <li>What makes these particular artifacts best suited to their tasks? </li>
    <li>Can you describe the type of sounds that you think would supplement 
          this exhibit?</li>
    <li>How might these sound effects work to enhance visitor experience? 
    </li>
    <li>Can you speak to the potential of linking content to other museums?</li>
  </ul>
  <p>The results of the interviews were largely successful; 
      however, there were problems and gaps in our information set. Some interviewees 
      limited their discussion to very high-level explanations of the exhibit that 
      were difficult to integrate into a museum visit, while others provided interesting 
      anecdotal information about artifacts. While we wanted to avoid an encyclopedia 
      approach to the information, we supplemented the interview information with 
      research from the museum&#39;s archives and research collections. We met with 
      archival experts to filter potential source material that already existed. 
      Source material was, for the most part, limited to audio tracks taken from 
      studies conducted in the field, as well as video productions that the museum 
      had collected or produced over the years. </p>
  <h2>4.2 Stage Two - Concept Mapping &amp; Ontologies</h2>
  <h3>4.2.1 Concept map development</h3>
  <p>In order to translate the information gathered in the interview process for 
      adaptive retrieval, we needed to conceptualize the information within a loose 
      taxonomy or concept map that could eventually be developed into semantic Web 
      ontologies. The concept map would guide us in the design and relationships 
      of the information in the form of digital objects. As part of the information 
      management related research, the strength of a semantic Web approach is the 
      interoperability of generic and specific topic ontologies. We wanted to test 
      the ability to develop specific ontologies that could function with generic 
      ontologies. In addition, we were very aware that the existing curatorship 
      and exhibition design represented a knowledge map in its own right, relevant 
      to the objects and collections on display; nevertheless our goal was to insert 
      another level of knowledge mapping that could be productively superimposed 
      on the existing exhibition. </p>
  <p>In order to develop the concept map for the ec(h)o version of the exhibition, 
      we analysed the recorded video and audio from the expert interviews. This 
      analysis entailed watching and listening to video and audio, followed by a 
      mapping process. This was undertaken by the entire interdisciplinary research 
      team to ensure that the design of the concept map could function in the different 
      contexts of adaptive retrieval, audio display and user experience. The concepts 
      and themes that the team clustered were organized into a relational map. 
      These concepts and themes became classifiers used during the meta-tagging 
      stage of audio object development. Conceptual and thematic classifiers evolved 
      out of the concept mapping exercise, whereas the topical classifiers were 
      taken from the established Dewey Decimal Classification system. The conceptual 
      map served as an important visualization tool that helped the team understand 
      the topical and conceptual links between artifacts and exhibit sections. 
      The map also served as a point of departure for helping the team recognize 
      potential openings for bringing in content from other museums. The concept 
      map was the starting point for the development and adoption of different ontologies. 
  </p>
  <p class="caption"><a href="wakkaryFig2.html"><img src="wakkary_figure_2.w.jpg" alt="Preliminary Concept mapping" width="400" height="320" border="0"/></a></p>
  <p class="caption"><a name="Fig2"></a>Figure 2. Preliminary concept mapping</p>
  <h3>4.2.2 Ontologies</h3>
  <p>The interaction model is based on the semantic description of the content 
      of the objects. We have developed an ontology where a sound object is described 
      using several properties. As an ability to link to other museums is an important 
      feature of ec(h)o, our ontology builds significantly on the standard Conceptual 
      Reference Model (CRM) for heritage content developed by CIDOC (Crofts, Dionissiadou 
      et al. 2002). The CRM provides definitions and a formal structure for describing 
      the implicit and explicit concepts and relationships used in cultural heritage 
      documentation. To describe sound objects we use CRM Temporal Entity concept for modeling 
      periods and events and Place 
      for modeling locations. We describe museum artifacts using the full CRM model. 
  </p>
  <p>The content of the sound object is not described directly, but annotated 
      with three entities: concepts, topics, and themes. The concepts describe the 
      domains that are expressed by the sound object, such as evolution, behaviour, 
      lifestyle, diversity, habitat, etc. Since the collections in individual museums 
      are different, so are the concept maps describing these collections. A topic 
      is a more abstract entity that is represented by several concepts, such as 
      botany, invertebrates, marine biology, etc. To facilitate the mappings between 
      topic ontologies in individual museums, we have mapped the topics to the Dewey 
      Decimal Classification whenever possible. Finally, themes are defined as entities 
      supported by one or more topics, e.g. the theme of bigness: in invertebrates 
      and marine biology. </p>
  <h2>4.3 Stage Three - Audio Object Design</h2>
  <p>In this stage, the aim is to design and develop the audio objects that support the interaction and audio display model, and that can be classified and 
      meta-tagged based on the concept mapping. In support of the interaction model, 
      the audio objects need to be different types of audio elements - prefaces, 
      audio objects, sound-marks, and keynote sounds. In the early stages of this 
      work we focused on developing audio objects and their corresponding prefaces 
      - sound marks and keynote sound production came later. The production of 
      the audio objects started with dividing up the interviews into manageable 
      information objects. In doing so it was clear that each object needed to be 
      cognitively manageable for the user, as well as manageable for the system, 
      meaning it needed to be classifiable. Embedded references to artifacts were 
      either made explicit or removed all together, and the scripts for all objects 
      were edited to be suitable, as well as interesting, to as broad an audience 
      as possible. </p>
  <p>Once refined, each discrete audio object was then entered into the repository 
      database where each was meta-tagged for retrieval purposes. Meta-tagged 
      information includes location and associative information such as the exhibit 
      an object belonged to, as well as the specific artifact it was most relevant 
      to. Objects were also meta-tagged based on their topical, conceptual, and 
      thematic qualities. For example, if we were classifying an object that spoke 
      about the collecting tools used by early plant collector Catherine Parr Trail, 
      its topical classifier would be <i>botany</i>, its conceptual classifier <i>tools 
      and techniques</i>, and its thematic classifier, <i>early collectors</i>. 
  </p>
  <h3>4.3.1 Audio Display</h3>
  <p>In order to deliver a seamless integrated audio display experience, ec(h)o 
      works on several levels. The first mode of interaction involves movement-related 
      immersion in a dynamic soundscape, related thematically to different parts 
      of the exhibit. A second mode of interaction involves the visitors engaging 
      with the audio display installation via a manipulation object, responding 
      to spatially displayed audio prefaces. A third level is knowledge acquisition 
      or learning by listening to the audio knowledge objects. It is important that 
      all levels work together, physically, cognitively and psychoacoustically in 
      order to deliver a worthwhile immersive experience. Issues of sound amplitude 
      and frequency range must be considered for all elements of the audio display 
      system.</p>
  <p>In addition, we felt it was important to provide the visitor a variety of 
      voices with a spectrum spanning the serious and authoritative to the playful 
      and whimsical. Before recording the audio objects, consideration was given 
      to choosing the voices to perform the scripted content. Issues of gender, 
      voice quality, timbre, clarity and other psychoacoustic sound markers came 
      into play. For ec(h)o, an even gender split between the voices is used with 
      care in order to develop differences in both timbre and performance and to 
      facilitate easy discrimination due to variations in range, frequency and timbre. 
      The voices consist of one deeper, broad-range strong male voice, one warmer-timbre, 
      softer male voice, one mid-high pure female voice, and one deeper, richer-timbre 
      female voice. For an initial database of just over 200 short sound objects, 
      four different voices (two male and two female) appeared to be sufficient 
      to provide the diverse, yet consistent and recognizable audio Web of information. 
  </p>
  <p>In order to create an atmosphere of engaged and fun learning, the aural design 
      attempts to stay away from a highly accented authoritative presentation of 
      museum information. For this reason, voice talents used are not professionals, 
      but real people. The style of narration determined during recording is natural 
      pace, moderate inflection, with an even dynamic speech envelope, in distinction 
      to the emphasized polished performance typical of professional voice talents.</p>
  <p>Preliminary testing of different approaches in the presentation of informational 
      audio options - options that are effective in pointing to thematically or 
      conceptually different information objects - suggests a conversational approach 
      is appropriate to maintaining a level of playful engagement and dialogue with 
      ec(h)o. Since this approach is based on a style of presenting artifact information 
      that has a teasing, humorous quality, the vocal approach taken is appropriately 
      different. Of the four voice talents delivering the audio objects, two are 
      used to present the prefaces - one male and one female. Again, the objective 
      is to have a natural, spontaneous voice, but with greater emphasis on character 
      - a more upbeat accent and inflection. This enhances the immediate playful 
      engagement of the museum visitor and, as a consequence, successfully provokes 
      greater interest in selecting a particular audio object.</p>
  <h3>4.3.2 Audio Production</h3>
  <p>Once the scripted objects are transferred to audio, the files are compressed 
      in high-resolution mp3 format for the purpose of quick retrieval over an Internet 
      connection. Given the slight loss of fidelity due to this compression technique, 
      it is essential that the source recordings be clear and of optimal amplitude 
      from the outset, in order to be clearly heard through the transmitted wireless 
      audio format.</p>
  <p>Another important production process to be considered is the storing and 
      categorizing of the database of audio objects as a basis for a cross-institutional 
      adaptive information retrieval and interaction model system. One option for 
      a naming convention involves using a semantic signifier in combination with 
      a numerical index: [botany]_00001.mp3, where this signifier could be derived 
      from any of the subject tags applied to each individual record. The final 
      ec(h)o audio object database design omits this signifier due to possible future 
      inconsistencies with the collections of other museums that might wish to participate 
      in the development and sharing of knowledge object repositories. </p>
  <h2>4.4 Stage Four - Inference Rules</h2>
  <p>In order to develop the inference rules we developed three models to conceptualize 
      and test the rules: 1) visitor model; 2) narrative model; 3) soundscape model. 
      In addition to the content and content mapping process outlined above, we 
      relied on our initial observational and site studies of museums and museum 
      visitors, discussions with museum administrators, exhibition designers and 
      curators, and the research literature in museum studies (Lehn, Heath et al. 
      2001; Sparacino 2002).</p>
  <h3>4.4.1 Visitor models</h3>
  <p>Our visitor model is comprised of three classifications of users:</p>
  <ul>
    <li> A <i>busy</i> visitor does not want to spend much time in each 
          exhibit. Instead, this user wants to stroll through the museum to get a general 
          idea;</li>
    <li> An <i>avaricious </i>visitor wants to know as much as possible. 
          This user does not rush, and moves from one exhibit to another in near sequent 
          order;</li>
    <li>   A<i> selective</i> visitor mainly chooses sound objects that 
          represent certain concepts.</li>
  </ul>
  <p>There are three levels of interest: <b>-1</b> (indicates disinterest), <b>0</b> 
      (indicates some interest), and <b>1</b> (indicates more interest), but they 
      can be extended. Visitor interest is computed as follows:</p>
  <ul>
    <li> When an <i>avaricious </i>visitor enters an exhibit, and is 
          slow, interests will be asserted to the primary concept of any narration 
          that describes an artifact in that exhibit. This makes sense, because we 
          do not need to be picky about interests and we can assume that this visitor 
          is interested in almost any concept.</li>
    <li> Interests of a <i>selective</i> visitor do not get easily overwritten. The rules engine should infer new interests only after this visitor repeatedly 
          chooses narrations with certain concepts.</li>
    <li> 
          For each exhibit, we need to calculate what the primary concept 
          is of most narrations that are about that exhibit. The interests of a <i>busy</i> visitor can only be overwritten with those when he enters an exhibit.</li>
    <li> For any visitor who repeatedly refuses to listen to narrations 
          with certain primary concepts, we can infer disinterest to those concepts.</li>
  </ul>
  <h3>4.4.2 Narrative models</h3>
  <p>In addition to our goal of linking repositories and ontologies across different 
      museums, we also faced the task of linking content across different exhibit 
      sections. In order to maintain coherency in an ec(h)o visitor experience, 
      we saw it as necessary to provide meaningful links between audio objects. 
      To facilitate this, it would be important to avoid situations where a clear 
      disconnect existed between two audio objects. In defining the notion of a 
      clear connection we identified the following categories of linkage types: 
  </p>
  <p class="listwithbullets"> 
      <i>Artifact to artifact</i>: This occurs when the content of 
      two audio objects makes reference to, or explicitly speaks to, the same artifact. 
      For example: audio object A and audio object B fall into this category when 
      they both reference the same moose antlers.</p>
  <p class="listwithbullets"> 
      <i>Concept to concept: </i>This link occurs when two audio objects 
      are conceptually linked - for example, audio object A and audio object B might 
      both talk about adaptation, and could therefore be linked without being considered 
      discontinuous. Note: it is our assumption that concept-to-concept links are 
      less tangible than artifact-to-artifact links. It is also worth noting that 
      an audio object will often speak to more than one concept. When multiple concepts 
      are present in an audio object, it is usually possible to discern one that 
      is more prominent than the others. Therefore, a classification hierarchy of 
      sorts can exist when we consider an audio object&#39;s conceptual make up - that 
      is, we might have an audio object with a primary concept and a secondary concept. 
      Here secondary concepts are defined as being less explicit than primary. </p>
  <p class="listwithbullets"> 
      <i>Localized links: </i>The notion of the localized link comes 
      from the observation that visitors like to explore when they are taking in 
      an exhibit. The idea here is that disconnects are not always a bad thing, 
      and that visitors find inherent satisfaction in the experience of re-orienting 
      themselves. To provide for this, we have made room for supporting discontinuous 
      links between objects, as long as they are at least partially contextually 
      localized - that is, in the same exhibit space.</p>
  <p>Based on the above explanation, linkage classifiers were formalized and used 
      to create rules that the system could then manage. In total, two types of 
      linkage classifiers were developed (primary and secondary), and each classifier 
      was given a point value. Point values reflected the concept of linkage tangibility. 
      It was our assumption that, in general, conceptually linked objects are less 
      tangible (unless, of course, the concept is made very explicit) than artifact-linked 
      objects. </p>
  <p>The primary link classifiers were those described in the discussion above 
      (artifact, concept, and localized) Secondary link classifiers deal with the 
      presence of contextual information embedded in an audio object itself. Context 
      information is defined as that which makes explicit reference to an artifact 
      - i.e.: <i>the shell marked number 5</i> - or, <i>the moose antlers in the center 
      of the display</i>. Contextual information helps to facilitate the visitor&#39;s 
      reference, and is thus important when dealing with artifact changes, and objects 
      that are linked based on the localization classifier. Two kinds of secondary 
      classifiers exist - contextualized, and non-contextualized. </p>
  <p>In evaluating the linkage potential between two objects, sameness and difference 
      across the primary link classifiers is considered. Contextualized content 
      with in the objects is also considered. To be linked, the sum of the primary 
      and secondary scores must achieve a certain value. An artifact-to-artifact 
      link is the most tangible, and therefore it is always classified as being 
      linkable, regardless of its conceptual and contextual information score. 
      Note that contextualized objects that are not localized are prone to creating 
      strong disconnects; therefore any objects that fall into this category are 
      never allowed to be linked.</p>
  <h3>4.4.3 Soundscape model</h3>
  <p>The soundscape model is composed of zones of ambient sounds that are modulated 
      when compared to a user&#39;s interactions and interests. In addition, proximity 
      to soundmarks affects the overall soundscape. The sounds are generally abstract 
      in nature.</p>
  <h1>5. Evaluation </h1>
  <p>Given the complex nature of the system and user evaluation, we tested our 
      design and development of the audio objects as we went along. User tests were 
      performed to evaluate the interaction model, the use and style of prefaces 
      and audio objects, and the inference rules and narrative models. A series 
      of technical and integration tests allowed for limited user testing of the 
      overall system. The final prototype will be installed at the Canadian Museum 
      of Nature in March, 2004, and we will then perform extensive user testing. 
      The series of progressive testing allowed us to modify our current design 
      and inform subsequent designs. </p>
  <p>To date, users have found the interaction experience coherent, and the design 
      of prefaces and audio objects effective. Participants reported no 
      significant issues around poor flow or clunky content presentation. A consensus 
      emerged in support of the style and flavor of the audio object prefaces, which 
      were viewed as being entertaining and effective based on their ability to 
      pique curiosity and motivate further interaction. For the most part, topical 
      links between objects were better observed than conceptual links. Two characteristic 
      behavior patterns emerged to indicate that our original concern over avoiding 
      disconnections across linked objects may have been unwarranted. First, participants 
      tended to jump across topically, and in doing so often encountered disparate 
      content in their turn taking. Second, participants admitted that their impetus 
      for choices was more in keeping with a need to satisfy their curiosity (curiosity 
      created by the prefaces, that is). This partly countered our assumption that 
      participants would be exercising choice based on a need to hear more information 
      about a specific topic or concept. Both of these insights indicated that 
      users were more inclined to approach the experience from a position of play, 
      rather than structured, focused exploration. A welcome result!</p>
  <h1>6. Future Work and limitations</h1>
  <p>Current limitation of our process is the timeliness by which audio objects 
      are designed, meta-tagged and then tested. This mitigates against open development 
      of audio objects available for use within the network by other producers. 
      The current system has very limited implementation of the networked potential 
      of the system.</p>
  <p>In the areas of audio display and interaction, we will need further testing 
      to evaluate if our minimal intervention in terms of contextual guidance is 
      successful or not. We may find that visitors need more explicit instructions, 
      either through audio or text. In addition, we have some concerns about issues 
      of selection and integration of the various modes of audio display and their 
      combination as determined by an inference system. For optimal auditory satisfaction, 
      It should be ensured that the frequency range, amplitude and ambient elements 
      from one sound layer are not interfering with the bandwidth and clarity of 
      the other sound layer.</p>
  <p>Future work will lead us to researching further the complex roles of the 
      design of audio objects, inference rules, audio display and the interaction 
      model in creating engaging and playfully exploratory interaction.</p>
  <h1>Acknowledgements</h1>
  <p class="ReferencesText">Work presented in this paper is supported by Canarie 
      Inc. grant under E-Content program. Authors  especially thank  
      Mark Graham and his colleagues in the Nature Museum in Ottawa for their enthusiastic 
      support to this project. We would also like to thank our colleagues and participants 
      in several workshops that contributed to the development of the project, namely 
      Doreen Leo, Gilly Mah, Robb Lovell, Mark Brady, Jordan Williams and Leila 
      Kalantari.</p>
  <h1>References</h1>
  <p class="ReferencesText">Andolesk, D. and M. Freedman. 
      (2001). Artifact As Inspiration: Using Existing Collections And Management Systems To Inform And Create New Narrative 
      Structures. <i>Museums and the Web</i> 2001, Pittsburgh, Archives &amp; Museum 
      Informatics, <a href="http://www.archimuse.com/mw2001/papers/andolsek/andolsek.html" target="_blank">http://www.archimuse.com/mw2001/papers/andolsek/andolsek.html</a></p>
  <p class="ReferencesText">Aoki, P. M., R. E. Grinter, et al. (2002<i>). </i>Sotto 
      Voce: Exploring the Interplay of Conversation and Mobile Audio Spaces., <i>Proceedings 
      of the SIGCHI conference on Human factors in computing systems: Changing our 
      world, changing ourselves</i> (Proc. CHI 2002) Minneapolis, Minnesota, USA, 
      431-438</p>
  <p>Bederson, B. (1995). Audio Augmented Reality: a prototype automated tour guide. <i>Conference companion on Human factors in computing 
      systems</i> (CHI &#39;95), Denver, Colorado, United States, 210-211 </p>
  <p class="ReferencesText">Crofts, N., I. Dionissiadou, et al. (2002). Definition 
      of the CIDOC object-oriented Conceptual Reference Model (version 3.2.1), <a href="http://www.cidoc-crm.org/docs/cidoc_crm_version_3.2.1.rtf" target="_blank">http://cidoc.ics.forth.gr/docs/cidoc_crm_version_3.2.1.rtf</a>.</p>
  <p class="ReferencesText">Goecks, J. and J. Shavlik (2000). Learning user&#39;s interests 
      by unobtrusively observing their normal behaviou<u>r</u>. ACM 5th International 
      Conference on Intelligent User Interfaces (IUI), New Orleans, 129-132 </p>
  <p class="ReferencesText">Hatala, M., L. Kalantari, et al. (2004). Ontology and Rule based Retrieval of 
      Sound Objects n Augmented Audio Reality System for Museum Visitors. ACM 
      Symposium on Applied Computing Conference, Nicosia, Cyprus, </p>
  <p class="ReferencesText">Kobsa, A. and J. Fink (2002). User Modeling for 
      Personalized City Tours. <i>Artificial</i><u> </u><i>Intelligence Review</i> 
      <b>18</b>: 33-74.</p>
  <p class="ReferencesText">Lehn, D. v., C. Heath, et al. (2001). Exhibiting 
      Interaction: Conduct and Collaboration in Museums and Galleries <i>Symbolic 
      Interaction</i> <b>24</b>(2): 189-216.</p>
  <p class="ReferencesText">Leinhardt, G. and K. Crowley (1998). Museum Learning 
      as Conversational Elaboration: A Proposal to Capture, Code and Analyze Talk 
      in Museums. Pittsburgh, Report for the Learning Research &amp; Development 
      Center, University of Pittsburgh<b>: </b>24, <a href="http://mlc.lrdc.pitt.edu/mlc" target="_blank">http://mlc.lrdc.pitt.edu/mlc</a>.</p>
  <p class="ReferencesText">Proctor, N. and C. Tellis (2003). The State of the Art 
      in Museum Handhelds. <i>Museums and the Web 2003</i>, Pittsburgh, Archives 
    &amp; Museums Informatics, <a href="http://www.archimuse.com/mw2003/papers/proctor/proctor.html" target="_blank">http://www.archimuse.com/mw2003/papers/proctor/proctor.html</a></p>
  <p class="ReferencesText">Sacks, H. (1974). An Analysis of the Course of a Joke&#39;s 
      Telling in Conversation. <i>Explorations in the Ethnography of Speaking</i>. 
      R. Bauman and J. Sherzer. Cambridge, Cambridge University Press<b>: </b>337-353.</p>
  <p class="ReferencesText">Sarini, M. and C. Strapparava (1998). Building a User Model for a Museum Exploration and Information Providing Adaptive System. Proceedings of the 2nd Workshop 
      on Adaptive Hypertext and Hypermedia, Ninth ACM Conference on Hypertext and 
      Hypermedia HYPERTEXT&#39;98, Pittsburgh, USA, June 20-24, 1998, available: <a href="http://wwwis.win.tue.nl/ah98/Sarini/Sarini.html" target="_blank">http://wwwis.win.tue.nl/ah98/Sarini/Sarini.html</a></p>
  <p class="ReferencesText">Sparacino, F. (2002). The Museum Wearable: real-time 
      sensor-driven understanding of visitors&#39; interests for personalized visually-augmented 
      museum experiences. <i>Museums and the Web 2002</i>, Pittsburgh, Archives 
    &amp; Museum Informatics, <a href="http://www.archimuse.com/mw2002/papers/sparacino/sparacino.html" target="_blank">http://www.archimuse.com/mw2002/papers/sparacino/sparacino.html</a></p>
  <p class="ReferencesText">Wahlster, W. and A. Kobsa (1989). <i>User Models in Dialog 
      Systems</i>. Heidelberg &amp; Berlin, Springer Verlag.</p>
  <p class="ReferencesText">Wakkary, R., M. Hatala, et al. (2003). Echoing the conversational 
      space of museums through audio augmented reality and adaptive information 
      retrieval. <i>Submitted</i> ACM / SIG CHI, Designing Interactive Systems, 2004, Cambridge 
      Mass., </p>
  <p class="ReferencesText">Woodruff, A., P. M. Aoki, et al. (2001). Electronic Guidebooks 
      and Visitor Attention. <i>International Cultural Heritage Informatics Meeting, 
      Cultural Heritage and Technologies in the Third Millennium: Long Papers </i>(ichim01), 
      Archives &amp; Museum Informatics / Politechnico di Milano, 2001. 437-454 
      <a href="http://citeseer.nj.nec.com/woodruff01electronic.html" target="_blank">http://citeseer.nj.nec.com/woodruff01electronic.html</a></p>
  <p class="ReferencesText">Woodruff, A., M. H. Szymanski, et al. (2001). The Conversational 
      Role of Electronic Guidebooks. Ubicomp 2001: Ubiquitous 
      Computing. Third International Conference Atlanta, Georgia, USA, 2001, Proceedings 
      G.D. Abowd, B. Brumitt, S. Shafer (Eds.): <i>Lecture Notes in Computer Science</i><i>, 
    </i>Springer-Verlag, 187-208 </p>
      <!-- #EndEditable --></td>
  </tr>
</tbody></table>

<p class="smallPurple"> </p>
<!--htdig_noindex-->

<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script> 
<script type="text/javascript" src="http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript></noscript>

<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>

<!--/htdig_noindex-->





</body><!-- InstanceEnd --><!-- Mirrored from www.museumsandtheweb.com/mw2004/papers/wakkary/wakkary.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:44:40 GMT --></html>