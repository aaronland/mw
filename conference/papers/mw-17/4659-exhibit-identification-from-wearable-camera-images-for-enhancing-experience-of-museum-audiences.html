<!DOCTYPE html>
<html lang="en-US" class="no-js">

<!-- Mirrored from mw17.mwconf.org/paper/exhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 23 Apr 2022 22:09:04 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,600,700" rel="stylesheet">
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"/>
	<link rel="profile" href="http://gmpg.org/xfn/11">
		<script>(function(html){html.className = html.className.replace(/\bno-js\b/,'js')})(document.documentElement);</script>
<title>Artwork identification from wearable camera images for enhancing experience of museum audiences &#8211; MW17: Museums and the Web 2017</title>
<link rel='dns-prefetch' href='../../index.html' />
<link rel='dns-prefetch' href='http://fonts.googleapis.com/' />
<link rel='dns-prefetch' href='http://s.w.org/' />
<link href='https://fonts.gstatic.com/' crossorigin rel='preconnect' />
<link rel="alternate" type="application/rss+xml" title="MW17: Museums and the Web 2017 &raquo; Feed" href="../../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="MW17: Museums and the Web 2017 &raquo; Comments Feed" href="../../comments/feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="MW17: Museums and the Web 2017 &raquo; Artwork identification from wearable camera images for enhancing experience of museum audiences Comments Feed" href="feed/index.html" />
		<script>
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/mw17.mwconf.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.6"}};
			!function(e,a,t){var r,n,o,i,p=a.createElement("canvas"),s=p.getContext&&p.getContext("2d");function c(e,t){var a=String.fromCharCode;s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,e),0,0);var r=p.toDataURL();return s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,t),0,0),r===p.toDataURL()}function l(e){if(!s||!s.fillText)return!1;switch(s.textBaseline="top",s.font="600 32px Arial",e){case"flag":return!c([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])&&(!c([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!c([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]));case"emoji":return!c([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}function d(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(i=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},o=0;o<i.length;o++)t.supports[i[o]]=l(i[o]),t.supports.everything=t.supports.everything&&t.supports[i[o]],"flag"!==i[o]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[i[o]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(r=t.source||{}).concatemoji?d(r.concatemoji):r.wpemoji&&r.twemoji&&(d(r.twemoji),d(r.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style>
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='wp-block-library-css'  href='../../wp-includes/css/dist/block-library/style.min40df.css?ver=5.6' media='all' />
<link rel='stylesheet' id='wp-block-library-theme-css'  href='../../wp-includes/css/dist/block-library/theme.min40df.css?ver=5.6' media='all' />
<link rel='stylesheet' id='sidebar-login-css'  href='../../wp-content/plugins/sidebar-login/build/sidebar-login3822.css?ver=1604077128' media='all' />
<link rel='stylesheet' id='parent-style-css'  href='../../wp-content/themes/twentysixteen/style40df.css?ver=5.6' media='all' />
<link rel='stylesheet' id='child-style-css'  href='../../wp-content/themes/mw-twentysixteen/style40df.css?ver=5.6' media='all' />
<link rel='stylesheet' id='twentysixteen-fonts-css'  href='https://fonts.googleapis.com/css?family=Merriweather%3A400%2C700%2C900%2C400italic%2C700italic%2C900italic%7CMontserrat%3A400%2C700%7CInconsolata%3A400&amp;subset=latin%2Clatin-ext&amp;display=fallback' media='all' />
<link rel='stylesheet' id='genericons-css'  href='../../wp-content/themes/twentysixteen/genericons/genericons3ce7.css?ver=20201208' media='all' />
<link rel='stylesheet' id='twentysixteen-style-css'  href='../../wp-content/themes/mw-twentysixteen/style3ce7.css?ver=20201208' media='all' />
<style id='twentysixteen-style-inline-css'>

		/* Custom Link Color */
		.menu-toggle:hover,
		.menu-toggle:focus,
		a,
		.main-navigation a:hover,
		.main-navigation a:focus,
		.dropdown-toggle:hover,
		.dropdown-toggle:focus,
		.social-navigation a:hover:before,
		.social-navigation a:focus:before,
		.post-navigation a:hover .post-title,
		.post-navigation a:focus .post-title,
		.tagcloud a:hover,
		.tagcloud a:focus,
		.site-branding .site-title a:hover,
		.site-branding .site-title a:focus,
		.entry-title a:hover,
		.entry-title a:focus,
		.entry-footer a:hover,
		.entry-footer a:focus,
		.comment-metadata a:hover,
		.comment-metadata a:focus,
		.pingback .comment-edit-link:hover,
		.pingback .comment-edit-link:focus,
		.comment-reply-link,
		.comment-reply-link:hover,
		.comment-reply-link:focus,
		.required,
		.site-info a:hover,
		.site-info a:focus {
			color: #e55747;
		}

		mark,
		ins,
		button:hover,
		button:focus,
		input[type="button"]:hover,
		input[type="button"]:focus,
		input[type="reset"]:hover,
		input[type="reset"]:focus,
		input[type="submit"]:hover,
		input[type="submit"]:focus,
		.pagination .prev:hover,
		.pagination .prev:focus,
		.pagination .next:hover,
		.pagination .next:focus,
		.widget_calendar tbody a,
		.page-links a:hover,
		.page-links a:focus {
			background-color: #e55747;
		}

		input[type="date"]:focus,
		input[type="time"]:focus,
		input[type="datetime-local"]:focus,
		input[type="week"]:focus,
		input[type="month"]:focus,
		input[type="text"]:focus,
		input[type="email"]:focus,
		input[type="url"]:focus,
		input[type="password"]:focus,
		input[type="search"]:focus,
		input[type="tel"]:focus,
		input[type="number"]:focus,
		textarea:focus,
		.tagcloud a:hover,
		.tagcloud a:focus,
		.menu-toggle:hover,
		.menu-toggle:focus {
			border-color: #e55747;
		}

		@media screen and (min-width: 56.875em) {
			.main-navigation li:hover > a,
			.main-navigation li.focus > a {
				color: #e55747;
			}
		}
	
</style>
<link rel='stylesheet' id='twentysixteen-block-style-css'  href='../../wp-content/themes/twentysixteen/css/blockscb8b.css?ver=20190102' media='all' />
<!--[if lt IE 10]>
<link rel='stylesheet' id='twentysixteen-ie-css'  href='https://mw17.mwconf.org/wp-content/themes/twentysixteen/css/ie.css?ver=20170530' media='all' />
<![endif]-->
<!--[if lt IE 9]>
<link rel='stylesheet' id='twentysixteen-ie8-css'  href='https://mw17.mwconf.org/wp-content/themes/twentysixteen/css/ie8.css?ver=20170530' media='all' />
<![endif]-->
<!--[if lt IE 8]>
<link rel='stylesheet' id='twentysixteen-ie7-css'  href='https://mw17.mwconf.org/wp-content/themes/twentysixteen/css/ie7.css?ver=20170530' media='all' />
<![endif]-->
<!--[if lt IE 9]>
<script src='https://mw17.mwconf.org/wp-content/themes/twentysixteen/js/html5.js?ver=3.7.3' id='twentysixteen-html5-js'></script>
<![endif]-->
<script src='../../wp-includes/js/jquery/jquery.min9d52.js?ver=3.5.1' id='jquery-core-js'></script>
<script src='../../wp-includes/js/jquery/jquery-migrate.mind617.js?ver=3.3.2' id='jquery-migrate-js'></script>
<link rel="https://api.w.org/" href="../../wp-json/index.html" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../xmlrpc0db0.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 5.6" />
<link rel="canonical" href="index.html" />
<link rel='shortlink' href='../../index820a.html?p=1120' />
<link rel="alternate" type="application/json+oembed" href="../../wp-json/oembed/1.0/embed31a5.json?url=https%3A%2F%2Fmw17.mwconf.org%2Fpaper%2Fexhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences%2F" />
<link rel="alternate" type="text/xml+oembed" href="../../wp-json/oembed/1.0/embed8cfe?url=https%3A%2F%2Fmw17.mwconf.org%2Fpaper%2Fexhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences%2F&amp;format=xml" />
	<script src="https://use.typekit.net/cfh1uqy.js"></script>
	<script>try{Typekit.load({ async: true });}catch(e){}</script>
	<script type="text/javascript" src="../../wp-content/themes/mw-twentysixteen/flexibility.js"></script>
</head>

<body class="paper-template-default single single-paper postid-1120 wp-custom-logo wp-embed-responsive">
<div id="page" class="site"> <!-- TAG CLOSES IN FOOTER.PHP (.site) -->
	<div class="site-inner"> <!-- TAG CLOSES IN FOOTER.PHP (.site-inner) -->
		<a class="skip-link screen-reader-text" href="#content">Skip to content</a>
		<header id="masthead" class="site-header" role="banner">
		<div class="site-header-main">
					<div class="site-branding">
						<a href="../../index.html" class="custom-logo-link" rel="home"><img width="240" height="240" src="../../wp-content/uploads/2017/02/cropped-mw-final.png" class="custom-logo" alt="MW17: Museums and the Web 2017" /></a>					</div>

											<button id="menu-toggle" class="menu-toggle">Menu</button>

						<div id="site-header-menu" class="site-header-menu">
							<div class="top-nav-desc">
								<p> MW17: Museums and the Web 2017 <span>Cleveland Ohio - April 19-22</span></p>
								<form class="search-form" role="search" method="get" action="#">
									<label>
										<span class="screen-reader-text">Search for:</span>
										<input class="search-field" placeholder="Search …" value="" name="s" type="search">
									</label>
									<button class="search-submit" type="submit">
										<span class="screen-reader-text">Search</span>
									</button>
								</form>
							</div>
															<nav id="site-navigation" class="main-navigation" role="navigation" aria-label="Primary Menu">
									<div class="menu-menu-1-container"><ul id="menu-menu-1" class="primary-menu"><li id="menu-item-3074" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-3074"><a href="https://museumsandtheweb.com/">MW Home</a></li>
<li id="menu-item-88" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-has-children menu-item-88"><a href="../../index.html">About</a>
<ul class="sub-menu">
	<li id="menu-item-90" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-90"><a href="../../key-dates/index.html">Key Dates</a></li>
	<li id="menu-item-580" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-580"><a href="../../local-information-and-accommodation/index.html">Local Information and Accommodation</a></li>
	<li id="menu-item-1917" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1917"><a href="../../faqs/index.html">FAQs</a></li>
</ul>
</li>
<li id="menu-item-172" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-172"><a href="../../attending/index.html">Attending</a>
<ul class="sub-menu">
	<li id="menu-item-552" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-552"><a href="../../registration/index.html">Registration</a></li>
	<li id="menu-item-475" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-475"><a href="../../scholarships-volunteering/index.html">Scholarships &#038; Volunteering</a></li>
</ul>
</li>
<li id="menu-item-108" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-108"><a href="../../call-for-proposals/index.html">Presenting</a>
<ul class="sub-menu">
	<li id="menu-item-490" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-490"><a href="../../call-for-proposals/index.html">Call for Proposals</a></li>
	<li id="menu-item-494" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-494"><a href="../../presentation-guidelines/index.html">Presentation Guidelines</a></li>
	<li id="menu-item-489" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-489"><a href="../../paper-guidelines/index.html">Paper Guidelines</a></li>
</ul>
</li>
<li id="menu-item-135" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-135"><a href="../../exhibiting/index.html">Exhibiting</a>
<ul class="sub-menu">
	<li id="menu-item-479" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-479"><a href="../../exhibitor-registration/index.html">Exhibitor Registration</a></li>
	<li id="menu-item-173" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-173"><a href="../../sponsorship-opportunities/index.html">Sponsorship Opportunities</a></li>
	<li id="menu-item-1689" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1689"><a href="../../advertising/index.html">Advertising</a></li>
</ul>
</li>
<li id="menu-item-573" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-573"><a href="../../program/index.html">Conference</a>
<ul class="sub-menu">
	<li id="menu-item-2972" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2972"><a href="../../program/index.html">Program</a></li>
	<li id="menu-item-483" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-483"><a href="../../exhibits/index.html">Exhibits</a></li>
	<li id="menu-item-3023" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3023"><a href="../../select-session-slides/index.html">Presentations</a></li>
	<li id="menu-item-1760" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1760"><a href="../../glami-awards/index.html">GLAMi Awards</a>
	<ul class="sub-menu">
		<li id="menu-item-2968" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2968"><a href="../../glami-winners/index.html">GLAMi Winners</a></li>
		<li id="menu-item-2963" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2963"><a href="../../glami-nominees/index.html">GLAMi Nominees</a></li>
	</ul>
</li>
	<li id="menu-item-2894" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2894"><a href="../../mwx/index.html">MWX 2017</a></li>
	<li id="menu-item-1252" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1252"><a href="../../attendees/index.html">Attendees</a></li>
	<li id="menu-item-460" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-460"><a href="../../committees/index.html">Committees</a></li>
</ul>
</li>
<li>
    <form role='search' method='get' class='search-form' action='http://mw17.mwconf.org/'>
	<label>
	<span class='screen-reader-text'>Search for:</span>
	<input type='search' class='search-field' placeholder='Search &hellip;' value='' name='s' />
	</label>
	<button type='submit' class='search-submit'>
	<span class='screen-reader-text'>Search</span></button>
	</form>    
	</li></ul></div>								</nav><!-- .main-navigation -->
							
													</div><!-- .site-header-menu -->
									</div><!-- .site-header-main -->

			</header><!-- .site-header -->


	
<section id="page-hero">
	<div class="background"> </div>
</section>
<script>
    var images = ['page-hero1.jpg', 'page-hero2.html', 'page-hero3.html', 'page-hero4.html', 'page-hero5.html','page-hero6.html', 'page-hero7.html', 'page-hero8.jpg' ];
    jQuery('#page-hero .background').css("background", "rgba(0, 0, 0, 0) url('https://mw17.mwconf.org/wp-content/themes/mw-twentysixteen/images/" +images[Math.floor(Math.random() * images.length)]+ "') no-repeat scroll center top / cover") 
	</script>

	<div id="content" class="site-content"> <!-- TAG CLOSES IN FOOTER.PHP (.site-header) -->

<div id="primary" class="content-area">
<main id="main" class="mw-main75" role="main">


<article id="post-1120" class="post-1120 paper type-paper status-publish hentry">

<style type="text/css">
h2 { font-weight: bold; font-size: 18px; }
h3, h4, h5, h6 { font-weight: bold; font-size: 16px; }

td, th { padding: 5px; }
th { background-color: #ccc; font-weight: bold; }
tr:nth-child(odd) { background-color: #eee }
</style>


<h1 class="entry-title">Artwork identification from wearable camera images for enhancing experience of museum audiences</h1>

<p><a target="_blank" href="http://www.museumsandtheweb.com/member/Renata.zhang/">Rui Zhang</a>, Hubei University of Arts and Science, China, <a target="_blank" href="http://www.museumsandtheweb.com/member/YTas/">Yusuf Tas</a>, Australian National University, Australia, <a target="_blank" href="http://www.museumsandtheweb.com/member/PK0514/">Piotr Koniusz</a>, Australian National University, Australia</p>
<h2>Abstract</h2>
Recommendation systems based on image recognition could prove a vital tool in enhancing the experience of museum audiences. However, for practical systems utilizing wearable cameras, a number of challenges exist which affect the quality of image recognition. In this pilot study, we focus on recognition of museum collections by using a wearable camera in three different museum spaces. We discuss the application of wearable cameras, and the practical and technical challenges in devising a robust system that can recognize artworks viewed by the visitors to create a detailed record of their visit. Specifically, to illustrate the impact of different kinds of museum spaces on image recognition, we collect three training datasets of museum exhibits containing variety of paintings, clocks, and sculptures. Subsequently, we equip selected visitors with wearable cameras to capture artworks viewed by them as they stroll along exhibitions. We use Convolutional Neural Networks (CNN) which are pre-trained on the ImageNet dataset and fine-tuned on each of the training sets for the purpose of artwork identification. In the testing stage, we use CNNs to identify artworks captured by the visitors with a wearable camera. We analyze the accuracy of their recognition and provide an insight into the applicability of such a system to further engage audiences with museum exhibitions.</p>
<p><b>Keywords:</b> Wearable camera, image recognition, museum artworks, audiences, experience, CNN</p>
<h2>1. Introduction</h2>
<p>A vast number of approaches exist dedicated to engaging and educating audiences in museums, e.g. augmented reality, mobile guides, interactive collections and 3D displays, to name a few. Artworks in museums engage visitors with their past experiences and trigger effective response which constitutes a vital aspect of a positive museum experience (Alelis et al., 2015). The value of emotional experiences in museums has been linked to reinforced trust, increased chances of recurring visits, as well as gaining donations (Suchy, 2006).</p>
<p>However, the experience of visitors is often incomplete because of the limited space dedicated to museum exhibitions, and personal time constraints during the visit. Beer (1987) pointed out that museum visitors spend less than one minute with each artwork during a typical visit. To a large extent, the audience has a limited idea of artworks they want to view or topics they are excited to cover. Therefore, they visit museums based on personal recommendations, advertisements, or a rough idea of the topics a museum covers. Viewers often adopt a fast pace as they stroll along through exhibition space, giving an incomplete or repetitive experience. Moreover, museums and cultural sites often lack interactive or personalized entertainment gadgets, guideline systems, and other technology to customize visits efficiently (Baraldi et al., 2007).</p>
<p>It is undeniable that museum audiences have access to smart phones and virtual interactive technology. However, robust guide systems that help satisfy their expectations and enhance their emotional experience are still rare. Kuflik et al. (2007) proposed a system customizing user’s experience which employs statistical machine learning capable of inferring visitors’ interests, based on their answers to a pre-specified questionnaire. By analogy, in order to aid a museum curator&#8217;s work, wearable or security cameras could provide an input to autonomous software which in turns would perform an analysis of audiences’ preferences inside the museum. Such a system could count numbers of visitors, capture the time they spend with specific artworks, or even attempt to recognize their mood based on facial expressions, in order to isolate the most popular artworks, as well as consider visitor’s likes and dislikes. However, wearable devices have limited processing power, and memories which are based on so-called local feature descriptors (Dalens et al., 2014). Nonetheless, more robust end-to-end recognition systems such as Convolutional Neural Networks (CNN) have been shown to be particularly well suited for object category recognition (Krizhevsky et al., 2012). We therefore assess the suitability of CNNs for image recognition of museum artworks captured with wearable cameras. CNNs require a lot of computational resources at their training stage. However, they can perform real-time recognition on Android-based systems with a camera.</p>
<p>In this work, we use wearable cameras for capturing images of artworks captured &#8220;in-the-wild&#8221; by audiences as they stroll along three different museum spaces and interact with various artworks. We used the data we collected to study the ability of CNNs to identify specific artworks in images.</p>
<p>As artworks vary from paintings, to sculptures, to other unusual rigid and non-rigid shapes and texture forms, we illustrate the impact of different types of museum spaces on image recognition. Specifically, we first collect non-occluded images of art pieces in each exhibition space with a phone camera. Next, we use the database of images collected by the audiences as they stroll with wearable cameras for testing recognition accuracy.</p>
<p>In the training stage, we use CNN pre-trained on the ImageNet dataset (Russakovsky et al., 2015) and fine-tune such a pre-trained CNN on each of our datasets for the purpose of artwork identification. Due to the major technical challenges in image recognition such as non-planar sculptures, glare of protective cabinets, reflective properties of surfaces, background clutter, occlusions, rotations, scale changes, viewpoint changes, lighting variations, motion blur, and other limiting factors, this work is conducted as a pilot study to identify the impact of these phenomena on recognition. The results will provide a better understanding of whether a wearable camera-based system can be used to help audiences engage with museum exhibitions, and if they reliably identified artworks from wearable cameras that could be used as an input for a recommendation system.</p>
<h2>2. Artwork identification with wearable cameras</h2>
<p>Our work aims to identify artworks using wearable cameras in the context of the museum. Our hope is that museums might benefit from wearable technology in order to improve guidance and management of audiences. For this purpose, we choose three different types of museum spaces that pose varied challenges in terms of image capturing with wearable cameras.</p>
<p><strong>Shenzhen Art Museum</strong>, located in Shenzhen, Guangdong Province, China, has a diverse collection of artworks such as traditional Chinese paintings, oil paintings, prints, sculptures, calligraphy, watercolors, caricatures, paper-cuttings, and photographic works. For this study, we capture the Chinese traditional paintings from this museum.</p>
<p><strong>The Palace Museum</strong>, located in Beijing, China, is a home to the Clock and Watch Gallery as well as the Indian and Chinese Sculpture Exhibition (AD400-700). The collections in the Clock and Watch Gallery consist of more than two hundred clocks from the 18th century. The sculptures of the Indian and Chinese Sculpture Exhibition mainly include Buddhist statues from India and China from AD400 to AD700.</p>
<h3>2.1 Data collection</h3>
<p>In order to train a recognition algorithm, we needed to collect a dataset of &#8220;objects to identify.&#8221; For this purpose, we used an ordinary Android phone. To account for viewpoint and scale changes, we captured between two and six photos of each artwork viewed from different viewpoints and distances. For testing purposes, we equiped six volunteers with a wearable camera and asked them to walk the exhibition space and interact with artworks. Afterwards, we annotated these images with labels assigned to the artworks that can be seen in each image. The wearable camera is configured to capture a picture every 10 seconds.</p>
<p><strong>Shenzhen Paintings</strong> consists of 79 distinct paintings that were displayed in the museum during the capturing process, each photographed several times, resulting in the total of 369 images. Figure 1 illustrates that these paintings were captured under several viewpoints. We also included a background category representing museum surroundings, which consists of 27 images, and a spurious category of 170 miscellaneous paintings that were not on display. The latter subset helps to refine the classifier which has to distinguish between the 79 specific instances of paintings, other possible artworks, and the background. This resulted in 566 training images. For the testing set, we equipped six volunteers with the wearable camera and collected six different splits, as detailed below.</p>
<figure id="attachment_1696" aria-describedby="caption-attachment-1696" style="width: 888px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1696" src="../../wp-content/uploads/2017/01/zhang.fig1_.png" alt="" width="888" height="176" /><figcaption id="caption-attachment-1696" class="wp-caption-text">Figure 1: examples of Shenzhen paintings. For the training set, we captured paintings from various viewpoints</figcaption></figure>
<p>Split 1 contains 86 images from the wearable camera, which was mounted at the right-hand side pocket at upper chest height. Split 2 contains 93 images from the camera mounted on the right-hand side of a jacket zipped up to chest height. Split 3 contains 54 images from the partially rotated camera mounted on the left-hand side belt of a backpack at the mid-chest height. Split 4 includes 86 close-up images from the camera mounted on the collar. Splits 5 and 6 contain 91 and 105 images from the camera mounted on a handbag strap at chest height and left-hand side bottom, respectively.</p>
<figure id="attachment_1703" aria-describedby="caption-attachment-1703" style="width: 897px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1703" src="../../wp-content/uploads/2017/01/zhang.fig2_.png" alt="" width="897" height="510" /><figcaption id="caption-attachment-1703" class="wp-caption-text">Figure 2: Shenzhen paintings. Top and middle rows show that the geometric transformations resulting from capturing the test set by the wearable camera are large. They include perspective changes, zoom, rotations and cropping. The bottom row also shows an occlusion by person, glare, motion blur and an occlusion by hand</figcaption></figure>
<p>Figure 2 illustrates images captured by the wearable camera and resulting transformations which make recognition a challenge. In total, the testing set resulted in 515 images of paintings. We annotated each image with ground truth labels that indicate the paintings which are visible in these images (ordered from the most visible artwork to the least visible one). During the training stage, we chose one of the splits for testing and the remaining five splits for validation. Therefore, to obtain accuracy on all six splits, we had to repeat the training six times. To enhance our study by recognizing artworks other than paintings, which are planar, we collected the following datasets:</p>
<p><strong>The Clocks </strong>dataset consists of 113 distinct clocks, each photographed several times, resulting in 394 images. Additionally, we captured 27 images of backgrounds not containing any clocks. For validation, we captured a separate set with the Android camera, which contains 259 images. Lastly, for testing, we devised two splits captured by two volunteers consisting of 182 and 141 images. They were captured with a camera mounted on the pocket (the top of chest) and on the handbag belt (mid-chest) with straight and rotated orientations, respectively. Overall, this resulted in 653 training and 323 testing images. Examples of clocks from training and testing sets are shown in figure 3 (top) and figure 4 (top).</p>
<p><strong>The Sculptures</strong> dataset consists of 44 distinct sculptures, each photographed several times, resulting in 206 images. An additional two categories were created which consist of photos of sculpture descriptions which may contain only tiny fragments of sculptures and 27 images of background. Two testing splits were captured by volunteers and resulted in 80 and 50 images, respectively. The cameras were mounted on the handbag belt (mid-chest) with clockwise and counterclockwise orientations, respectively. When testing on the first split, the second one is used for validation, and vice versa. Overall, training and testing sets resulted in 233 and 130 images, respectively. While this is the smallest testing set, it is also the most challenging, due to large nonplanar sculptures on display in several locations, which include other sculptures in the background. Examples of sculptures from training and testing sets are shown in figure 3 (bottom) and figure 4 (bottom).</p>
<figure id="attachment_1697" aria-describedby="caption-attachment-1697" style="width: 922px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1697" src="../../wp-content/uploads/2017/01/zhang.fig3_.png" alt="" width="922" height="362" /><figcaption id="caption-attachment-1697" class="wp-caption-text">Figure 3: examples of pieces from the Clock and Sculptures training sets are given in the top and bottom row, respectively. Note the non-planarity of these pieces as well as glare from the protective glass</figcaption></figure>
<figure id="attachment_1698" aria-describedby="caption-attachment-1698" style="width: 922px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1698" src="../../wp-content/uploads/2017/01/zhang.fig4_.png" alt="" width="922" height="239" /><figcaption id="caption-attachment-1698" class="wp-caption-text">Figure 4: examples from the Clock and Sculptures testing sets are given in the top and bottom row, respectively. Glares, viewpoint changes, rotations, background clutter, occlusions, and salt-and-pepper noise (Gonzalez &amp; Woods, 2006) occur in large quantities</figcaption></figure>
<h3>2.2 Image recognition</h3>
<p>For the purposes of artwork identification, we employ one of the latest CNN architectures known as VGG16 (Simonyan &amp; Zisserman, 2014) which consists of 13 so-called convolutional layers and three fully connected layers which results in an extremely large number of network parameters that need to be inferred in the training stage. Therefore, we pre-train it with the ImageNet dataset containing over 14 million images and 1000 object categories. Subsequently, we utilize the training data we collected to perform image augmentations (Krizhevsky et al., 2012) and we fine-tune the VGG16 network on these images. Details and discussions on fine-tuning can be found in numerous literature (Chu et al., 2016). The hyper-parameters are selected in the cross-validation process by used validation sets, as described in section 2.1. Lastly, in the testing stage, we applied the trained network to our test sets in a feed-forward manner, quantifying whether identification agrees with the ground truth. This outcome is indicative of whether CNN can reliably recognize what visitors see in wearable camera images. Figure 5 illustrates the pipeline used in our experiments:</p>
<figure id="attachment_1699" aria-describedby="caption-attachment-1699" style="width: 818px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1699" src="../../wp-content/uploads/2017/01/zhang.fig5_.png" alt="" width="818" height="475" /><figcaption id="caption-attachment-1699" class="wp-caption-text">Figure 5: training of CNN. In figure 5a, the network is first pre-trained on augmented images from the ImageNet dataset. Then, the augmented training set is used for so-called fine-tuning to adapt the network to recognize the training set. Figure 5b shows the testing stage.</figcaption></figure>
<p><strong>Data augmentation. </strong>A standard technique to train CNN representations, which are somewhat invariant to partial image translation, rotation, scale and viewpoint changes, is to augment the training dataset with multiple crops of images (e.g. left, right, top, bottom, center crops), mirroring images by left-right flips, arbitrary rotations, and contrast changes. We apply this technique to each training set to replicate expected variations between training and testing splits resulting from the capturing process.</p>
<h2>3. The impact of different types of museum spaces on data capturing and image recognition</h2>
<p>In the museum context, design decisions such as interior light, visitor circulation quality, audiences’ time limitations, layout of showcases, size of artwork and type of artwork have been shown to affect the number of pieces that visitors will encounter. Without a doubt, these factors will also impact the quality of images captured by wearable cameras. For instance, good quality uniform illumination will be positively correlated with the acquisition of crisp images. However, artworks with scarce lighting will result in images that show signs of the sensor noise, e.g. the salt-and-pepper noise known in digital photography (Gonzalez &amp; Woods, 2006).</p>
<figure id="attachment_1700" aria-describedby="caption-attachment-1700" style="width: 802px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1700" src="../../wp-content/uploads/2017/01/zhang.fig6_.png" alt="" width="802" height="530" /><figcaption id="caption-attachment-1700" class="wp-caption-text">Figure 6: the layouts of Shenzhen Art Museum and the Palace Museum are given in Figure 6a and 6b, respectively</figcaption></figure>
<h3>3.1 Paintings</h3>
<p>Paintings in Shenzhen Art Museum are displayed in an ordered manner within the given exhibition space. The arrangement of paintings in the exhibition halls influences to what degree audiences interact with these paintings, and it imposes some natural order in which artworks are viewed and captured by our wearable cameras.</p>
<p>Specifically, paintings that are located close to each other on the wall are usually captured in one shot. Therefore, pictures from wearable cameras often contain more than one paining per image. While it may be hard to determine which paintings in an image are of direct interest to visitors, some paintings are captured more than once, thus increasing a chance of successful identification. Moreover, the number of captures of the same painting potentially correlates with the time and interest dedicated to such a painting. Additionally, some paintings are captured partially, e.g. they are truncated, making recognition even harder. Other practical issues include shadows cast on artworks due to lighting and proximity of the painting to the viewer, as well as motion blur. The layout of the museum imposes some partial order in which paintings are displayed and captured by wearable cameras. According to the path delineated in figure 6a, we see that the visitor&#8217;s route is clear and easy to follow, so that audiences are not likely to miss many artworks. Volunteers who walked around the exhibition wearing cameras tended to stop next to various paintings for various durations of time. Moreover, they could easily avoid revisiting the same artworks, unless they desired to approach some of them again. Note that audiences in this museum are not allowed to touch any artworks, however, they can look at any of the paintings close-up, which may result in a partial capture, i.e. zooming in at a fragment of a painting. The spacious exhibition hall provided good conditions for our volunteers to capture images in a varied manner; some viewers preferred to approach artworks, others just strolled along at a steady pace. Therefore, we were able to collect six testing sets, as detailed in section 2.1.</p>
<h3>3.2 Clocks</h3>
<p>The clocks in the Palace Museum are displayed in cases, under the necessary preservation conditions. Because they are located behind glass, the clocks cannot usually be interacted with through touch or seen from extremely close up; and because it is difficult to get clear shots of artworks due to the low-light interior environment and reflections from the glass surfaces, the photos may be blurry due to overexposure. Moreover, we also noted that it was hard to take shots from acute viewpoints. This complication was due to clocks being located close to each other; taking photos of the rear side of these clocks was often impossible, as only their frontal parts were clearly exposed to the visitors. Of course, this specific constraint on viewpoints seems to have a positive effect in the sense that it limits the number of views an object can be seen from, while the front view remains very distinct. However, three clocks are displayed without any glass case due to their large size, and can be seen by viewers from all sides. These artworks are still protected from audiences by handrails. In this case, the artworks stretch beyond the field of view of the wearable camera, making it difficult to capture good images of entire objects. This is especially undesirable, because if only partial views are being captured, the representations of these artworks are much less distinct.</p>
<p>The Palace Museum is a very popular attraction, with large numbers of tourists visiting everyday. This crowded space resulted in some photos of timepieces that were partially occluded by visitors. Therefore, adverse conditions described above differ from the case outlined in section 3.1 and should affect, to some degree, identification of the artworks. Lastly, the red dotted line in figure 6b illustrates visitor circulation in the Clock and Watch Gallery.</p>
<h3>3.3 Sculptures</h3>
<p>In the Indian and Chinese Sculpture Exhibition Hall, many sculptures are located in the middle of the exhibition space; therefore they are set against a background cluttered by other sculptures. This makes both the annotation of ground truth data and its identification a challenging process, because numerous artworks are often captured at once. The way audiences move in this museum space has a more complex pattern compared to the case study in section 3.2.; volunteers often exhibited counterclockwise movement around the perimeter if they turned right at the entrance, and clockwise movement otherwise. In the hall area with artworks located on both sides, volunteers often followed a zigzag path between these artworks. Moreover, volunteers often circled smaller patches emerging between artworks. Therefore, in this dataset, one cannot expect a clear order in which artworks were captured, or a clear correlation between frequently viewed sculptures and audiences’ preference. Another adverse factor included large-sized sculptures which did not fit well into wearable camera&#8217;s field of vision, occlusions, and poor lighting. In our opinion, such factors make this exhibition space the most challenging for the purpose of capturing images with wearable cameras.</p>
<h3>3.4 Other exhibition spaces</h3>
<p>During the capturing process in the three different art museum spaces, we observed that viewers are actively enjoying the parts of exhibitions they are interested in, while ignoring the others. Using this process, artworks can be identified using scientific tools that give museums the opportunity to re-think the way they communicate, i.e. beyond offering the standard guided tours and fixed exhibitions (Balsamo, 2012). However, each museum space poses unique challenges for artwork identification. For instance, science museum exhibitions often include items which are large and may look very similar to the untrained eye, such as engines, pumps, radio-communication equipment, etc. These items may be bulky, highly non-planar, and not clearly localized; they may emit light, change appearance during interaction, and so forth. Other artworks such as crafts are also likely to be highly non-planar, e.g. miniature replicas of houses, famous buildings, and monuments. Exhibitions with non-rigid objects such as carpet, Gobelin tapestry, and clothing are a further example of artworks of varied nature in the exhibition spaces. Modern art may include objects that lack texture, making them harder to recognize, while porcelain and glass work are likely to be the source of glares. Hieroglyphs, ancient books, and even jewelry may all look similar to a non-expert eye. Exhibits in natural history museums such as birds, insects, butterflies, rodents, etc. may pose similar challenges. These last two are examples of so-called fine grained image recognition (Wah et al., 2011) which requires an algorithm match and expert knowledge about what makes these exhibits differ between many similar items. However, we leave these challenges for future work.</p>
<h2>4. Experiments</h2>
<p>To conclude our work, below we present experimental findings from our study. We separately fine-tuned three VGG16 networks for the paintings, clocks and sculptures, respectively. To achieve this, we followed the augmentation and cross-validation process as detailed in section 2.1. Below, we report results in terms of mean accuracy, which quantifies how many test images on average were assigned labels agreeing with our ground truth annotations. Note that some images annotated by us contained more than one museum item. We assigned ground truth labels to these images in descending order; that is, the central artwork was assigned its ground truth label first while less visible peripheral pieces were assigned their ground truth labels next.</p>
<figure id="attachment_1701" aria-describedby="caption-attachment-1701" style="width: 645px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1701" src="../../wp-content/uploads/2017/01/zhang.fig7_.png" alt="" width="645" height="268" /><figcaption id="caption-attachment-1701" class="wp-caption-text">Figure 7: the Paintings dataset. Figure 7a illustrates accuracy in percents (the higher the better) for each of six testing splits–each collected by a different volunteer. Figure 7b shows the average over the six splits as well as the standard deviation</figcaption></figure>
<p>Figure 7a illustrates performance obtained on the Paintings dataset for each of six testing splits from six volunteers. We count prediction as a valid piece of identification if the predicted label is within<em> top-k</em> ground truth labels (k being a number along axis) assigned by us in the data annotation process. As demonstrated, most of the predictions point to the central pieces in images from wearable cameras; therefore, accuracy improves only marginally as the <em>top-k</em> value increases in the plot. For instance, split sp4 shows no variation w.r.t. the top-k value. However, splits sp3 and sp5 show close to 4% variation. This can be explained by the fact that volunteers who collected the data for these two splits tended to stroll along the exhibition space away from paintings. Therefore, many images collected this way contained several paintings. Figure 7b shows the average performance over the six splits. As demonstrated, due to differences in how volunteers explored the museum space and mounted wearable cameras on their clothing, the standard deviation between results varies by up to +/-6.7%. The best performing split, sp3, scored 51.8% accuracy while the worst performing scored only 33.3% accuracy. This highlights the difficulty in attaining equally good recognition rates for the data from every visitor. The average accuracy for top-1 labels obtained in this experiment is 42.6%, which means that exactly such a portion of all images from wearable cameras were recognized correctly.</p>
<figure id="attachment_1702" aria-describedby="caption-attachment-1702" style="width: 647px" class="wp-caption aligncenter"><img loading="lazy" class=" wp-image-1702" src="../../wp-content/uploads/2017/01/zhang.fig8_.png" alt="" width="647" height="288" /><figcaption id="caption-attachment-1702" class="wp-caption-text">Figure 8: the Clocks and Sculpture datasets are evaluated in Figures 8a and 8b, respectively. The mean accuracy in percents is indicated by the bar plots</figcaption></figure>
<p>Figure 8a shows performance on the Clock dataset for both testing splits. As demonstrated, recognition rates differ by 6.1% between these two testing sets. We suspect this highlights a big difference in how the two volunteers explored this museum space. Another explanation is that recognition is affected by the way visitors mounted wearable cameras. However, we also note that additional ground truth labels (when multiple clocks were visible in an image) turned out to be not needed as the accuracy for larger<em> top-k</em> values (e.g. top-2, . . . ,top-10) increases by up to 3.3%. We suspect that because clocks were located behind protective glass, visitors approached each artwork and explored it up-close. Therefore, wearable cameras were able to obtain clear, well centered pictures of most of the timepieces. The average accuracy for top-1 labels obtained in this experiment is 40.9%, which is slightly below the average accuracy of the Paintings dataset. We note that this dataset constitutes a contrast with the Paintings dataset. We expected that recognition of non-planar artworks behind the protective glass in a darker and more crowded environment would be a harder task; however, the need to approach these pieces helped the cameras capture their clear close-up pictures.</p>
<p>Figure 8b shows the performance on the Sculptures dataset for both testing splits. Firstly, we note that in some cases the difference in accuracy for top-1 vs. top- 10 measure differs by up to 4%. We expect this is due to other sculptures present in the background. The CNN network was very likely unable to distinguish between the central object and other surrounding items. Moreover, we also expect some noise in our ground truth annotations, as sometimes it was not clear which object in an image was the central object approached by the volunteer. Lastly, we note that the average accuracy for top-1 labels obtained in this experiment is only 30.7% which is a drop of over 10% compared to results on the Paintings and Clocks datasets. This highlights a challenge of identifying non-planar artworks in cluttered exhibition spaces.</p>
<p>Because we are interested in identifying artworks that the volunteers interacted with, for each dataset, we asked one of the volunteers to approach all artworks in a given museum space. For paintings, clocks and sculptures, we were able to recognize 36, 54, and 15 distinct paintings, clocks and sculptures out of 79, 113, and 44 distinct art pieces in each exhibition space. This means that the fine-tuned CNN was able to recognize 45.6%, 47.8% and 34.1% of all distinct artworks.</p>
<h2>5. Conclusions</h2>
<p>This work addresses the challenging problem of artwork identification in museum spaces. We have shown that, with state-of-the-art computer vision CNN algorithms, we are able to reliably identify up to half of the artworks that audiences interact with in various museum spaces. We found that our discussion of the challenges posed by the various types of exhibition spaces (and specific artworks) to the capturing and recognition process are indeed reflected by the quantitative results we obtained. It appears that for now, identification of paintings is perhaps the simplest task due, to their planarity. However, non-planar items such as clocks and sculptures pose a somewhat bigger challenge. Above all, this pilot study reveals that the off-the-shelf fine-tuning so popular in computer vision is perhaps still insufficient, and requires a more customized recognition algorithm. Suitable modifications may include a variation of CNN (Mairal et al., 2014) and so-called bag-of-words or domain adaptation approaches (Koniusz &amp; Cherian, 2016; Koniusz &amp; Mikolajczyk; Koniusz et al., 2012, 2016). With just below half of the artworks identified correctly, it may be sufficient to combine the artwork identification module with a recommendation system, though the need for further improvement is clear. In the future, we plan to extend the current dataset to contain pictures from more kinds of exhibition spaces, as well as investigate new classification algorithms.</p>
<h2>References</h2>
<p>Alelis, G., A. Bobrowicz, &amp; C.S. Ang. (2015). &#8220;Comparison of engagement and emotional responses of older and younger adults interacting with 3d cultural heritage artefacts on personal devices.&#8221; <em><i>Behaviour and Information Technology</i></em> 34(11), 1064.</p>
<p>Balsamo, A. (2011). <em><i>Designing culture: The technological imagination at work</i></em>. Durham, NC: Duke University Press, 63:1899.</p>
<p>Baraldi, L., F. Paci, G. Serra, L Benini, &amp; R. Cucchiara. (2007). &#8220;Gesture recognition using wearable vision sensors to enhance visitors’ museum experiences.&#8221; <em><i>Sensors Journal </i></em>15(5), 2705.</p>
<p>Beer, V. (1987). &#8220;Great expectations: Do museums know what visitors are doing?&#8221; <em>Curator </em>30(3), 206-15.</p>
<p>C. Gonzalez R., &amp; R. E. Woods. (2006). <em>Digital Image Processing</em> (3rd Edition). Prentice-Hall, Inc., Upper Saddle River, NJ, USA. ISBN 013168728X.</p>
<p>Chu, B., V. Madhavan, O. Beijbom, J. Hoffman, &amp; T. Darrell. (2016). &#8220;Best practices for fine-tuning visual classifiers to new domains.&#8221; In Proceeding of the <em><i>European Conference on Computer Vision 2016 (ECCV 2016)</i></em>, Part III,  435–42.</p>
<p>Dalens, T., J. Sivic, I. Laptev, &amp; M Campedel. (2014). &#8220;Painting recognition from wearable cameras.&#8221; Available https://hal.inria.fr/hal-01062126.</p>
<p>Koniusz, P. &amp; A. Cherian. (2016). &#8220;Sparse Coding for Third-order Super-symmetric Tensor Descriptors with Application to Texture Recognition.&#8221; <em>Computer Vision and Pattern Recognition (CVPR), 5395.</em></p>
<p>Koniusz, P., F. Yan, P.-H. Gosselin, &amp; K. Mikolajczyk. (2016). &#8220;Higher-order Occurrence Pooling for Bags-of-Words: Visual Concept Detection.&#8221; <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 39(2), 313.</em></p>
<p>Koniusz, P., F. Yan, &amp; K. Mikolajczyk. (2012). &#8220;Comparison of Mid-Level Feature Coding Approaches And Pooling Strategies in Visual Concept Detection.&#8221; <em>Computer Vision and Image Understanding 117(5), 479.</em></p>
<p>Koniusz, P., &amp; K. Mikolajczyk. (2010). &#8220;On a Quest for Image Descriptors Based on Unsupervised Segmentation Maps.&#8221; <em>International Conference on Pattern Recognition (ICPR), 762.</em></p>
<p>Koniusz, P., Y. Tas, &amp; F. Porikli. (2016). &#8220;Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors.&#8221; <em>CoRR, </em>abs/1611.08195.</p>
<p>Krizhevsky, A., I. Sutskever, &amp; G. E. Hinton. (2012). &#8220;ImageNet classification with deep convolutional neural networks.&#8221; <em>Advances in Neural Information Processing Systems (NIPS)</em>, 1106–1114.</p>
<p>Kuflik, T., J. Sheidin, S. Jbara, D. Goren-Bar, P. Soffer,  O. Stock, &amp; M. Zancanaro. (2007). &#8220;Supporting small groups in the museum by context-aware communication services.&#8221; <em>Proceedings of the 2007 International Conference on Intelligent User Interfaces</em>,  305-8. Available http://doi.acm.org/10.1145/1216295.1216353</p>
<p>Mairal, J., P. Koniusz, Z. Harchaoui, &amp; C. Schmid. (2014). &#8220;Convolutional Kernel Net-works.&#8221; <em>Advances in Neural Information Processing Systems (NIPS)</em>. HAL-01005489v2. Available http://nips.cc.</p>
<p>Russakovsky, O., J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, &amp; L. Fei-Fei. (2015). &#8220;ImageNet large scale visual recognition challenge.&#8221; <em>International Journal of Computer Vision (IJCV) </em>115(3), 211–52. DOI: 10.1007/s11263-015-0816-y.</p>
<p>Simonyan, K., &amp; A. Zisserman. (2014). &#8220;Very deep convolutional networks for large-scale image recognition.&#8221; <em><i>CoRR</i></em>, abs/1409.1556.</p>
<p>Suchy, S. (2006). &#8220;Museum management: Emotional value and community engagement.&#8221; <em><i>INTERCOM</i></em>, 354-62.</p>
<p>Wah, C., S. Branson, P. Welinder, P. Perona, &amp; S. Belongie. (2011). &#8220;Caltech-UCSD Birds-200-2011 dataset.&#8221; <em>CVPR Workshop on Fine-Grained Visual Categorization.</em></p>
<p>Zhang, R., &amp; A. Russo. (2015). &#8220;Towards comparative methods for evaluating cross-cultural digital creativity in museum exhibitions.&#8221; <i>MWA2015: Museums and the Web Asia 2015</i>. Published August 9, 2015. Available http://mwa2015.museumsandtheweb.com/paper/towards-comparative-methods-for-evaluating-cross-cultural-digital-creativity-in-museum-exhibitions</p>
<hr>

<script type="text/javascript">
var d=new Date();
var month=new Array();
month[0]="January";
month[1]="February";
month[2]="March";
month[3]="April";
month[4]="May";
month[5]="June";
month[6]="July";
month[7]="August";
month[8]="September";
month[9]="October";
month[10]="November";
month[11]="December";

var day_number=d.getDate();
var month_name=month[d.getMonth()];
var year=d.getFullYear();
var full_date = month_name+" "+day_number+", "+year;
</script>

Cite as:<br /> 

Zhang, Rui, Yusuf Tas and Piotr Koniusz. "Artwork identification from wearable camera images for enhancing experience of museum audiences." <i>MW17: MW 2017</i>. Published February 1, 2017. Consulted <script type="text/javascript">document.write(full_date);</script>.<br />
https://mw17.mwconf.org/paper/exhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences/<br /><br /><hr>

<div id="comments" class="comments-area">

			<h2 class="comments-title">
			2 thoughts on &ldquo;Artwork identification from wearable camera images for enhancing experience of museum audiences&rdquo;		</h2>

		
		<ol class="comment-list">
					<li id="comment-2" class="comment byuser comment-author-renata-zhang bypostauthor even thread-even depth-1">
			<article id="div-comment-2" class="comment-body">
				<footer class="comment-meta">
					<div class="comment-author vcard">
						<img alt='' src='https://secure.gravatar.com/avatar/73c0a8bb4086c4467d435719abd1d625?s=42&amp;d=mm&amp;r=g' srcset='https://secure.gravatar.com/avatar/73c0a8bb4086c4467d435719abd1d625?s=84&#038;d=mm&#038;r=g 2x' class='avatar avatar-42 photo' height='42' width='42' loading='lazy'/>						<b class="fn">Rui Zhang</b> <span class="says">says:</span>					</div><!-- .comment-author -->

					<div class="comment-metadata">
						<a href="index.html#comment-2"><time datetime="2017-02-20T14:05:03+00:00">February 20, 2017 at 2:05 pm</time></a>					</div><!-- .comment-metadata -->

									</footer><!-- .comment-meta -->

				<div class="comment-content">
					<p>Dear editor, I have re-edit the paper due to your suggestions. There were some data errors of the previous edition, so we have to correct. And can you figure out what citations are missing? I think we have completed all citations.</p>
				</div><!-- .comment-content -->

				<div class="reply"><a rel="nofollow" class="comment-reply-login" href="../../wp-login6ac3.html?redirect_to=https%3A%2F%2Fmw17.mwconf.org%2Fpaper%2Fexhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences%2F">Log in to Reply</a></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		<li id="comment-3" class="comment byuser comment-author-renata-zhang bypostauthor odd alt thread-odd thread-alt depth-1">
			<article id="div-comment-3" class="comment-body">
				<footer class="comment-meta">
					<div class="comment-author vcard">
						<img alt='' src='https://secure.gravatar.com/avatar/73c0a8bb4086c4467d435719abd1d625?s=42&amp;d=mm&amp;r=g' srcset='https://secure.gravatar.com/avatar/73c0a8bb4086c4467d435719abd1d625?s=84&#038;d=mm&#038;r=g 2x' class='avatar avatar-42 photo' height='42' width='42' loading='lazy'/>						<b class="fn">Rui Zhang</b> <span class="says">says:</span>					</div><!-- .comment-author -->

					<div class="comment-metadata">
						<a href="index.html#comment-3"><time datetime="2017-02-20T14:57:08+00:00">February 20, 2017 at 2:57 pm</time></a>					</div><!-- .comment-metadata -->

									</footer><!-- .comment-meta -->

				<div class="comment-content">
					<p>I have re-editted the references as well.</p>
				</div><!-- .comment-content -->

				<div class="reply"><a rel="nofollow" class="comment-reply-login" href="../../wp-login6ac3.html?redirect_to=https%3A%2F%2Fmw17.mwconf.org%2Fpaper%2Fexhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences%2F">Log in to Reply</a></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
		</ol><!-- .comment-list -->

		
	
	
		<div id="respond" class="comment-respond">
		<h2 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="index.html#respond" style="display:none;">Cancel reply</a></small></h2><p class="must-log-in">You must be <a href="../../wp-login6ac3.html?redirect_to=https%3A%2F%2Fmw17.mwconf.org%2Fpaper%2Fexhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences%2F">logged in</a> to post a comment.</p>	</div><!-- #respond -->
	
</div><!-- .comments-area -->
</article>
</main>
<div id="mw-sidebar" class="mw-sidebar25">

	<aside id="secondary" class="sidebar widget-area" role="complementary">
		<section id="text-7" class="widget widget_text">			<div class="textwidget"><h2 class="widget-title">Community Conveners</h2>
<p><a title="Wezit" href="https://wezit.io/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/wezit-250-75.jpg" alt="Wezit" /></a><br />
<a title="Axiell" href="http://www.axiell-alm.com/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" style="border: 0px;" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/axiell-250-75.jpg" alt="Axiell" /></a></p>
<h2 id="gold-sponsor" class="widget-title">Platinum Sponsor</h2>
<p><a title="Microsoft" href="http://aka.ms/museweb" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/Microsoft-250-75.jpg" alt="Microsoft" /></a><br />
<!-- <a title="Izi Travel" href="http://www.izi.travel" target="_blank" rel="noopener"><img src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/izi-250-75.jpg" alt="Izi Travel" class="sponsor-img sponsor-top" /></a> --></p>
<h2 id="gold-sponsor" class="widget-title">Gold Sponsor</h2>
<p><a title="Piction" href="http://www.piction.com/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/piction-250-75.jpg" alt="Piction" /></a></p>
<h2 id="bronze-sponsor" class="widget-title">Bronze Sponsors</h2>
<p><a title="MailChimp" href="https://mailchimp.com/resources/guides/mailchimp-for-nonprofits/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://mw2015.museumsandtheweb.com/wp-content/themes/mw-theme/mailchimp-250-75.jpg" alt="mailchimp" /></a><br />
<a title="Netx" href="http://netx.net/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/netx-250-75.gif" alt="Netx" /></a><br />
<a title="PatronTech" href="http://patrontechnology.com/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/patron-250-75.jpg" alt="PatronTech" /></a></p>
<h2 id="conference-sponsor" class="widget-title">Conference Sponsors</h2>
<p><a title="CMA" href="http://clevelandart.org/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/cma-250-75.jpg" alt="CMA" /></a><br />
<a title="Rock &amp; Roll" href="http://www.rockhall.com/" target="_blank" rel="noopener"><img class="sponsor-img sponsor-top" src="https://www.museumsandtheweb.com/wp-content/themes/mw-theme/rrhf-250-75.jpg" alt="Rock &amp; Roll" /></a></p>
</div>
		</section>	</aside><!-- .sidebar .widget-area -->
</div>
</div>

		</div><!-- TAG OPENS IN HEADER.PHP (.site-content) -->

		<footer id="colophon" class="site-footer" role="contentinfo">

			<div class="site-info">
				<div class="site-title">Founded by Archives &amp; Museum Informatics<br><a href="http://www.archimuse.com/" target="_blank">www.archimuse.com</a></div>
				<div class="site-title right">Managed by <a href="http://www.mwconf.org/">Museums and the Web LLC</a><br> 703 Dale Drive Silver Spring MD 20910 USA<br><a href="mailto:info@mwconf.org">info@mwconf.org</a></div>
				
				
			</div><!-- .site-info -->
		</footer><!-- .site-footer -->

	</div><!-- TAG OPENS IN HEADER.PHP (.site-inner) -->
</div><!-- TAG OPENS IN HEADER.PHP (.site) -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-26332456-1"></script>
<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-26332456-1', {
  		'linker': {
    	'domains': ['museweb.net', 'mwconf.org', 'museumsandtheweb.com']
  }
});
</script>

<script src='../../wp-content/themes/twentysixteen/js/skip-link-focus-fixfd15.js?ver=20170530' id='twentysixteen-skip-link-focus-fix-js'></script>
<script src='../../wp-includes/js/comment-reply.min40df.js?ver=5.6' id='comment-reply-js'></script>
<script id='twentysixteen-script-js-extra'>
var screenReaderText = {"expand":"expand child menu","collapse":"collapse child menu"};
</script>
<script src='../../wp-content/themes/twentysixteen/js/functionsb889.js?ver=20181217' id='twentysixteen-script-js'></script>
<script src='../../wp-includes/js/wp-embed.min40df.js?ver=5.6' id='wp-embed-js'></script>
</body>

<!-- Mirrored from mw17.mwconf.org/paper/exhibit-identification-from-wearable-camera-images-for-enhancing-experience-of-museum-audiences/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 23 Apr 2022 22:09:38 GMT -->
</html>
