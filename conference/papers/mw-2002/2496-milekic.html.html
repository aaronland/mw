<html><!-- #BeginTemplate "/Templates/mw2002-papers.dwt" --><!-- DW6 -->

<!-- Mirrored from www.museumsandtheweb.com/mw2002/papers/milekic/milekic.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:48:13 GMT -->
<head>
<!-- #BeginEditable "doctitle" -->

	<title>MW2002: Papers: Towards Tangible Virtualities: Tangialities</title>

	<meta http-equiv="Content-Type" content="text/html; charset=en">

	<meta name="DC.Format" content="text/html">

	<meta name="DC.language" content="ISO 8859-1">

	<meta name="DC.Title" content="Towards Tangible Virtualities: Tangialities">

	<meta name="htDig.keywords" content="interaction devices, gesture, touch, digital medium, humane

interaction,  virtual information, interaction technologies">

	<meta name="keywords" content="interaction devices, gesture, touch, digital medium, humane

interaction,  virtual information, interaction technologies">

	<meta name="DC.Subject" content="interaction devices, gesture, touch, digital medium, humane

interaction,  virtual information, interaction technologies">

	<meta name="description" content="Rapid proliferation of different types of interaction devices that use more natural channels (voice, touch, gesture) for interfacing with the digital medium illustrates the trend (and need) towards the creation of more 'humane' interaction mechanisms. However, the current historical paradox is that modern technological advances are dramatically ahead of our understanding of their possible uses and meaning on a conceptual level.

In this paper I will present an overview of some of the currently available interaction technologies, the conceptual barriers that limit their use and the case for the creation of interaction mechanisms that make abstract (virtual) information more tangible.

">

	<meta name="DC.Description" content="Rapid proliferation of different types of interaction devices that use more natural channels (voice, touch, gesture) for interfacing with the digital medium illustrates the trend (and need) towards the creation of more 'humane' interaction mechanisms. However, the current historical paradox is that modern technological advances are dramatically ahead of our understanding of their possible uses and meaning on a conceptual level.

In this paper I will present an overview of some of the currently available interaction technologies, the conceptual barriers that limit their use and the case for the creation of interaction mechanisms that make abstract (virtual) information more tangible.

">

	<meta name="DC.Publisher" content="Archives & Museum Informatics">

	<meta name="DC.Creator" content="Milekic, Slavko"><!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset="><!-- #BeginEditable "script" --><!-- #EndEditable -->
<script language="JavaScript">
<!--
function MM_swapImgRestore() { //v3.0
  var i,x,a=document.MM_sr; for(i=0;a&&i<a.length&&(x=a[i])&&x.oSrc;i++) x.src=x.oSrc;
}

function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.01
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && d.getElementById) x=d.getElementById(n); return x;
}

function MM_swapImage() { //v3.0
  var i,j=0,x,a=MM_swapImage.arguments; document.MM_sr=new Array; for(i=0;i<(a.length-2);i+=3)
   if ((x=MM_findObj(a[i]))!=null){document.MM_sr[j++]=x; if(!x.oSrc) x.oSrc=x.src; x.src=a[i+2];}
}
//-->
</script>
<link rel="stylesheet" href="../../Library/mw2002-paper.css" type="text/css">
</head>

<body bgcolor="#FFFFFF" background="../../images/mw2002.bg.gif" text="#000000" link="#003399" vlink="#660000" onLoad="MM_preloadImages('../../images/register_on.gif','../../images/workshops_on.gif','../../images/sessions_on.gif','../../images/speakers_on.gif','../../images/interact_on.gif','../../images/demos_on.gif','../../images/exhibit_on.gif','../../images/events_on.gif','../../images/best_on.gif','../../images/dates_on.gif','../../images/boston_on.gif','../../images/sponsors_on.gif')">
<table width="600" border="0" cellspacing="2" cellpadding="5">
  <tr> 
    <td width="145" align="LEFT" valign="TOP"> 
      <p><a href="../../index.html"><img src="../../images/mw.gif" width="112" height="155" border="0" alt="/mw/"></a></p>
      <p> <a href="../../register/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('register','','../../images/register_on.gif',1)"><img name="register" border="0" src="../../images/register_off.gif" width="114" height="18" alt="Register"></a><a href="../../workshops/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('workshops','','../../images/workshops_on.gif',1)"><img name="workshops" border="0" src="../../images/workshops_off.gif" width="114" height="18" alt="Workshops"></a><a href="../../sessions/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('sessions','','../../images/sessions_on.gif',1)"><img name="sessions" border="0" src="../../images/sessions_off.gif" width="114" height="18" alt="Sessions"></a><a href="../../speakers/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('speakers','','../../images/speakers_on.gif',1)"><img name="speakers" border="0" src="../../images/speakers_off.gif" width="114" height="18" alt="Speakers"></a><a href="../../interact/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('interactions','','../../images/interact_on.gif',1)"><img name="interactions" border="0" src="../../images/interact_off.gif" width="114" height="18" alt="Interactions"></a><a href="../../demos/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('demonstrations','','../../images/demos_on.gif',1)"><img name="demonstrations" border="0" src="../../images/demos_off.gif" width="114" height="18" alt="Demonstrations"></a><a href="../../exhibit/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('exhibits','','../../images/exhibit_on.gif',1)"><img name="exhibits" border="0" src="../../images/exhibit_off.gif" width="114" height="18" alt="Exhibits"></a><a href="../../events/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('events','','../../images/events_on.gif',1)"><img name="events" border="0" src="../../images/events_off.gif" width="114" height="18" alt="Events"></a><a href="../../best/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('best','','../../images/best_on.gif',1)"><img name="best" border="0" src="../../images/best_off.gif" width="114" height="18" alt="Best of the Web"></a><a href="../../dates/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('dates','','../../images/dates_on.gif',1)"><img name="dates" border="0" src="../../images/dates_off.gif" width="114" height="18" alt="Key Dates"></a><a href="../../boston/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('boston','','../../images/boston_on.gif',1)"><img name="boston" border="0" src="../../images/boston_off.gif" width="114" height="18" alt="Boston"></a><a href="../../sponsor/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('sponsors','','../../images/sponsors_on.gif',1)"><img name="sponsors" border="0" src="../../images/sponsors_off.gif" width="114" height="18" alt="Sponsors"></a><br>
        <br>
        <a href="../http://www.archimuse.com/" target="_top"><img src="../../images/nav_ami.gif" width="135" height="25" border="0" alt="A&amp;MI home"></a> 
        <br>
        <span class="small">Archives &amp; Museum Informatics<br>
        158 Lee Avenue<br>
        Toronto, Ontario<br>
        M4E 2P3 Canada</span></p>
      <p class="small">info @ archimuse.com<br>
        <a href="../http://www.archimuse.com/" style="text-decoration: underline">www.archimuse.com</a></p>
      <table width="74">
        <tr> 
          <td> <a href="../http://search.museumsandtheweb.com/search" target="_top"> <img src="../../images/search.gif" width="24" height="25" alt="Search" border="0" name="Search"></a> 
          </td>
          <td valign="MIDDLE"> <a href="../http://search.museumsandtheweb.com/search" style="text-decoration: underline"> 
            <span class="verysmall">Search<br></span></a> </td>
        </tr>
      </table>
      <p><font face="Arial, Helvetica, sans-serif" class="verysmall"><span class="small">Join 
        our <a href="../http://search.museumsandtheweb.com/mailinglist/" style="text-decoration: underline"> 
        Mailing List</a>. <br>
        <a href="../http://search.museumsandtheweb.com/terms-of-use-privacy/" style="text-decoration: underline"> 
        Privacy</a>.</span></font> </p>
       <p><font SIZE='-2' class='verysmall'>published: April, 2002 </font>
        <!--

document.write("<FONT SIZE='-2' class='verysmall'>"+"analytics scripts updated:&nbsp;"+document.lastModified);

// -->
      </p>
   <p><font face="Arial, Helvetica, sans-serif" class="small">© Archives & Museum Informatics, 2002.</font><br>
  <a rel="license" href="../http://creativecommons.org/licenses/by-nc-nd/3.0/"><img src="../http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png" alt="Creative Commons Att
   ribution-Noncommercial-No Derivative Works 3.0  License" width="88" height="31" style="border-width:0" /></a>      </p>
    </td>
    <td width="455" align="LEFT" valign="TOP" class="normal">
      <p><a href="../../speakers/index.html"><img src="../../images/papers.gif" width="390" height="55" border="0" alt="MW2002: Papers"></a></p>
     <!-- #BeginEditable "Body of Page" --> 

      <p class=PaperTitle>Towards Tangible Virtualities: Tangialities</p>

      <p class=Author>Slavko Milekic, M.D., PhD, The University of the Arts, Philadelphia, 

        USA</p>

      <p class=AbstractTitle>Abstract</p>

      <p class=AbstractText>Rapid proliferation of different types of interaction 

        devices that use more natural channels (voice, touch, gesture) for interfacing 

        with the digital medium illustrates the trend (and need) towards the creation 

        of more 'humane' interaction mechanisms. However, the current historical 

        paradox is that modern technological advances are dramatically ahead of 

        our understanding of their possible uses and meaning on a conceptual level.</p>

      <p class=AbstractText>In this paper I will present an overview of some of 

        the currently available interaction technologies, the conceptual barriers 

        that limit their use and the case for the creation of interaction mechanisms 

        that make abstract (virtual) information more tangible.</p>

      <h1>Introduction</h1>

      <p class=MsoBodyText>The meanings associated with the adjective &quot;tangible&quot; 

        in an on-line version of the Merriam-Webster dictionary (see references 

        for URL) include:</p>

      <p class=BlockQuote><b>1</b> <b>a</b> : capable of being perceived especially 

        by the sense of touch : <b>PALPABLE</b> <b>b</b> : substantially real 

        : <b>MATERIAL</b></p>

      <p class=BlockQuote><b>2</b> : capable of being precisely identified or 

        realized by the mind &lt;her grief was tangible&gt;</p>

      <p class=MsoBodyText>The listed synonym for &quot;tangible&quot; is PERCEPTIBLE, 

        which in turn, has the following synonyms: SENSIBLE, PALPABLE, TANGIBLE, 

        APPRECIABLE, PONDERABLE. The evolution of the term, starting with sense 

        percepts related to the sense of touch and ending with precise mental 

        identification and realization of abstract concepts (like 'grief' in the 

        definition above), corresponds, more or less, to my view on this topic. 

        In this paper, I would like to make a case that association of virtual 

        and abstract information with multimodal sensory experiences creates a 

        new layer of knowledge and action spaces that is more natural and efficient 

        for humans. These in-between domains, where interactions with virtual 

        data produce tangible sensations, I dubbed tangialities (see Figure 1.). 

      </p>

      <p class=MsoCaption><img src="milekicFig1.gif" width="441" height="146" alt="Representation of different action/knowledge "><br>

        <a name="fig1"></a>Figure 1. Representation of different action/knowledge 

        domains </p>

      <p class=MsoBodyText>Please note that the way I define the term tangiality 

        includes all sensory modalities and is not reduced just to those related 

        to the sense of touch (haptic, cutaneous, tactile). </p>

      <p class=MsoBodyText>Our body may be considered as the first interface between 

        ourselves and the real world. The interactions were guided by our goals 

        (intentions), carried out through actions, and repeated or corrected based 

        on perception of the consequences of actions (observations).</p>

      <p class=MsoBodyText>Body actions were soon enhanced through the use of 

        tools (artifacts). Norman ( Norman 1991) introduced the concept of a cognitive 

        artifact, as a tool that enhances cognitive operations. Although the enhancement 

        of body actions is sometimes achieved by sheer magnification (using a 

        lever, or inclined plane), the enhancement of cognitive operations is 

        most often the consequence of <i>changing the nature of the task</i>. 

        An example of a cognitive artifact that enhances our ability to memorize 

        and recall events is a personal calendar. Instead of trying to rehearse 

        and memorize all of the events for weeks to come, we have to remember 

        only to write them down into the calendar, and to consult the calendar 

        every day. In the context of tangialities, cognitive enhancements are 

        also the consequence of changing the nature of the task. Most often a 

        change is in shift from relying on formal, abstract operations as a means 

        of gaining knowledge, to direct manipulation of data (properties) with 

        instantaneously observable results. For example, in order to answer the 

        problem illustrated in Figure 2., &#147;are the dimensions of the smaller 

        cubes exactly one-half of the larger one?&#148; we may use conventional 

        knowledge of algebra and solve the problem. Direct manipulation approach 

        would be just to juxtapose two smaller cubes next to the larger one and 

        the answer becomes self-evident. Note that the formal solution can be 

        made harder by choosing different dimensions (for example, the height 

        of the big cube could be 8.372914) but this does not influence the direct 

        manipulation solution. </p>

      <p class=MsoCaption><img src="milekicFig2.GIF" width="419" height="71" alt="Getting the result using abstract operation or direct manipulation"><br>

        <a name="fig2"></a> Figure 2. Getting the result using abstract operation 

        or direct manipulation</p>

      <p class=MsoBodyText>I would like to add another word of clarification. 

        For the purposes of this paper, I am not going to address a very fruitful 

        area of research often referred to as &quot;tangible interfaces&quot; 

        (for example, see Ullmer, B., Ishii, H. 2000, and Patten, J., Ishii, H., 

        Hines, J., Pangaro, G. 2001). The cornerstone of this approach is in using 

        real objects with desired physical (manipulable) properties as data representations/containers. 

        These objects embody computation regardless of whether they are connected 

        to a computer or not. Although the areas of development of tangible interfaces 

        and tangialities overlap, and will probably merge in the future, in this 

        paper I will focus on procedures that endow data representations with 

        tangible properties and thus make manipulations carried out on data available 

        to our senses.</p>

      <h1>Direct Manipulation</h1>

      <p class=MsoBodyText>The first human-computer interfaces were abstract, 

        efficient and accessible only to expert users. They involved learning 

        the vocabulary and syntax of a command language which was then used to 

        initiate some operations on the digitally stored data, and often one needed 

        to issue a separate command to see the results of the previous one. There 

        was no continuity of interaction - once the command was issued, there 

        was no way of interfering with the process (short of aborting it). There 

        was also no sensory feedback that would provide relevant information about 

        the operation on an experiential level.</p>

      <p class=MsoBodyText>One of the first examples of a tangiality domain was 

        the introduction of Graphical User Interface (GUI) and the concept of 

        direct manipulation. &quot;Direct manipulation&quot;, a somewhat misleading 

        term, was introduced by Ben Shneiderman in 1983 to describe what we take 

        today to be an integral part of human-computer interaction - the use of 

        a mouse (cursor) for pointing at and manipulating graphically represented 

        objects. The crucial characteristics of direct manipulation are: a) continuous 

        visibility of the manipulated object; b) all the actions carried out on 

        the objects are rapid, incremental and reversible; and c) the consequences 

        of actions are immediately visible (Shneiderman, 1983, 1998). </p>

      <p class=MsoBodyText>What makes direct manipulation a tangiality domain 

        is the fact that it provided continuous sensory input (visual and kinesthetic 

        feedback from hand-on-mouse positions) while acting on abstract parameters 

        (like location coordinates, adjacency, parallelism) of digitally represented 

        data. In spite of the fact that the output in direct manipulation depended 

        on a single sense (vision), it truly revolutionized human-computer interaction. 

      </p>

      <p class=MsoCaption><img src="milekicFig3.gif" width="376" height="156" alt="&quot;Direct manipulation&quot;. Notice that in spite of the term the movements of the mouse are (indirectly) mapped to the screen coordinates."><br>

        <a name="fig3"></a> Figure 3. &quot;Direct manipulation&quot;. Notice 

        that in spite of the term the movements of the mouse are (indirectly) 

        mapped to the screen coordinates. </p>

      <p class=MsoBodyText>Suddenly, anyone who could see and make hand movements 

        could use the computer. However, it is the very success of the direct 

        manipulation paradigm that is now one of the obstacles to creating even 

        more efficient interfaces.</p>

      <h1>Problems with the Traditional Interface</h1>

      <p class=MsoBodyText>As Malcolm McCullough aptly put it in his book &quot;Abstracting 

        Craft&quot; (McCullough, 1996), one of the problems with the traditional 

        &quot;point-and-click&quot; interface was the increased separation of 

        the hand and the eye. In performing operations on digital data, the eye 

        was given a mayor role of identifying, focusing, monitoring and interpreting, 

        while the hand was reduced to performing simple repetitive gestures. The 

        fact that the rising number of repetitive motion injuries is associated 

        with individuals working with computers indicates that this analysis is 

        not just a handy metaphor. Shifting the control to the eye did not bring 

        any benefits either. Besides its physiological role in finding and interpreting 

        visual clues, in the traditional GUI the eye is forced to play the role 

        of the white cane of the blind for the hand. All of the cursor guidance 

        and positioning, often demanding a single pixel precision (in graphic 

        programs), is done under the guidance of the eye. This leads to overstraining 

        of this sensory channel to the point that computer operators &#145;forget&#146; 

        to blink, ultimately developing the chronic dry conjunctivitis of the 

        eyes.</p>

      <p class=MsoBodyText>In our other daily activities we rarely depend on single-sense 

        feedback. Take, for example, the simple act of putting a pencil on the 

        table. Although the eyes are involved, their role is more general &#150; 

        seeing whether the surface of the table is within our reach, and if it 

        is clear of other objects. The actual act of putting the pen on the surface 

        is guided more by cutaneous and proprioceptive clues, and augmented by 

        discrete but definite auditory cues. This multimodal and complementary 

        feedback is the reason our daily actions do not cause over-straining of 

        any particular sense.</p>

      <h1>Introducing Multimodal Interaction</h1>

      <p class=MsoBodyText>Although the term multimodal interaction encompasses 

        both input and output, I will focus more on ways of making the output 

        in human-computer interaction more tangible by using different sensory 

        channels. This should not be taken as an indication that multimodal input 

        (for example, using both speech and gestures) is of lesser importance. 

        It just introduces another level of complexity that goes beyond the topic 

        of this paper.</p>

      <p class=MsoBodyText>The value of multimodal output (feedback) was recognized 

        in HCI design and is currently most widely-spread as coupling of actions 

        to sounds. Sound production is a standard part of modern computer systems, 

        and it is astonishing that the value of consistent sound feedback was 

        not recognized earlier and integrated into interface design guidelines. 

        By coupling actions to sounds I do not mean often exotic &quot;sound schemes&quot; 

        featuring drum-rolls for window closings and alarm clock sounds for warning 

        messages. An example of consistent auditory feedback is a discrete &quot;click&quot; 

        associated with opening of a new window in Microsoft Internet Explorer. 

        The sound is so discrete that many users don't consciously perceive it, 

        yet immediately notice its absence when they switch to another browser. 

      </p>

      <p class=MsoBodyText>Another indicator of this trend is the introduction 

        of a mouse (Logitech, iFeel &#153; mouse) that allows the user to &quot;feel&quot; 

        different objects (for example, folders) and actions (dragging, scrolling) 

        in the traditional GUI interactions. Haptic feedback is provided by a 

        vibro-tactile unit in the mouse and can be finely tuned to fit individual 

        preferences. Although the additional information initially seems trivial 

        and meager (a series of vibration patterns), it becomes very quickly evident 

        that it significantly increases the comfort of interactions (Figure 4.). 

        With complementary information about cursor location or action, the user 

        does not have to rely as much on an already over strained sense of vision. 

        The difference in the experience can be compared to the difference between 

        typing on a standard keyboard where every key-press is accompanied by 

        a tactile, kinesthetic and auditory cue, and typing using keyboards where 

        the keys are outlined on a touch-sensitive surface and provide no specific 

        feedback. Numbers seem to confirm the benefits to the user of haptic technologies 

        &#150; in the first year they were introduced Logitech sold a quarter 

        of a million of iFeel&#153; mice (quoted from Immersion TouchSense&#153; 

        Web site).</p>

      <p class=MsoCaption><img src="milekicFig4.gif" width="464" height="257" alt="Movement of the iFeel mouse (cursor) over the table cells is augmented by haptic output produced by vibro-tactile unit in the mouse. Although the stimulus is fairly discreet it significantly reduces the load of the visual sense during table input or selections using pull-down menus."><br>

        <a name="fig4"></a>Figure 4. Movement of the iFeel mouse (cursor) over 

        the table cells is augmented by haptic output produced by vibro-tactile 

        unit in the mouse. Although the stimulus is fairly discreet it significantly 

        reduces the load of the visual sense during table input or selections 

        using pull-down menus. </p>

      <p class=MsoBodyText>Manifest increase in the quality of interaction experience 

        is making the devices that use haptic or force-feedback a standard in 

        the area of computer games. A quote from Briggs and Srinivasan illustrates 

        the use of haptic interfaces in PC game playing:</p>

      <p class=BlockQuote>Active haptic interfaces can improve a user&#146;s sense 

        of presence: Haptic interfaces with 2 or fewer actuated degrees of freedom 

        are now mass-produced for playing PC videogames, making them relatively 

        cheap (about US$100 at the time of this writing), reliable, and easy to 

        program. Although the complexity of the cues they can display is limited, 

        they are surprisingly effective communicators. For example, if the joystick 

        is vibrated when a player crosses a bridge (to simulate driving over planks) 

        it can provide a landmark for navigation, and signal the vehicle&#146;s 

        speed (vibration frequency) and weight (vibration amplitude).<br>

        <span class="BlockQuoteAuthor">(Briggs and Srinivasan, 2001)</span></p>

      <p class=MsoBodyText>An important area where multimodal sensory feedback 

        plays a crucial role is the area of affective computing. The pioneer of 

        research in affective computing, Rosalind Picard (Picard, 1997) suggests 

        a number of possible applications where the affective state of a human 

        user becomes accessible to a computer, or another remote user. One of 

        the applications is TouchPhone, developed by Jocelyn Sheirer (described 

        in Picard, 2000), where the pressure that a participant in a phone conversation 

        applies to the headset is transmitted to other party's computer screen 

        as a color range - blue corresponding to slight pressure and red corresponding 

        to the maximum pressure value. Inspired by TouchPhone I dared imagine 

        a more sophisticated model where haptic information is transmitted back 

        and forth by holding the other party's simulated &quot;hand&quot; while 

        carrying on the conversation (Figure 5.).</p>

      <p class=MsoBodyText>Besides making abstract data manipulations tangible 

        and accessible to humans and providing a channel for affective communication, 

        using complementary sensory feedback to illustrate complex physical interactions 

        is becoming a method of choice, especially for getting feedback while 

        operating complex machinery or vehicles. The paradox here is that the 

        human operator is most often not directly exposed to the relevant physical</p>

      <p class=MsoCaption><img src="milekicFig5.GIF" width="320" height="240" alt="By &quot;holding hands&quot; one can carry out both verbal and non-verbal parts of a conversation. It remains to be seen whether the actual emotional &quot;value&quot; to the users is high enough for this invention to be economically viable. (photo and hand simulation by S. Milekic, modeled by M. Lengauer)"><br>

        <a name="fig5"></a>Figure 5. By &quot;holding hands&quot; one can carry 

        out both verbal and non-verbal parts of a conversation. It remains to 

        be seen whether the actual emotional &quot;value&quot; to the users is 

        high enough for this invention to be economically viable. (photo and hand 

        simulation by S. Milekic, modeled by M. Lengauer)</p>

      <p>changes, and these are made available by <i>translating</i> numerical 

        data into a sensory experience readily interpretable by a human. The value 

        of adding additional <i>simulated</i> sensory information to the real 

        world task is beautifully illustrated by an example provided by cognitive 

        scientist David Kirsh:</p>

      <p class=BlockQuote>This odd situation which digital technology creates 

        is nicely portrayed by the way modern airplanes rely on simulations of 

        the feel of flying to improve the control of pilots. Apparently, jets 

        fly faster if their center of mass is moved closer to the plane&#146;s 

        nose, thereby changing the relative position of the center of mass with 

        respect to the center of lift. The trouble is that in moving the center 

        of mass forward there is an increased chance that the plane will tip into 

        a nose dive. To keep the plane flying on this knife edge the speed and 

        sensitivity of adjustments is so great that pilots can no longer use mechanical 

        means to control their planes. To assure fast enough response such jets 

        now rely on digital networks to relay a simulated feel to the pilots. 

        When a pilot pulls up on his steering wheel the computers inside the plane 

        simulate the resistance of the ailerons delivering to the pilot the haptic 

        information he or she needs to know what they are doing. Small computer 

        adjustments augment and speed up these pilot reactions. To the pilot this 

        force feedback is an integral part of the way he or she flies the plane. 

        But, of course, there is no true resistance in the steering wheel. Pulling 

        harder on the wheel is just a way of sending the number 7 to the wing 

        actuators instead of the number 5. Computation is so irremediably built 

        into planes that pilots could be in simulators. <span class="BlockQuoteAuthor">(Kirsh, 

        2001)</span></p>

      <p class=MsoBodyText>There is yet another area, academically not that well 

        researched, that readily embraced (no pun intended) the prospect of multisensory 

        interaction. This is the vast domain of on-line sex. As NBC reports, there 

        exist already a number of (multisensory) products that belong to the new 

        &#147;cyberdildonics&#148; area, as well as full &#147;cyber sex suits&#148; 

        that allow a wide variety of tactile sensations to be experienced on strategic 

        body parts (Brunker, MSN NBC online).</p>

      <h1>Barriers</h1>

      <p class=MsoBodyText>While it may seem that all arguments are in favor of 

        building tangialities, it is worth investigating the barriers and problems 

        associated with this approach. As I see them, there are some closely related 

        general problems that are listed here separately only for the sake of 

        clarity. These are:</p>

      <ul>

        <li class="normal"><span class="normal">success of &quot;drag-and-drop&quot; 

          and &quot;point-and-click&quot; interface;</span></li>

        <li class="normal"><span class="normal">failure to realize that changing 

          to, or addition of, another interaction device calls for redesign of 

          the GUI;</span></li>

        <li class="normal"><span class="normal">need to un-learn established (imposed) 

          conventions.</span></li>

      </ul>

      <p class=MsoBodyText>Although it may seem to be a paradox that an early 

        solution would be blocking the introduction of more advanced ones, historically 

        this is a common occurrence. The QWERTY keyboard arrangement became standard 

        for typewriters, and continued to be used even for computer keyboards 

        where any conceivable keyboard layout is equally accessible. In the same 

        way, the very success of mouse/cursor point-and-click interface made it 

        a de facto standard closely associated with the very concept of what it 

        means to interact with a computer. This standardized notion of what an 

        interface should look like affected adversely the introduction of any 

        new interaction device. For example, using a touchscreen as an interaction 

        device while preserving the traditional GUI creates huge problems for 

        the user because of the discrepancy in scale of objects necessary for 

        comfortable interaction. An icon with dimension of 8 x 8 pixels; for example, 

        a window closing box, is perfectly acceptable for a single pixel active 

        tip of the cursor, but is definitely inappropriate for finger interaction. 

        In environments adequately scaled for finger interactions, touchscreens 

        have been shown to be superior pointing devices (Sears, Shneiderman, 1991). 

        The same holds true for other interaction devices - introduction of continuous 

        speech recognition calls for adequate feedback that verbal information 

        has been successfully transmitted. The use of haptic mice and joysticks 

        introduces the texture as a GUI design element, etc. </p>

      <p class=MsoBodyText>Another general problem that has to be taken into account 

        when introducing new ways of interaction is the need for un-learning of 

        adopted conventions. This can be a very slow process, and a transitional 

        stage should be a part of the design of any new convention (and lack thereof 

        is often the reason for their failure). </p>

      <p class=MsoBodyText>More specific problems with the introduction of multimodal 

        feedback come from our lack of knowledge of the complexities of multimodal 

        interaction. Just adding another channel to human-computer interaction 

        is not by default beneficial. A logical and commonsensical analysis tells 

        us that additional information presented through another channel (like 

        any other information) may fall into the following categories:</p>

      <ul>

        <li class="normal"><span class="normal">conflicting</span></li>

        <li class="normal"><span class="normal">competing</span></li>

        <li class="normal"><span class="normal">redundant</span></li>

        <li class="normal"><span class="normal">complementary</span></li>

      </ul>

      <p class=MsoBodyText>In the above list, only the last category has a beneficial 

        effect on interaction. In the following paragraphs I will provide examples 

        of different types of multimodal information.</p>

      <h2>Conflicting</h2>

      <p class=MsoBodyText><i>Conflicting information</i> is often the consequence 

        of hardware/bandwidth limitations as is the case with lack of audio/video 

        synchronization in streamed Web- videos. Sometimes it is a product of 

        poor design; for example, when an animated character's mouth movements 

        are inappropriate for actual utterances. Conflicting information is sometimes 

        purposefully designed into an application. An example is some Web site 

        designs which try to keep the user &quot;glued&quot; to the site in order 

        to artificially boost ratings.</p>

      <h2>Competing</h2>

      <p class=MsoBodyText>An example of <i>competing</i> multimodal information 

        is a voice overlay that is not synchronized to the printed text one is 

        trying to read. Animated graphics (Gifs) on Web sites are another example 

        of information competing for the same sensory modality (visual) and claiming 

        a part of cognitive resources. An example from everyday life is the effect 

        carrying on a phone conversation has on driving ability.</p>

      <h2>Redundant</h2>

      <p class=MsoBodyText>A definition of <i>redundant</i> multimodal information 

        conveyed through another sensory channel, which does not increase the 

        total amount of information about the interaction but is also not adversely 

        affecting the interaction.</p>

      <h2>Complementary</h2>

      <p class=MsoBodyText><i>Complementary</i> multimodal information is information 

        conveyed through another sensory channel that does increase the total 

        amount of information received and has a beneficial effect on interaction. 

        This effect can be manifest as an increase in efficacy of interaction, 

        or decrease in number of errors. This is the only instance where the bandwidth 

        of human-computer information channel is increased by engagement of another 

        channel. </p>

      <h1>Case for Building Tangialities</h1>

      <p class=MsoBodyText>In conclusion, one can make the case for building tangialities 

        for the following reasons:</p>

      <ul>

        <li class="normal"><span class="normal">widening bandwidth of human/computer 

          communication channel;</span></li>

        <li class="normal"><span class="normal">adding affective dimension to 

          interaction;</span></li>

        <li class="normal"><span class="normal">allowing grasping and manipulation 

          of complex concepts without the need for explicit formalization;</span></li>

        <li class="normal"><span class="normal">reducing cognitive load by use 

          of intuitive body (biological) knowledge;</span></li>

        <li class="normal"><span class="normal">reducing the strain on one sense 

          (vision) - single sense fatigue;</span></li>

        <li class="normal"><span class="normal">possibility of adding another 

          dimension to meta data &#150; &#147;how does it feel&#148; (being able 

          to feel the texture of paintings and other, otherwise &#147;untouchable&#148; 

          objects)</span></li>

      </ul>

      <p class=MsoBodyText>Another significant use of tangible descriptions of 

        data and results of data manipulations is in making the digital domain 

        more accessible for populations with special needs. There are already 

        some promising results in this area (Yu, Ramloll and Brewster, 2001; Gouzman, 

        Karasin, Braunstein 2000). </p>

      <p class=MsoBodyText>Currently, we are lacking a satisfactory theory of 

        multimodal interaction and most often arrive at usable results through 

        a process of trial and error. It is evident that this theory has to come 

        from interdisciplinary efforts bridging the disciplines as diverse as 

        neurophysiology, tele-robotics and computer science. </p>

      <h1>References</h1>

      <p class=ReferencesText>Biggs, J., Srinivasan, M.A. (2001) Haptic Interfaces, 

        in Stanney, K.M. (ed.) <i>Handbook of Virtual Environment Technology,</i> 

        Lawrence Erlbaum Associates, Inc.</p>

      <p class=ReferencesText>Brewster, S.A.(2001). The Impact of Haptic &#145;Touching&#146; 

        Technology on Cultural Applications In Proceedings of EVA 2001. (Glasgow, 

        UK), Vasari UK, s28 pp1-14, also available on line: <a

href="../http://www.dcs.gla.ac.uk/~stephen/papers/EVA2001.pdf" target="_blank">http://www.dcs.gla.ac.uk/~stephen/papers/EVA2001.pdf</a></p>

      <p class=ReferencesText>Brunker, M., (2001) Sex toys blaze tactile trail 

        on Net, MSNBC News online article:<a

href="../http://www.msnbc.com/news/318124.asp?cp1=1" target="_blank">http://www.msnbc.com/news/318124.asp?cp1=1</a></p>

      <p class=ReferencesText>Gouzman, R., Karasin, I., Braunstein, A. (2000) 

        The Virtual Touch System by VirTouch Ltd: Opening New Computer Windows 

        Graphically for the Blind, Proceedings of &quot;Technology and Persons 

        with Disabilities&quot; Conference, Los Angeles March 20-25, 2000, on-line: 

        <a href="../http://www.csun.edu/cod/conf2000/proceedings/0177Gouzman.html" target="_blank">http://www.csun.edu/cod/conf2000/proceedings/0177Gouzman.html</a></p>

      <p class=ReferencesText>Immersion TouchSense&#153; technology, consulted 

        on line 24/3/02: <a href="../http://www.immersion.com/products/ce/generalmice.shtml" target="_blank">http://www.immersion.com/products/ce/generalmice.shtml</a></p>

      <p class=ReferencesText>Kirsh, D., (2001) <i>Changing the Rules: Architecture 

        in the new Millennium</i>, Convergence </p>

      <p class=ReferencesText>Logitech, <a href="../http://www.logitech.com/" target="_blank">http://www.logitech.com</a></p>

      <p class=ReferencesText>McCullough, M. (1996) <i>Abstracting Craft: The 

        Practiced Digital Hand</i>, The MIT Press, Cambridge, MA </p>

      <p class=ReferencesText>Merriam-Webster Dictionary consulted on line 2/28/02: 

        <a

href="../http://www.m-w.com/home.htm" target="_blank">http://www.m-w.com/home.htm</a></p>

      <p class=ReferencesText>Norman, D. (1991) Cognitive Artifacts, in Carroll, 

        M.J.(ed.) <i>Designing Interaction: Psychology at the Human-Computer Interface</i>, 

        Cambridge University Press</p>

      <p class=ReferencesText>Oakley, I., Brewster S.A. and Gray, P.D. (2000). 

        Communicating with feeling. In Proceedings of the First Workshop on Haptic 

        Human-Computer Interaction, 17-21 </p>

      <p class=ReferencesText>Patten, J., Ishii, H., Hines, J., Pangaro, G. (2001) 

        Sensetable: a wireless object tracking platform for tangible user interfaces, 

        in Proceedings of the SIGCHI conference on Human factors in computing 

        systems 2001, Seattle, Washington 253-260, ACM Press, NY</p>

      <p class=ReferencesText>Picard, R.W. (1997) <i>Affective Computing</i>, 

        The MIT Press, Cambridge, MA</p>

      <p class=ReferencesText>Picard, R.W. (2000) Toward computers that recognize 

        and respond to user emotion, <i>IBM Systems Journal</i>, Vol. 39, 705-717</p>

      <p class=ReferencesText>Roy, D. (2000) Learning from multimodal observations. 

        Proc. IEEE Int. Conf. Multimedia and Expo (ICME), New York, NY, consulted 

        on line 3/4/02: <a

href="../http://dkroy.www.media.mit.edu/people/dkroy/papers/pdf/ieee_multimedia2000.pdf" target="_blank">http://dkroy.www.media.mit.edu/people/dkroy/papers/pdf/ieee_multimedia2000.pdf</a></p>

      <p class=ReferencesText>Sears, A., Shneiderman, B., (1991) High precision 

        touchscreens: design strategies and comparisons with a mouse, <i>International 

        Journal of Man-Machine Studies</i> 34, 593-613</p>

      <p class=ReferencesText>Shneiderman, B. (1983), Direct Manipulation: A step 

        beyond programming languages, <i>IEEE Computer</i>, 16, 8, 57-69</p>

      <p class=ReferencesText>Shneiderman, B. (1998) <i>Designing the User Interface: 

        Strategies for Effective Human-Computer Interaction</i>, Addison-Wesley</p>

      <p class=ReferencesText>Ullmer, B., Ishii, H. (2000) Emerging Frameworks 

        for Tangible User Interfaces, <i>IBM Systems Journal</i>, v393n3, 915-931</p>

      <p class=ReferencesText>Yu, W., Ramloll, R., Brewster S.A. (2001) Haptic 

        graphs for blind computer users, in Brewster, S.A. and Murray-Smith, R. 

        (Eds.) <i>Haptic Human-Computer Interaction</i>, Springer LNCS, Vol 2058, 

        41-51.</p>

      <!-- #EndEditable --></td>
  </tr>
</table>
<!--htdig_noindex-->

<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script> 
<script type="text/javascript" src="../http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript></noscript>

<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>

<!--/htdig_noindex-->

</body>
<!-- #EndTemplate -->
<!-- Mirrored from www.museumsandtheweb.com/mw2002/papers/milekic/milekic.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:48:14 GMT -->
</html>
