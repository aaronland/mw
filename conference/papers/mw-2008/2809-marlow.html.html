<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><!-- InstanceBegin template="/Templates/mw2008-papers.dwt" codeOutsideHTMLIsLocked="true" -->

<!-- Mirrored from www.museumsandtheweb.com/mw2008/papers/marlow/marlow.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 16:58:01 GMT -->
<head>

<!-- InstanceBeginEditable name="HeadPageTitle" -->
<title>Archives &amp; Museum Informatics: Museums and the Web 2008: Papers: Marlow, J., et al., The Multimatch Project: Multilingual/Multimedia Access To Cultural Heritage On The Web</title>
<!-- InstanceEndEditable -->
<!-- InstanceBeginEditable name="metaTitle" -->
<meta name="title" content="Archives  &amp; Museum Informatics: Museums and the Web 2008" />
<!-- InstanceEndEditable -->
<!-- InstanceBeginEditable name="Keywords" -->
<meta name="Keywords" content="archives &amp; museum informatics, archives, museums, informatics, digital museums, digital archives, digital art, museums online, archives online, libraries online. world wide web, www, conferences, professional papers, peer-reviewed, digital libraries, online exhibits, online exhibitions, on-line" />
<!-- InstanceEndEditable -->
<!-- InstanceBeginEditable name="Description" -->
<meta name="Description" content="Museums and the Web 2008: the international conference for culture and heritage on-line" />
<!-- InstanceEndEditable -->

<!-- InstanceBeginEditable name="copyright" -->
<meta name="copyright" content="Archives &amp; Museum Informatics, 2008" />
<!-- InstanceEndEditable -->
<meta name="document-class" content="Published" />
<meta name="document-rating" content="General" />
<meta http-equiv="Content-Language" content="EN" />
<meta name="document-rights" content="Copyrighted Work" />
<!-- InstanceBeginEditable name="charset" -->
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<!-- InstanceEndEditable -->
<link rel="stylesheet" href="../../css/mw2008.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../css/papers.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../css/mw2008-noNews.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../css/print.css" type="text/css" media="print" />
<!--htdig_noindex-->
<!-- InstanceBeginEditable name="ImageRotateScript" --><!-- InstanceEndEditable -->
<!--/htdig_noindex-->
<!--htdig_noindex-->
<!-- InstanceBeginEditable name="PageScript" --><!-- InstanceEndEditable -->
<!--/htdig_noindex-->
</head>
<body  onload="initImgRotation()">
<div id="container">
  <div id="header" onclick="location.href='../../index.html';" style="cursor:pointer;"> </div>
  <!--htdig_noindex-->
  <div id="top-nav">
    <ul id="top-nav-list">
      <li><a href="../../register/index.html">Register</a></li>
      <li><a href="../../dates/index.html">Key Dates</a></li>
      <li><a href="../../sescal/index.html">Schedule</a></li>
      <li><a href="../../local/index.html">Local Info</a></li>
    </ul>
    <!--/htdig_noindex-->
  </div>
  <div id="header-photo"> 
	 <!-- InstanceBeginEditable name="header-photo" -->
			<img src="marlow-t.jpg" alt="MW-photo" name="img1" width="229"  height="132" id="img1" />
			<!-- InstanceEndEditable --></div>
<div id="print-title">
	<h1>Museums and the Web 2008: the international conference for culture and heritage on-line</h1>
	<h2>produced by Archives &amp; Museum Informatics</h2>
	<h3>site at http://www.archimuse.com/mw2008/</h3>	 
</div> 
 <div id="date"> <a href="../../sescal/index.html">April 9-12, 2008</a><br />
    <a href="../../local/index.html">Montr&eacute;al, Qu&eacute;bec, Canada</a> </div>
  <div id="title">
    <h1> 
		<!-- InstanceBeginEditable name="PageTitle" -->
    	  The Multimatch Project: Multilingual/Multimedia Access To Cultural Heritage On The Web

				<!-- InstanceEndEditable --> </h1>
  </div>
  <!--htdig_noindex-->
  <div id="main-content">
    <div id="news">
    </div>
    <!--/htdig_noindex-->
    <div id="intro-paragraph"> 
		<!-- InstanceBeginEditable name="IntroParagraph" -->

        <h2 class="Author"><a href="../../bios/au_440015628.html">Jennifer Marlow</a>, <a href="../../bios/au_440015627.html">Paul Clough</a>, <a href="../../bios/au_440016428.html">Neil Ireson</a>, University of
          Sheffield, United Kingdom; <a href="../../bios/au_440016429.html">Juan Manuel Cigarr&aacute;n Recuero</a>, <a href="../../bios/au_440016430.html">Javier Artiles</a>,
          Universidad Nacional de Educaci&oacute;n a Distancia, Spain; and <a href="../../bios/au_440016228.html">Franca Debole</a>,
        ISTI-CNR, Italy</h2>
        <p class="URL"><a href="http://www.multimatch.eu/">http://www.multimatch.eu</a></p>
        <h3 class="AbstractTitle">Abstract</h3>
        <p class="AbstractText">The EU-funded MultiMatch project aims to overcome language barriers and media and distribution problems currently affecting access to on-line cultural heritage material. Partners are developing a vertical search engine able to harvest heterogeneous information from distributed sources and present it in a synthesized manner. To design such a system, user requirements were initially gathered and then translated into specific design features to ensure that the search engine developed was consistent with user needs. This paper presents these user requirements, the initial design of the MultiMatch system, and technical discussion of the system architecture and components used to turn these design implications into a working interactive prototype. Following this, we discuss user evaluation and present results from an initial user study. These are being used, in addition to other input, to drive the functionality and design of the final system.</p>
        <p class="keywords">Keywords: User-centered design, cultural heritage on-line,
          search engine, task analysis, user-centred interface design</p>
		<!-- InstanceEndEditable --> 
	</div>
    <div id="body-text"> 
		<!-- InstanceBeginEditable name="BodyText" -->


    <h1>1. Introduction</h1>
      <p>For most
        organisations, providing access to information on their collection of available
        resources is of critical importance, either to provide competitive advantage
        (allowing customers to more easily find what they want), or to satisfy
        fundamental reasons for the organisation&rsquo;s existence, as is the case in the
        cultural heritage (CH) domain for libraries, museums, archives or other
        repositories. The digitization of information on the Web enables CH institutions to
        provide wider access to their resources by removing the barrier of physical
        location. To satisfy some information needs, it may be necessary for
        individuals to access resources distributed over a number of different
        institutions around the world. Although a meta-search engine that collates
        material retrieved from different sources could be a useful first step, the
      problems of cross-language and cross-media search would remain.</p>
      <p>The MultiMatch
        project aims to address some of these issues by developing an information
        retrieval system able to harvest heterogeneous information from distributed
        sources and present it in a coherent and synthesized manner. Although several
        studies have investigated how users access CH information, little is known
        about the impact of cross language/media search or the use of aggregated
      resources.</p>
      <p>In general terms,
        the MultiMatch system incorporates data mapped directly from CH content providers&rsquo;
        databases and identifies further relevant material via a focused crawl of CH
        institutions&rsquo; Web sites. The retrieved information is enriched using automatic
        semantic annotation and translation processes, and the resultant metadata
        ingested into the MultiMatch data model. This process allows the user to
        retrieve information from the MultiMatch system, including relevant text, image
        and video resources, regardless of the source and target languages used to
        write the query and/or describe the resources. Finally, the system organises
        search results in an integrated, user-friendly manner, allowing users to
        access, interpret and interact with the information retrieved, and, if
      necessary, enables refinement of the query to better meet their requirements. </p>
      <p>This paper addresses
        the design of the first interactive prototype of the MultiMatch system, in
        accordance with a typical user-centered design cycle (Rubin, 1994; Faulkner,
      2000; Preece et al., 2002). In MultiMatch, the development cycle consists of: </p>
      <ol>
        <li>needs assessment and task analysis; </li>
      <li>preliminary design using low-fidelity prototypes; </li>
      <li>design and development of interactive prototype; </li>
      <li>heuristic evaluation and redesign; and </li>
      <li>user evaluation. </li>
    </ol>
      <p>This paper begins
        with a literature review and discussion of user requirements gathering in the
        context of MultiMatch. Next, the system architecture is described, followed by
        a report of initial evaluation results. Finally, the functionalities envisioned
      for the final system are presented. </p>
      <h1>2. User Requirements Gathering</h1>
      <h2>2.1 Previous Work</h2>
      <p>Past research on
        gathering requirements for professionals in CH has varied in terms of the
        methodologies used, information gathered, and audiences surveyed. Some studies
        were conducted in the context of evaluating specific Web sites or systems
        (Khoon et al., 2002; Economou, 2002; Smith et al., 2005); whilst others were
        more general and exploratory (Sexton et al., 2004; HEIRNET, 2002). Further work
        has also addressed the needs of professional or expert user, as opposed to the
        more casual or general user. For example, in the context of European research
        projects, there have been analyses of multilingual journalists in CLARITY
        (Petrelli et al., 2004) and broadcast professionals in VideoActive (O&rsquo;Dwyer,
        2007). Approaches used by these have varied, but commonalities included the
        drafting of user profiles/scenarios influenced by responses to questionnaires
      and interviews.</p>
      <p>Other research has
        been carried out with academic users in the humanities domain, focusing on
        topics ranging from information sources used (Brown et al., 2006) to users&rsquo;
        search behaviour (Frost et al., 2000). Several studies have examined the nature
        of search queries in the CH field, both for general information (Cunningham et
        al, 2004) and for images (Pask, 2005; Choi &amp; Rasmussen, 2003; Collins,
        1998; Chen, 2001). These all had similar findings: namely, that people, places,
        time periods, and subjects were popular topics of search in the CH domain.
        Whilst findings from the aforementioned studies provide useful background
        knowledge, none of them compare and contrast the needs of a diverse group of
        individuals across job boundaries. Additionally, most tend to focus on the way
      users interact with just a single type of media (text, images or video). </p>
      <p>Similar projects
        focusing on the aggregation, presentation, and semantic navigation of on-line
        CH information include eCHASE (Sinclair et al., 2005) and MultimediaN
        (Schreiber et al., 2006). These systems were identified as part of an initial
        competitor analysis that influenced the overall design process and also helped
        reveal how MultiMatch sets itself apart from similar projects. This
        differentiation occurs in MultiMatch&rsquo;s overall combination of functionalities:
        it enables both multilingual and multimedia search of domain-specific, semantically-enriched
      CH material provided by reputable archives and crawled from the Web. </p>
      <h2>2.2 User Requirements Gathering</h2>
      <p>One hundred
        person-to-person interviews were conducted with domain experts (educational,
        tourism and CH professionals) to collect their opinions and needs. The interviews
        were conducted mainly in face-to-face mode using a questionnaire, and backed up
        by a set of scenarios and a vision document to give the respondents an idea of
        the proposed system functionality. In addition to the interviews, analysis of
        selected log files from on-line portals provided by project partners WIND (a
        major Italian Internet service provider) and Fratelli Alinari (a large historical
        photographic archive), as well as examination of the results of previous user
        studies in the CH domain, occurred. (For further information, see Minelli et
        al., 2007). Responses by potential users (both experts and general users)
        yielded a very large set of requirements. Summarising briefly, we can say that
      the main findings were the following:</p>
      <ul>
        <li>CH professionals do use the Internet widely and
          as part of their daily work routine, but they currently depend largely on
        generic search engines to find the information they need.</li>
      <li>They would like full capabilities for
        multimedia retrieval (i.e., images and video as well as text), but in most
        cases, are only accustomed to executing text searches.</li>
      <li>Their main focus is about works of art and
        their creators. They would like access to all associated information; such as
        critical reviews, information on exhibitions, and different versions of the same
        document.</li>
      <li>They tend to be frustrated by the volumes of
        information available on the same subject and would find information filtering,
        clustering and aggregation functionalities very useful.</li>
      <li>They tend to restrict their searches to their
        own language plus English, thus missing information only available in other
        languages.</li>
      <li>If multilingual search were available, they
        would like to have the results associated with descriptive snippets in their
        own language (preferably) or English (optionally).</li>
    </ul>
      <p>To further explore
        the needs of expert users in more detail and to learn more about their typical
        tasks, contextual interviews were conducted with fifteen CH professionals at
        three different institutions. Seven interviewees were academics (university
        professors in the fields of arts and heritage management, architecture,
        archaeology, French literature, and history) from the University of Sheffield (UK), four were image professionals from
        Fratelli Alinari (IT), who worked for the photographic archive, and four were
        video professionals from the Netherlands Institute for Sound and Vision (the
        Dutch national television archive) who regularly searched through various
        motion picture archives. This provided a variety of perspectives relating to
        the CH field and expanded upon past studies by addressing a broader range of
        tasks and topic areas. Table 1 illustrates some typical tasks (scenarios) for
        the different cultural user groups. These were developed, with the help of the
      expert users, to assist with the design and evaluation of MultiMatch. </p>
      <table border="0" cellspacing="0" cellpadding="0">
        <thead>
          <tr>
            <th>User type</th>
          <th>Task</th>
          <th>Media and languages involved</th>
        </tr>
        </thead>
        <tr>
          <td>CH professional</td>
        <td>Searching for video footage
          on Pier Paolo Pasolini, needs to gather background information on who he was</td>
        <td> Text, Images, Video<br />
          <br />
          English, Dutch</td>
      </tr>
        <tr>
          <td>CH professional</td>
        <td>Looking for images of
          (non-famous) people drinking coffee &#8211;images that capture a certain
          emotion</td>
        <td> Images English, Italian</td>
      </tr>
        <tr>
          <td>Academic</td>
        <td>Preparing a presentation on
          Don Quixote and how it has influenced the arts</td>
        <td> Text, Images, Video,  Audio (?) English only</td>
      </tr>
        <tr>
          <td>Cultural tourist/General
          user</td>
        <td>Planning a visit to Turin,
          wants to know about museums to visit, what can see whilst there</td>
        <td> Text, Images, Audio  (podcasts) English, Italian</td>
      </tr>
      </table>
      <p class="caption">Table 1: Revised scenarios based on comments and observations
      from interviews</p>
      <p>The interview
        questions focused on topics including a general overview of the interviewee&rsquo;s
        work, commonly used tools, use of the Internet, use of multimedia material,
        concrete examples of past search processes, ways of improving the current
        search process, and language issues relating to information access. Interviews
        were recorded and subsequently transcribed to facilitate detailed analysis and
        categorization of responses. Each comment made was classified into a category
        based on whether it related to typical tasks, multimedia use, search topics,
      tools employed, the search process, general issues, or language-related issues. </p>
      <p>Despite the
        differences in job roles and areas of expertise, many common patterns emerged
        across all groups of interviewees. Some of these confirmed the findings from
        the earlier questionnaires, as mentioned previously, while others emerged as
        specific needs of a particular group as more information was learned; for
        example, video professionals also had unique needs that did not necessarily
        apply to other user groups. An updated summary of similar and different needs,
        along with their design implications, can be seen in Table 2. Many of these
        needs formed the basis for the functional specification of the first working prototype
        system. However, due to time constraints, others were not initially implemented
      but will be included in the second and final prototype. </p>
      <table>
        <tr>
          <th>Relevant Groups</th>
        <th>Need</th>
        <th>Design Implication</th>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="1">
          <li>Look for the same material on different sites (either
            because it couldn&rsquo;t be found or because it needed to be confirmed)</li>
          </ol></td>
        <td><ol start="1">
          <li>Aggregate multimedia material dispersed across
            the Web and fuse results in a single place<br />
            Employ
            a common ontology to facilitate the exploration of content coming from
            diverse origins in a unified and organized way. The system will create a new,
            unique metadata scheme to create common links among the content whilst
            respecting the original native metadata formats. </li>
          </ol>
        </td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="2">
          <li>Search and browse (typically search for a topic
            and browse through results)</li>
          </ol></td>
        <td><ol start="2">
          <li>Provide support for both types of behaviour.
            Organize and sub-divide large results sets into semantically related
            clusters to avoid duplication of results and to facilitate ease of
            finding the desired information when the query is ambiguous. </li>
          </ol></td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="3">
          <li>Have support for a variety of searches relating
            to people, subjects, time, and places (who, what, when, and where),
            particularly queries involving two or more of these aspects</li>
          </ol></td>
        <td><ol start="3">
          <li>Let users navigate semantic relationships between
            multiple categories (via faceted browsing or some other method).
            Categories/facets should be logical, intuitive, and correspond to the
            main classes of searches.<br />
            Utilize
          specialized thesauri relating to names of people and places to support
          linguistic variations in these areas   </li>
          </ol>
        </td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="4">
          <li>Search for items conveying a feeling, mood,
            style, or other aspect that cannot be easily conveyed verbally</li>
          </ol></td>
        <td><ol start="4">
          <li>Facilitate content-based retrieval, query by
            example, and multimodal querying</li>
          </ol></td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="5">
          <li>Consult authoritative, quality information
            sources</li>
          </ol></td>
        <td><ol start="5">
          <li>Provide information about provenance and
            facilitate the filtering of results by domain (e.g., exclude Wikipedia
            results)</li>
          </ol></td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="6">
          <li>Be aware of copyright issues</li>
          </ol></td>
        <td><ol start="6">
          <li>Provide information about copyright; institute
            a log-in system so that users must register and accept a copyright
            policy in order to access non public domain material</li>
          </ol></td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="7">
          <li>Conduct searches in unknown languages</li>
          </ol></td>
        <td><ol start="7">
          <li>Give the option of performing and inspecting
            automatic query translation, thus broadening the coverage of a search
            and eliminating the need to rely on dictionaries to manually translate
            terms <br />
            Use
          various thesauri relating to areas in which multiple variations of words
          exist across languages (e.g. the Getty ULAN for artist names and TGN for
          place names). Useful for queries whose spelling varies across languages (e.g.
          Raphael in English, Raffaello in Italian)  </li>
          </ol>
        </td>
      </tr>
        <tr>
          <td>Academic<br />
            Image<br />
          Video</td>
        <td><ol start="8">
          <li>Have assistance in browsing documents or
            results written in foreign languages</li>
          </ol></td>
        <td><ol start="8">
          <li>Offer the option of automatically translating
            foreign-language summaries or documents (although the degree and nature
            of support needed might depend on the media type being searched for)</li>
          </ol></td>
      </tr>
        <tr>
          <td>Video</td>
        <td><ol start="9">
          <li>Get a quick overview of a video&rsquo;s content </li>
          </ol></td>
        <td><ol start="9">
          <li>Provide both textual descriptions and keyframe
            storyboards to use as summaries. Allow users to retrieve relevant
            portions of a video clip based on automatic speech recognition
            transcripts and visual metaphor tools. </li>
          </ol></td>
      </tr>
        <tr>
          <td>Video</td>
        <td><ol start="10">
          <li>
            Find places in a video where certain words are spoken (e.g., a famous
            quotation)</li>
        </ol></td>
        <td><ol start="10"><li>Enable keyword search of video transcript and the
          ability to jump into relevant parts of the video</li></ol></td>
      </tr>
      </table>
      <p class="caption">Table 2: Summary of needs and associated design implications.</p>
      <p>These design specifications were incorporated into a series
        of low fidelity mockups which were converted into a working interactive
        prototype. Many of them are present in the first prototype, whilst others will
      not be implemented until the final version of the system. </p>
      <h1>3. Architecture</h1>
      <p>As mentioned before,
        the first prototype system was intended to establish the system architecture
        and provide a basic initial set of services to evaluate. Needs 1, 5, 7, 9, and
        10 are addressed in the first system by providing aggregated multimedia result
        sets for a query. The content of the collection has been specifically crawled
        from reputable and relevant sites on the Web, or donated by CH partners. Also
        included in the first prototype is a cross-language search functionality that
        offers automatic query translation (which can facilitate searching for both
        bilingual and monolingual users). Finally, the various media types can be
        interacted with in a more sophisticated way; these specialized searches are
      intended to address the needs of specialized professional users. </p>
      <p>The system is being
        developed using a service-orientated architecture (SOA), with co-operating but
        independent components working together to provide a complete system.
        MultiMatch uses a centralised index where metadata for CH objects (texts,
        images sounds and videos) is crawled from sources (e.g., culturally-specific
        Web pages, blogs and wikis) provided by the cultural institutions themselves,
        or harvested from OAI-compliant sources. The index is searched centrally, but
        the resources themselves can be located anywhere on the Internet. The current
        prototype system consists of four co-operating but independent sub-systems as
      shown in Figure 1.</p>
      <p class="caption"><a name="fig1" id="fig1"></a><a href="marlow-fig1.html"><img src="marlow.fig1.JPG" alt="Figure 1" width="388" height="287" /></a></p>
      <p class="caption"><a href="marlow-fig1.html">Fig 1: Components in the MultiMatch system.</a></p>
      <p>Each of the sub-systems provides
        a specific functionality optimised for the CH domain. That is, they are
        designed to specifically support the gathering, information extraction,
        indexing, searching, and display of European CH objects, rather than more
        general material. However, the basic functionality is to a degree
        domain-independent, so that components can be ported to other domains in the
        future without having to perform extensive revision. The architecture of the
      current prototype consists of four sub-systems discussed in detail below.</p>
      <h2>3.1 Information Indexing and Extraction</h2>
      <p>This
        sub-system ingests data into the MultiMatch system, either mapped from CH
        databases or using focused crawling to locate Web pages of interest to the
        MultiMatch from which relevant data is extracted (e.g., artist names and artwork
        titles). The data is transformed into metadata in a format defined within the
        MultiMatch project (Ireson and Oomen, 2007)); the linguistic data is translated
        into the target system languages before the metadata is indexed and passed into
      the metadata repository.</p>
      <p>The content used in
        the first prototype system comprises data from a variety of sources, ranging
        from structured data from the CH partners, through semi-structured data (e.g.
        Wikipedia articles), to unstructured textual data from crawled Web pages, and
        additional audio files (Table 3). This offers varying levels of difficulty with
        respect to processing the data, and additional difficulties in providing a
      unified view of the content indexed by the MultiMatch system.</p>
      <table>
        <tr>
          <th >Origin&nbsp; </th>
          <th>Quantity </th>
          <th>Metadata dimensions </th>
        </tr>
        <tr>
          <td>Alinari&nbsp; </td>
          <td>5,000 stills(jpg) </td>
          <td>Proprietary Dublin Core </td>
        </tr>
        <tr>
          <td>Beeld and Geluid&nbsp; </td>
          <td>50 videos (mpeg-1) </td>
          <td>Proprietary Dublin Core </td>
        </tr>
        <tr>
          <td>Biblioteca Virtual Miguel de Cervantes&nbsp; </td>
          <td>9,000 texts </td>
          <td>Proprietary Dublin Core </td>
        </tr>
        <tr>
          <td>Wikipedia&nbsp; </td>
          <td>65,000 articles </td>
          <td>Proprietary format </td>
        </tr>
        <tr>
          <td>White list crawl&nbsp; </td>
          <td>40,000 pages </td>
          <td>Proprietary format </td>
        </tr>
        <tr>
          <td>University of Amsterdam Audio Corpus&nbsp; </td>
          <td>20 hours audio </td>
          <td>Proprietary format </td>
        </tr>
        <tr>
          <td>The European Library</td>
          <td>1.6 million records</td>
          <td>OAI-DC</td>
        </tr>
      </table>
      <p class="caption">Table 3: Content indexed in the first MultiMatch prototype</p>
      <p>MultiMatch aims to
        produce a complete metadata record for the digital object, allowing users to
        interpret the wealth of CH information by presenting objects not as isolated
        individual items, but as situated, richly connected entities. Figure 2 shows an
        overall view of the data model used to
        represent the CH domain in MultiMatch, and Figure 3 shows the interconnection
        between the main entities in the model. Such a representation allows the user
      to explore the information space related to a given object. </p>
      <p class="caption"><a name="fig2" id="fig2"></a><a href="marlow-fig2.html"><img src="marlow.fig2.JPG" alt="Figure 2" width="400" height="101" /></a></p>
      <p class="caption"><a href="marlow-fig2.html">Fig 2: MultiMatch data model </a></p>
      <p class="caption"><a name="fig3" id="fig3"></a><a href="marlow-fig3.html"><img src="marlow.fig3.jpg" alt="figure 3" width="400" height="124" /></a></p>
      <p class="caption"><a href="marlow-fig3.html">Fig 3: MultiMatch Entity Relationships </a></p>
      <p>Due to the fact that
        the MultiMatch ingests information from multifarious sources and applies
        automatic enrichment, the representation maintains the source of the
        information (both the original data source and the information extraction
        process). This information is necessary both for system development purposes
        (to evaluate and debug the information gathering process) and for certain CH
        users who require an &ldquo;audit&rdquo; trail so that the provenance of the information is
        explicit, enabling them to determine their level of trust in the information
      provided (Need 5).</p>
      <h2>3.2 Searchable Metadata Repository</h2>
      <p>The
        searchable metadata repository provides a service for storing and retrieving
        MultiMatch cultural objects (images, video, text, audio, etc) and the
        associated metadata documents generated by the information extraction
        sub-system. The repository is optimised for querying the specific types of data
        that are contained within the metadata records, whether this is text, numeric,
        temporal, geographic, image, etc. It
        also caches each MultiMatch object with its metadata record for fast retrieval
        by the Enhanced Information Retrieval module. Storage, versioning and serving
        the digital objects are handled by the metadata repository content cache, which
      is based upon the Milos Multimedia Media server (Amato et al., 2004).</p>
      <h2>3.3 Enhanced Information Retrieval</h2>
      <p>The enhanced information retrieval sub-system provides a set
        of four services offering a sophisticated multilingual CH-oriented search
        facility. This sub-system provides an external interface to MultiMatch
        functionality, such that organizations or users can develop their own
      custom-built applications. The four main services are:</p>
      <ol>
        <li> <strong>MultiMatch Search Service:</strong> this provides a stateless search interface, allowing
          the client to specify what types of objects to search, how many to return and
          which items in the result set should be returned. The results of the search can
          be either a brief snippet such as might be provided on a results summary
          screen; or the full records including all metadata and digital content. </li>
        <li> <strong>MultiMatch Query Translation Service: </strong>this translates the query terms between
          English, Spanish, Dutch and Italian. MultiMatch is developing components for
          both document and query translation and procedures for matching one against the
          other. Much effort is being dedicated to the building of domain-specific
          multilingual resources catering for the terminology adopted in the CH domain
          (Jones et al., 2007). Both dictionary-lookup and Machine Translation services
          have been developed. </li>
        <li> 
          <strong>The MultiMatch Query Expansion Service:</strong> this provides three mechanisms for
          query expansion that can be used to refine the users search experience. It
          offers the blind relevance feedback which expands the query without user
          intervention, relevance feedback using user selected items as seeds for new
          terms, and a thesaurus service that provides cognate terms. </li>
        <li> 
          <strong>MultiMatch Browse Service:</strong> this service allows navigation through indexed
          content using any of the metadata schema indexes known to MultiMatch, e.g.
          browsing for names, titles, dates, locations, etc. While browsing
          functionalities will not be present in the first prototype, this service is a
          part of the final system&rsquo;s architecture.        </li>
      </ol>
      <h2>3.4 User Interface</h2>
      <p>The User Interface sub-system provides a set of client
        services allowing development of user interfaces that offers both simple (free
        text) and advanced (metadata based) search, browsing of the content, as well as
        stored personalisation, search history, and preferences features. MultiMatch
        provides a default browser-based interface for design and test purposes,
        enabling the user to interact with content indexed by the MultiMatch system,
        and using functionalities developed for search and browse. The interface is
        responsible for collecting user queries, presenting results, and allowing users
        to enhance and refine their information need<b>. </b>Users are able to retrieve cultural objects through two different
      search modes:</p>
      <ul>
        <li><b>Free
          Text Search.</b> This search mode is similar
          to any general purpose search engine (e.g. Google), with the difference that
          (1) MultiMatch will provide more precise results since the information indexed
          is selected from sources containing CH data, and (2) MultiMatch will provide
          support for multilingual search. This means that the user can formulate queries
          in a given language and retrieve results in one or all languages covered by the
          prototype (according to his/her preferences). Figure 4 shows an example of how
          multilingual search is integrated into a free text search action through a
          &ldquo;translation wizard&rdquo;. In this case, the user has queried about &ldquo;still life&rdquo; but
          wants to retrieve information in Spanish. The query is translated into Spanish
          and launches the retrieval process. Once the retrieval has been performed, the
          system shows the user the translation, giving the opportunity for change if
          necessary by adding, removing or writing new translation terms. In this case,
          the system has also detected &ldquo;still life&rdquo; as a phrase so it suggests the
          Spanish term &ldquo;bodeg&oacute;n&rdquo; as the translation. </li>
      </ul>
      <p class="caption"><a name="fig4" id="fig4"></a><a href="marlow-fig4.html"><img src="marlow.fig4.jpg" alt="figure 4" width="395" height="233" /></a></p>
      <p class="caption"><a href="marlow-fig4.html">Fig 4: Illustration of cross-language search functionality</a></p>
      <ul>
        <li><b>Metadata Search.</b> From the results of the expert users&rsquo; survey,
          we can conclude that, on average, CH professionals tend to classify searches
          for information about creators (authors, artists, sculptors, composers, etc.)
          and creations (works of art and masterpieces) as their most common search
          tasks. The use of metadata search is designed to meet user needs 2 and 3 by
          providing an alternative way of searching for a topic (based on metadata fields
          such as creator or creation) or browsing through results (e.g. to find other
          artworks made by the same creator of a given result). </li>
      </ul>
      <p style='text-autospace:none'>A key objective is to provide a
        system that can be easily adapted to different user needs. For this reason,
        MultiMatch searches can be performed at two main levels of interaction in the
      current prototype: </p>
      <ul>
        <li><b>Default search mode.</b> The
          simplest search mode is the default MultiMatch search level. This is provided
          for generic users with a limited knowledge of MultiMatch system capabilities,
          or with very general search needs. In this case, no assumption is made on the
          user query, and MultiMatch retrieves information from all indexed material.
          This interaction level thus involves the retrieval of Web pages, images and
          videos related to the query. Figure 5 shows this behaviour for the current
          prototype. For the query &ldquo;pasolini,&rdquo; the system retrieves all available
          resources matching the query and shows them to the user in an overview mode. In
          this case there are images and Web pages available in English, Spanish, and
          Italian, with the addition of videos in Dutch. </li>
      </ul>
      <p class="caption"><a name="fig5" id="fig5"></a><a href="marlow-fig5.html"><img src="marlow.fig5.jpg" alt="figure 5" width="400" height="400" /></a></p>
      <p class="caption"><a href="marlow-fig5.html">Fig 5: Illustration of default search</a></p>
      <ul>
        <li><b>Specialised search mode.</b> Users with a more precise knowledge of
          MultiMatch system functionality, and with specific search needs, may use one of
          the specialized interaction levels available. These allow the user to query
          MultiMatch specific search services (for instance, video search, image search,
          etc.) and retrieve all the relevant information available via the selected
          search service. In this way, MultiMatch will include standalone image, video
          and metadata-based searches, each with its own search fields, display and
          refinement options. The image search gives as a result a set of images, ranked
          according to some relevant criteria, which will give access to image
          thumbnails, original images, and to the image relevance feedback functionality,
          centered around visual attributes (such as texture and colors) and realized
          using the capabilities of GIFT, a research effort at the Vision Group at the
          CUI (computer science center) of the University of Geneva
          (http://www.gnu.org/software/gift/).</li>
      </ul>
      <p>The video search service provides the user with a video
        list, ranked and retrieved using text retrieval techniques based on spoken
        audio transcriptions, and it offers also the possibility of retrieving only a
        specific segment of a given video clip relevant to a specific query. In order
        to return only the audiovisual information of interest, a streaming video
      server has been installed on the MultiMatch integration server.</p>
      <p style='text-autospace:none'>Figure 6 shows the specialized
        video interface where the user has queried for &ldquo;Dutch videos about Picasso&rdquo;.
        The system shows the set of retrieved videos and also gives access to a set of
        specific video tools where the user can manipulate a video for playback. These
        specialized functionalities were designed to meet the needs of video
        professionals (see needs 9 and 10 of Table 2). They allow a user to view
        keyframes for a given video, to start playback at a desired point by clicking
        on the relevant keyframe, and also to search the ASR transcript for words of
      interest. </p>
      <p class="caption"><a name="fig6" id="fig6"></a><a href="marlow-fig6.html"><img src="marlow.fig6.JPG" alt="figure 6" width="307" height="400" /></a></p>
      <p class="caption"><a href="marlow-fig6.html">Fig 6: Screenshot of specialized video playback</a></p>
      <h1>4. Evaluation</h1>
      <p>In accordance with the user-centred design methodology being
        adopted, on completion of the first interactive prototype (designed in
        accordance with user needs and requirements), it was necessary to evaluate it
        with both expert and general users. Overall, the evaluation consisted of three
        parts, each of which yielded feedback on various aspects of the system. These
        three exercises included the distribution of questionnaires associated with a
      system demo, an internal evaluation, and larger scale user testing. </p>
      <h2>4.1 Questionnaires</h2>
      <p>The system demo was carried out at a large education
        conference that was attended by many people whose work involved CH in some way.
        After viewing a guided demo of the first prototype system and its
        functionalities, they filled out a questionnaire asking them to rate the
        usefulness of the various features. In total, 20 people responded and the
      top-rated functionalities were as follows:</p>
      
      <table border="0" cellspacing="0" cellpadding="0" width="363">
        <tr>
          <th>Functionality</th>
          <th>Mean Rating (1=not useful; 5=very useful)</th>
        </tr>
        <tr>
          <td>Advanced search based on
          metadata fields</td>
          <td>4.50</td>
        </tr>
        <tr>
          <td>Search for video fragments
          in specialized video search</td>
          <td>4.25</td>
        </tr>
        <tr>
          <td>Term suggestion to perform
          text relevance feedback</td>
          <td>4.20</td>
        </tr>
        <tr>
          <td>Image similarity search to
          perform visual relevance feedback</td>
          <td>4.18</td>
        </tr>
        <tr>
          <td>Different interaction
          levels (overview vs. specialized)</td>
          <td>4.13</td>
        </tr>
      </table>
      <p class="caption">Table 4:  Top 5 rated functionalities of first prototype</p>
      <h2>4.2 Internal Evaluation</h2>
      <p>The second stage
        occurred prior to conducting the main user testing. An internal pilot testing
        of the evaluation was carried out with project partners. Realistic scenarios of
        use drove the evaluation; users were given a series of search tasks to complete
        using MultiMatch, which were based on typical behaviours mentioned in the
        requirements gathering phase (see Table 2). The tasks guided the users through
        the system and highlighted the various specialized media functionalities. After
        completing these tasks, users filled in questionnaires focusing on system
      satisfaction and usability. </p>
      <p>This highlighted important system bugs and also provided
        some input into further development of the system. Many related to minor design
      or technical issues, but the following suggested changes were mentioned:</p>
      <ul type="disc">
        <li>Users wished to be able to use
          the cross-language functionality to search across all languages,  not just
        language pairs</li>
      <li>Users requested further
        support in reading cross-language search results in languages they did not
        understand </li>
      </ul>
      <h2>4.3 Large Scale User Evaluation</h2>
      <p>The large-scale user evaluation is in progress at the time
        of writing. Overall, over 40 expert and general users are expected to complete
        the tasks and offer their feedback on the system. These comments will then be
        used to help guide the improved design of the second prototype. Initial results
        indicate that, once again, users are pleased with the cross-language searching
        functionality. However, they did have some trouble dealing with material in
        languages they could not speak or read. The evaluation concluded with a
        question asking users about the usefulness of a variety of features that could
        be included in the second prototype. The features rated as most useful included
        (in order): document translation, exploring relationships between artists, and
        viewing artist information on a timeline. This supports the inclusion of the
      features mentioned in the following section. </p>
      <h1>5. Final Vision </h1>
      <p>New features in the final prototype system will take two
        forms: improvements made in response to user evaluation comments, and additions
        relating to functionalities based on user needs that were not able to be
        included in the first prototype due to time and technical constraints. With
        regards to this first category, the final system will address language issues
        by including an enhanced &ldquo;translation wizard&rdquo; for cross-language searching
        purposes and providing document translation (as requested above and in Need 8
      in Table 2). </p>
      <p>The new system features will build upon the first prototype
        and address further needs by providing added functionalities to enhance
        interaction with the content. These enhancements will primarily be related to
        the areas of collection browsing (Need 2) through a faceted browsing feature
        (Need 3). This will provide a more directed search and exploration of the
        MultiMatch collection by facilitating faceted browsing of author and
        artwork-related material (a user can search using more than one category as
        criteria; for example, find all American male artists from the 20<sup>th</sup> century). Both expert and general users signaled this as a useful way of
        enhancing access to information beyond currently available methods. In addition
        to enabling the faceted browsing of artists and their artworks (Figure 7),
        alternative display modes are available to more naturally present geographic
        and temporal information, so that the users can also explore artist birthplaces
        on a map (Figure 8) and view timelines putting different artists&rsquo; lives into
        context (Figure 9). Again, these myriad ways of exploring the content will
      support the varied requirements of an individual as mentioned in Need 3. </p>
      <p class="caption"><a name="fig7" id="fig7"></a><a href="marlow-fig7.html"><img src="marlow.fig7.jpg" alt="figure 7" width="400" height="287" /></a></p>
      <p class="caption"><a href="marlow-fig7.html">Fig 7: Sample faceted browsing for American male artists in
      the 20th century</a></p>
      <p class="caption"><a name="fig8" id="fig8"></a><a href="marlow-fig8.html"><img src="marlow.fig8.jpg" alt="figure 8" width="400" height="274" /></a></p>
      <p class="caption"><a href="marlow-fig8.html">Fig 8: Sample map depicting artists&rsquo; birthplaces (using
      Google Maps)</a></p>
      <p class="caption"><a name="fig9" id="fig9"></a><a href="marlow-fig9.html"><img src="marlow.fig9.jpg" alt="figure 9" width="400" height="268" /></a></p>
      <p class="caption"><a href="marlow-fig9.html">Fig 9: Sample artists&rsquo; timeline view</a></p>
      <h1>6. Conclusion</h1>
      <p>This paper has described the design process for the first prototype
        system of a targeted search engine for the cultural heritage domain. The
        process began with an analysis of user requirements which were then fed into a
        list of functional specifications being implemented as a series of prototypes.
        The working prototype was evaluated with typical users, which then yielded
        recommendations and improvements for the next cycle of redesign. It is expected
        that the system will provide added benefit to individuals searching for
        cultural heritage material on-line through the provision of aggregation,
        specialized multimedia interaction facilities, and the potential of exploring
        semantic relationships between creators and their creations. Beyond the context
        of MultiMatch, the work presented here reveals how an understanding of cultural
        heritage professional user needs can also be of use to others developing
      information systems for this type of audience. </p>
      <h2>Acknowledgements</h2>
      <p class="AcknowedgementsText">We would like to acknowledge the input of all
        MultiMatch partners for contributions to this paper in the form of the
        interactive prototype. Work  is partially supported by European Community under
        the Information Society Technologies (IST) programme of the 6th FP for RTD -
      project MultiMatch contract IST-033104. </p>
      <h2>References</h2>
      <p class="ReferencesText">Amato, G., C. Gennaro, P. Savino and F. Rabitti (2004)<i>. </i>Milos:
      A Multimedia Content Management System for Digital Library Applications. In <i>Research and Advanced Technology for Digital Libraries</i> <i>ECDL 2004.</i> Bath, U.K., Springer 14-26.</p>
      <p class="ReferencesText">Brown, S., R. Ross, D. Gerrard, M. Greengrass &amp; J.
        Bryson (2006). RePAH: Research portals in the Arts and Humanities: A user
      analysis project. <a href="http://repah.dmu.ac.uk/report">http://repah.dmu.ac.uk/report</a>.</p>
      <p class="ReferencesText">Chen, H. (2001). &ldquo;An analysis of image queries in the
      field of art history&rdquo;. <i>JASIST,</i> 52(3), 260-273.</p>
      <p class="ReferencesText">Choi, Y., &amp; Rasmussen, E.M. (2003). Searching
        for Images: The Analysis of Users' Queries for Image Retrieval in American
      History. JASIST, 54(6), 498-511.</p>
      <p class="ReferencesText">Collins, K. (1998). &ldquo;Providing subject access to
      images: A study of user queries&rdquo;. <i>The American Archivist</i>, 61, 36-55.</p>
      <p class="ReferencesText">Cunningham, S.J., D. Bainbridge &amp; M. Masoodian
        (2004). &ldquo;How people describe their image information needs: A grounded theory
        analysis of visual arts queries&rdquo;. <i>JCDL</i>,
      47-48.</p>
      <p class="ReferencesText">Economou, M. (2002). National Council on Archives and
        National Archives Network User Research Group (NANURG): User Evaluation: Report
      of Findings. <a href="http://www.mla.gov.uk/resources/assets/n/nanurg_pdf_4796.pdf">http://www.mla.gov.uk/resources/assets/n/nanurg_pdf_4796.pdf</a></p>
      <p class="ReferencesText">Faulkner, X. (2000). <i>Usability Engineering</i>. Houndmills: Macmillan.</p>
      <p class="ReferencesText">Frost, C.O., B. Taylor, A. Noakes, S. Markel, D. Torres
        &amp; K.M. Drabenstott (2000). &ldquo;Browse and Search Patterns in a Digital Image
        Database&rdquo;. <i>Information Retrieval</i>, 1,
      287-313.</p>
      <p class="ReferencesText">HEIRNET. (2002). Historic Environment Information
      Resources Network: Users and their uses of HEIRs. <a href="http://www.britarch.ac.uk/HEIRNET/users.pdf">http://www.britarch.ac.uk/HEIRNET/users.pdf</a>.</p>
      <p class="ReferencesText">Ireson, N. &amp; J. Oomen (2007). <a
href="http://www.delos.info/files/pdf/DELOS%20Multimatch%202007/papersdelostirrenia.pdf">Capturing e-Culture: Metadata in MultiMatch</a>.
        In <i>Proc. DELOS-MultiMatch workshop</i>.
      February 2007, Tirrenia, Italy. </p>
      <p class="ReferencesText">Jones, G.J.F., Y.. Zhang, Y., F. Fantino,
        E. Newman, and F. Debole (2007). &ldquo;Multilingual Search for Cultural Heritage
        Archives by Combining Multiple Translation Resources&rdquo;. In <i>Proc. of the ACL
      Workshop on Language Technology for Cultural Heritage Data</i> (LaTeCH 2007). Prague, Czech Republic, June 2007.</p>
      <p class="ReferencesText">Khoon, L., C. Ramaiah &amp; S. Foo (2002). &ldquo;The design
        and development of an online exhibition for heritage information access&rdquo;. In J.
      Trant and D. Bearman (eds.) <i>Museums and the Web 2002: Proceedings. </i>Pittsburgh: Archives &amp; Museums Informatics. <a href="http://www.archimuse.com/mw2002/papers/ramaiah/ramaiah.html">http://www.archimuse.com/mw2002/papers/ramaiah/ramaiah.html</a></p>
      <p class="ReferencesText">Minelli, S., J. Marlow, P. Clough, J. Cigarran, J.
        Gonzalo, &amp; J. Oomen  (2007). &ldquo;Gathering requirements for multilingual search
          of audiovisual material in cultural heritage&rdquo;. To appear in <i>Proc.
            of Workshop on User Centricity &ndash; state of the art</i> (16th IST Mobile and Wireless Communications
      Summit), Budapest, Hungary, 1-5 July 2007. </p>
      <p class="ReferencesText">O&rsquo;Dwyer, A. (2007). User Profiles and Requirements
      Plan. Deliverable 6.1 for VideoActive Project. (Internal document). </p>
      <p class="ReferencesText">Pask, A. (2005). &ldquo;Art Historians' Use of Digital
        Images: A Usability test of ARTstor&rdquo;. Dissertation at University of North
      Carolina, Chapel Hill. </p>
      <p class="ReferencesText">Petrelli, D., P. Hansen , M. Beaulieu, M. Sanderson, G.
        Demetriou, P. Herring. (2004). &ldquo;Observing Users - Designing Clarity: A Case
      study on the user-centred design of a cross-language retrieval system&rdquo;. <i>JASIST,</i> 55(10), 923-934.</p>
      <p class="ReferencesText">Preece, J., Y. Rogers and H. Sharp (2002). <i>Interaction
      design: Beyond human-computer interaction.</i> New York: Wiley. </p>
      <p class="ReferencesText">Rubin, J. (1994). <i>Handbook of usability testing: how
        to plan, design, and conduct effective tests</i>.
      New York: Wiley.</p>
      <p class="ReferencesText">Sexton, A., C. Turner, G. &amp; S. Hockey (2004).
        &ldquo;Understanding users: A prerequisite for developing new technologies&rdquo;. <i>Journal
      of the Society of Archivists</i>, 25(1), 33-49.</p>
      <p class="ReferencesText">Schreiber, G., A. Amin, M. van Assem, V.
        de Boer, L. Hardman et al. (2006). &ldquo;MultimediaN: E-Culture
      Demonstrator&rdquo;. <i>LNCS</i> 4273, 951-958.</p>
      <p class="ReferencesText">Sinclair, P., P. Lewis, K. Martinez, M. Addis, A.
        Pillinger &amp; D. Prideaux  (2005). &ldquo;eCHASE: Exploiting cultural heritage
        using the semantic Web&rdquo;. <i>Proceedings of the 4<sup>th</sup> International
      Semantic Web Conference,</i> ISWC 2005, Galway.</p>
      <p class="ReferencesText">Smith, R., D. Howes, W. Shapiro, &amp; H. Witchey
        (2005). &ldquo;Shaping Pachyderm 2.0 with user requirements&rdquo;. In J. Trant and D.
        Bearman (eds.) <i>Museums and the Web 2005: Proceedings.</i> Toronto: Archives &amp; Museum Informatics,
      published March 31, 2005 at <a href="http://www.archimuse.com/mw2005/papers/smithR/smithR.html">http://www.archimuse.com/mw2005/papers/smithR/smithR.html</a></p>
    <!-- InstanceEndEditable --> </div>
  				<h4>Cite as:</h4>
				<p class="references"><!-- #BeginEditable "OnlineCitation" -->Marlow, J., et al., The Multimatch Project: Multilingual/Multimedia Access To Cultural Heritage On The Web<!-- #EndEditable -->, 
				in J. Trant and D. Bearman (eds.). <em>Museums and the Web 2008: Proceedings</em>,
				 Toronto: Archives &amp; Museum Informatics. Published March 31, 2008. Consulted  

                 <script language="JavaScript" type="text/javascript">
               <!-- <![CDATA[
// current date - based on http://rainbow.arch.scriptmania.com/scripts
// Array of day names
var dayNames = new Array("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday");

var monthNames = new Array("January","February","March","April","May","June","July",
                           "August","September","October","November","December");

var dt = new Date();
var y  = dt.getYear();

// Y2K compliant
if (y < 1000) y +=1900;

document.write(monthNames[dt.getMonth()] + " " + dt.getDate() + ", " + y + ". ");
	                // ]]> -->
				  </script>

http://www.archimuse.com/mw2008/papers/<!-- #BeginEditable "URL" -->marlow/marlow.html
				 <!-- #EndEditable --></p>
</div>
  <!--htdig_noindex-->
  
  <div id="sidebar">
    <div id="side-nav">
      <ul id="side-nav-list">
        <li id="workshops"> <a href="../../workshops/index.html" class="menu">Workshops</a> </li>
        <li id="sessions"> <a href="../../sessions/index.html" class="menu" >Sessions</a> </li>
        <li id="speakers"> <a href="../../speakers/index.html" class="menu" >Speakers</a> </li>
        <li id="interactions"> <a href="../../interact/index.html" class="menu">Interactions</a> </li>
        <li id="demonstrations"> <a href="../../demos/index.html" class="menu">Demonstrations</a> </li>
        <li id="events"> <a href="../../events/index.html" class="menu">Events</a> </li>
        <li id="exhibits"> <a href="../../exhibit/index.html" class="menu">Exhibits</a> </li>
        <li id="best"> <a href="../../best/index.html" class="menu">Best of the Web </a></li>
        <li id="pc"> <a href="../../thanks/index.html" class="menu">Committees</a></li>
      </ul>
    </div>
    <div id="search-site">
<form method="get" id="searchform" action="http://wp.museumsandtheweb.com/">
<input type="text" class="field" name="s" id="s" placeholder="Search">
<input type="submit" class="submit" name="submit" id="searchsubmit" value="Search">
</form>
    </div>
    <div id="sidebar-links">
      <ul id="sidebar-links-list">
        <li><a href="../../../index.html" class="produced-by">produced by<br />
          <img src="../../images/sidelogo.png" width="144" height="37" alt="AMI logo" /></a></li>
        <li><a href="../../../index.html">Join our mailing list</a></li>
        <li><a href="http://search.museumsandtheweb.com/search">Search A&amp;MI<br />
          <img src="../../images/search.gif" alt="search" width="24" height="25" border="0" /></a></li>
      </ul>
    </div>
    <div id="sidebar-acknowledge">
      <p><img src="../../images/PCH-logo.png" alt="PCH" width="160" height="22" /><br />
        Presented  in conjunction with the<br />
        Department of Canadian Heritage through the <a href="http://www.chin.gc.ca/" target="_blank">Canadian
      Heritage Information Network (CHIN)</a> and <a href="http://www.pch.gc.ca/progs/pcce-ccop/index_e.cfm" target="_blank">Canadian Culture Online (CCO)</a>.</p>
      <div id="sponsor">
 <p>Sponsored by<br />
   <a href="../../exhibit/ex_335001860.html">Interwoven</a> and <a href="../../exhibit/ex_335001863.html">Interflow</a></p>
</div>
    </div>
   <!--/htdig_noindex-->
  </div>
  <div id="footer">
   <div id="last-updated">published April 9, 2008<br />
	 last updated:
	   <!-- #BeginDate format:Am1a -->October 27, 2010 10:14 PM<!-- #EndDate --><br />
      <div id="cc">
        <!--htdig_noindex-->
        <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img src="http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0  License" width="88" height="31" style="border-width:0" /></a>
        <!--/htdig_noindex-->
      </div>
    </div>
    <!--htdig_noindex-->
    <div id="footer-content">
      <p>Archives &amp; Museum Informatics, 158 Lee Avenue, Toronto, Ontario, M4E
        2P3 Canada<br />
        Telephone: +1 416 691 2516 | Fax: +1 416 352 6025 | E-mail:
        <script language="JavaScript" type="text/javascript">
               <!-- <![CDATA[
	           user = "mw2008";
	           site = "archimuse.com";
	           position = "Page Footer";
	           document.write('<a href=\"mailto:' + user + '@' + site + '\?subject=Response from MW2008 Web Site: ' + position + '\">');
	           document.write( user  + ' @ ' + site + '<\/a>');
	                // ]]> -->
				  </script>
        <br />
        Copyright &copy; 2008 &ndash; Archives &amp; Museum Informatics &ndash; All
        rights reserved.<br />
      </p>
    </div>
    <div style="clear:both"></div>
  </div>
</div>
<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script> 
<script type="text/javascript" src="http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript></noscript>

<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>

<!--/htdig_noindex-->
</body>
<!-- InstanceEnd -->
<!-- Mirrored from www.museumsandtheweb.com/mw2008/papers/marlow/marlow.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 16:58:12 GMT -->
</html>
