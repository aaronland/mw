<html><!-- #BeginTemplate "/Templates/mw2000papers.dwt" --><!-- DW6 -->

<!-- Mirrored from www.museumsandtheweb.com/mw2000/papers/marty/marty.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:50:15 GMT -->
<head>
<!--template applied and analytics code added Nov 2010 -->
<!-- #BeginEditable "doctitle" --> 
<TITLE>Museums and the Web 2000 Marty & Twidale, Encouraging data quality feedback 
from online visitors</TITLE>
<!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=">
<script language="JavaScript">
<!--
function MM_swapImgRestore() { //v2.0
  if (document.MM_swapImgData != null)
    for (var i=0; i<(document.MM_swapImgData.length-1); i+=2)
      document.MM_swapImgData[i].src = document.MM_swapImgData[i+1];
}

function MM_preloadImages() { //v2.0
  if (document.images) {
    var imgFiles = MM_preloadImages.arguments;
    if (document.preloadArray==null) document.preloadArray = new Array();
    var i = document.preloadArray.length;
    with (document) for (var j=0; j<imgFiles.length; j++) if (imgFiles[j].charAt(0)!="#"){
      preloadArray[i] = new Image;
      preloadArray[i++].src = imgFiles[j];
  } }
}

function MM_swapImage() { //v2.0
  var i,j=0,objStr,obj,swapArray=new Array,oldArray=document.MM_swapImgData;
  for (i=0; i < (MM_swapImage.arguments.length-2); i+=3) {
    objStr = MM_swapImage.arguments[(navigator.appName == 'Netscape')?i:i+1];
    if ((objStr.indexOf('document.layers[')==0 && document.layers==null) ||
        (objStr.indexOf('document.all[')   ==0 && document.all   ==null))
      objStr = 'document'+objStr.substring(objStr.lastIndexOf('.'),objStr.length);
    obj = eval(objStr);
    if (obj != null) {
      swapArray[j++] = obj;
      swapArray[j++] = (oldArray==null || oldArray[j-1]!=obj)?obj.src:oldArray[j];
      obj.src = MM_swapImage.arguments[i+2];
  } }
  document.MM_swapImgData = swapArray; //used for restore
}
//-->
</script>
<style type="TEXT/CSS">
<!--
.heading {  font-family: Arial, Helvetica, sans-serif; font-size: 12px; font-style: normal; font-weight: bold}
.normal {  font-family: Arial, Helvetica, sans-serif; font-size: 12px}
.small {  font-family: Arial, Helvetica, sans-serif; font-size: 10px}
.verysmall {  font-family: Arial, Helvetica, sans-serif; font-size: 9px}
.red {  font-family: Arial, Helvetica, sans-serif; font-size: 12px; font-style: normal; font-weight: bold; color: #660000}
.author { font-family: Arial, Helvetica, sans-serif; font-size: 10px; font-style: normal; font-weight: bold ; text-align: left}
.abstract { font-family: Arial, Helvetica, sans-serif; font-size: 10pt; text-align: left; text-indent: 0in; left: 0in; clip:   rect(   )}
.caption { font-family: Arial, Helvetica, sans-serif; font-size: 10pt; font-style: italic; text-align: center}
.workscited { font-family: Arial, Helvetica, sans-serif; font-size: 12px ; text-align: left; text-indent: .5in}
-->
</style></head>

<body bgcolor="#FFFFFF" background="../../mw.images/mw2000_bg.gif" text="#000000" link="#003399" vlink="#660000" onLoad="MM_preloadImages('../../nav/mw2000_nav_registeron.gif','#943398414283');MM_preloadImages('../../nav/mw2000_nav_workshopson.gif','#943398455733');MM_preloadImages('../../nav/mw2000_nav_sessionson.gif','#943398500450');MM_preloadImages('../../nav/mw2000_nav_speakerson.gif','#943398534416');MM_preloadImages('../../nav/mw2000_nav_demoson.gif','#943398577050');MM_preloadImages('../../nav/mw2000_nav_exhibiton.gif','#943398612066');MM_preloadImages('../../nav/mw2000_nav_eventson.gif','#943398640266');MM_preloadImages('../../nav/mw2000_nav_beston.gif','#943398663166');MM_preloadImages('../../nav/mw2000_nav_keydateson.gif','#943398701050');MM_preloadImages('../../nav/mw2000_nav_minneapolison.gif','#943398731383');MM_preloadImages('../../nav/mw2000_nav_sponsoron.gif','#943398934733')">
<span class="author"></span> <span class="abstract"></span> 
<table width="600" border="0" cellspacing="2" cellpadding="5">
  <tr> 
    <td width="145" align="LEFT" valign="TOP" height="646"> 
      <p><a href="../../index.html" target="_blank"><img src="../../mw.images/mw.gif" width="112" height="155" border="0" alt="MW2000"></a></p>
      <p> <a href="../../register/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.register','document.register','../../nav/mw2000_nav_registeron.gif','#943398414283')"><img name="register" border="0" src="../../nav/mw2000_nav_registeroff.gif" width="125" height="15" alt="Register"></a> 
	<br>
	<a href="../../workshops/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.workshops','document.workshops','../../nav/mw2000_nav_workshopson.gif','#943398455733')"><img name="workshops" border="0" src="../../nav/mw2000_nav_workshopsoff.gif" width="125" height="15" alt="Workshops"></a> 
	<br>
	<a href="../../sessions/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.sessions','document.sessions','../../nav/mw2000_nav_sessionson.gif','#943398500450')"><img name="sessions" border="0" src="../../nav/mw2000_nav_sessionsoff.gif" width="125" height="15" alt="Sessions"></a> 
	<br>
	<a href="../../speakers/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.speakers','document.speakers','../../nav/mw2000_nav_speakerson.gif','#943398534416')"><img name="speakers" border="0" src="../../nav/mw2000_nav_speakersoff.gif" width="125" height="15" alt="Speakers"></a> 
	<br>
	<a href="../../demos/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.demos','document.demos','../../nav/mw2000_nav_demoson.gif','#943398577050')"><img name="demos" border="0" src="../../nav/mw2000_nav_demosoff.gif" width="125" height="15" alt="Demonstrations"></a> 
	<br>
	<a href="../../exhibit/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.exhibits','document.exhibits','../../nav/mw2000_nav_exhibiton.gif','#943398612066')"><img name="exhibits" border="0" src="../../nav/mw2000_nav_exhibitoff.gif" width="125" height="15" alt="Exhibits"></a> 
	<br>
	<a href="../../events/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.events','document.events','../../nav/mw2000_nav_eventson.gif','#943398640266')"><img name="events" border="0" src="../../nav/mw2000_nav_eventsoff.gif" width="125" height="15" alt="Events"></a> 
	<br>
	<a href="../../best/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.best','document.best','../../nav/mw2000_nav_beston.gif','#943398663166')"><img name="best" border="0" src="../../nav/mw2000_nav_bestoff.gif" width="125" height="15" alt="Best of the Web"></a> 
	<br>
	<a href="../../dates/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.dates','document.dates','../../nav/mw2000_nav_keydateson.gif','#943398701050')"><img name="dates" border="0" src="../../nav/mw2000_nav_keydatesoff.gif" width="125" height="15" alt="Key Dates"></a> 
	<br>
	<a href="../../minneapolis/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.minneapolis','document.minneapolis','../../nav/mw2000_nav_minneapolison.gif','#943398731383')"><img name="minneapolis" border="0" src="../../nav/mw2000_nav_minneapolisoff.gif" width="125" height="15" alt="Minneapolis"></a> 
	<br>
	<a href="../../sponsor/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('document.sponsor','document.sponsor','../../nav/mw2000_nav_sponsoron.gif','#943398934733')"><img name="sponsor" border="0" src="../../nav/mw2000_nav_sponsoroff.gif" width="125" height="15" alt="Sponsor"></a> 
	<br>
	<br>
	<br>
	<span class="small"><a href="http://search.museumsandtheweb.com/conferences/">When is the next 
    <br>
    Museums and the Web?</a></span><br>
    <br> 
    <a href="http://www.archimuse.com/" target="_top"><img src="../../../external.gif?link=http://www.archimuse.com/ami.images/nav/nav_ami.gif" width="135" height="25" border="0" alt="A&amp;MI home"></a> 
		<br>
        <font face="Arial, Helvetica, sans-serif" class="verysmall">Archives &amp; 
        Museum Informatics<br>
        158 Lee Avenue<br>
        Toronto, Ontario<br>
        M4E 2P3 Canada<br>
        info@ archimuse.com<br>
	<a href="http://www.archimuse.com/">www.archimuse.com</a></font></p>
      <table width="74">
	<tr> 
	  <td> <a href="http://search.museumsandtheweb.com/search" target="_top"> <img src="../../nav/search.gif" width="24" height="25" alt="Search" border="0" name="Search"> 
	    </a> </td>
	  <td valign="MIDDLE"> <a href="http://search.museumsandtheweb.com/search"> 
	    <span class="small"><font face="Arial, Helvetica, sans-serif"><span class="verysmall">Search 
	    <br>
	    A&amp;MI</span></font></span></a> </td>
	</tr>
      </table>
      <p><span class="smallvery"><span class="small"><font face="Arial, Helvetica, sans-serif" class="verysmall">Join 
	our <a href="http://search.museumsandtheweb.com/mailinglist/"> Mailing List</a>. 
	<br>
	<a href="http://search.museumsandtheweb.com/terms-of-use-privacy/"> Privacy</a>.</font></span></span> 
      </p>
      <p class="verysmall">Copyright<br>
	Archives &amp; Museum Informatics<br>
	2000 </p>
    </td>
    <td width="429" align="LEFT" valign="TOP" height="646"><!-- #BeginEditable "Body of Page" --> 
      <p><A HREF="../../speakers/index.html"><img src="../papers.gif" width="400" height="66" BORDER="0"></A></p>
      <p class="heading"><b>Unexpected help with your web-based collections: Encouraging 
        data quality feedback from your online visitors</b></p>
      <p class="author"><b>Paul F. Marty and Michael B. Twidale, Graduate School 
        of Library and Information Science University of Illinois, USA</b></p>
      <p align="CENTER" class="heading"><b>Abstract</b></p>
   <blockquote>
          
        <p align="JUSTIFY" class="small">This paper examines how monitoring the 
          access and use of web-based museum collections can improve the overall 
          quality of artifact data. All databases have quality problems that arise 
          both from input errors and also from errors that emerge over time; museum 
          databases are no exception. Indeed, the complexity and variety of the 
          information recorded in museum databases actually adds to the probability 
          of such errors occurring. Given the expense of checking through each 
          record for errors, we here present alternative approaches that can contribute 
          to the improvement of data quality of web-accessible museum database 
          systems. </p>
    </blockquote>
      <p align="JUSTIFY" class="heading"><b>Introduction</b></p>
      <p align="JUSTIFY"><span class="normal">All organizations collect data and 
        then use, analyze, manipulate, add to and modify it. Increasing automation 
        offers both potential cost savings as well as opportunities for new forms 
        of data manipulation hitherto impossible or infeasibly expensive. However, 
        as many failed automation attempts of white collar work have shown, it 
        is all too easy for an impressively efficient system to be unable to cope 
        with exceptions, and particularly with erroneous or unexpected data. Although 
        some problems were clearly present in older paper-based systems, others 
        have appeared as a product of automation itself. For example, when a paper 
        form passes through many hands, there is at least the possibility of errors 
        being detected and remedied by alert members of staff. However, where 
        much of the same work is partially automated, there will be fewer eyes 
        looking at the data; moreover, those spotting a problem may not be able 
        to report or correct it easily.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">This paper explores ways of reintroducing 
        such aspects of informal error checking back into advanced systems, while 
        maintaining the cost and functionality advantages of those systems. We 
        explore the possibilities for improving the quality of a systemís data 
        by taking advantage of the usage of that same data by people both inside 
        and outside the organization. We use the results of a study of data management 
        in a particular museum to provide exemplars of good practice and raise 
        wider issues for consideration.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Our approach has been to examine 
        and understand existing practice and then consider how activities may 
        be extended by suitable changes involving the use of new systems functionalities. 
        In particular, we consider how the insights and techniques from Computer 
        Supported Cooperative Work (CSCW) may be applied in this area to support 
        the goal of improving data quality. By such an analysis, we are able to 
        formulate a number of research questions, and, we hope, encourage a debate 
        on this method of analyzing the problem. Our aim is to uncover issues 
        applicable not merely to other museums, but also to related organizations 
        such as libraries and archives, and ultimately to databases in general.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">We believe that databases should 
        make it easy for their users to indicate the existence of an error so 
        it can be corrected by a suitably authorized person. This process relies 
        on the altruistic behavior of the users and so raises questions of why 
        and whether people will bother, as well as issues of how to minimize the 
        effort required. We provide preliminary evidence for the plausibility 
        of such activity occurring in certain circumstances. We also explore a 
        complementary approach where metadata (including usage information) is 
        employed to predict data elements that have a greater probability of being 
        erroneous and so prioritize the data checking activity.</span></p>
      <p align="JUSTIFY" class="heading"><b>Why Museums?</b></p>
      <p align="JUSTIFY"><span class="normal">Museum studies may be regarded by 
        many as unrelated to the fields of advanced computing, interface design 
        and CSCW. Nevertheless, we believe that there is the potential for productive 
        transfer of ideas between these disciplines.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">There is a growing interest in the 
        field of museum informatics, where studies in information science are 
        informing (and we hope, being informed by) the problems and challenges 
        of the museum context. These challenges include managing widespread non-standard 
        data, the evolving knowledge of the meaning of that data, and various 
        attempts to interlink data sources between institutions. Moreover, the 
        use of computers for educational and cultural purposes by visitors (both 
        real and virtual) to museums and the integration of advanced information 
        technology into the daily tasks of museum staff members raises the potential 
        to contribute powerful insights into how technology and the iterative 
        design process can have an important impact across the information society.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Moreover, from the point of view 
        of this study, museums provide accessible examples of general problems 
        that pervade databases of all types. A museum database serves as a useful 
        source of examples of all types of database errors: inaccurate descriptors 
        are typed into fields, data becomes obsolete over time, accurate data 
        is difficult to obtain, and quality of data is often compromised by constraints 
        of time and money. The complexities of cataloguing museum artifacts yield 
        a rich variety of errors. In addition, the public nature of museum research 
        means that this information is generally more accessible for study than 
        the conventional commercial database. Thus, a study of data quality issues 
        in museum databases provides us with a wide variety of examples that can 
        easily generalize to other kinds of database systems. </span></p>
      <p align="JUSTIFY" class="heading"><b>The Context of Case Study</b></p>
      <p align="JUSTIFY"><span class="normal">The Spurlock Museum at the University 
        of Illinois is a cultural heritage museum with a collection of over 45,000 
        ethnographic artifacts from around the world. These artifacts represent 
        a broad spectrum of history and culture ranging from ancient Sumeria to 
        modern day Ecuador, from Paleolithic chipped-stone tools to tiles from 
        the Space Shuttle.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">The museum is currently in the middle 
        of a five-year process begun in 1996 to complete a 100% re-inventory of 
        the museumís collections, as part of a move to a new building. As part 
        of this process, each artifact is being retrieved from storage, analyzed 
        and evaluated, weighed, measured, and photographed, identified and catalogued; 
        the resulting data is stored in specially developed relational databases. 
        Each record in the primary <i>Artifacts</i> database features over one 
        hundred fields tracking such detailed specifications as nomenclature classifications, 
        physical dimensions, material analyses, geographical, cultural, and temporal 
        designations, accession records, artifact histories, exhibit information, 
        scholarly remarks, condition and conservation records, research notes, 
        etc. (Marty, 1999).</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">This effort represents a project 
        of enormous scope, requiring the cooperation of museum staff members, 
        various external experts, and dozens of part-time undergraduate student 
        employees. Since the people involved in this project have varying levels 
        of expertise, there is the potential for many different errors of various 
        types. The challenge (as for many other databases) is to maintain high 
        data quality when it is known beforehand that the work of some participants 
        will necessarily be of relatively low quality. The museum registrar has 
        overall responsibility for monitoring quality and organizing and modifying 
        work practices to maximize this quality under the constraints of budget 
        and time. </span></p>
      <p align="JUSTIFY" class="heading"><b>Data Quality in the Spurlock Museum</b></p>
      <p align="JUSTIFY"><span class="normal">During the process of data gathering, 
        entry, and analysis, an unexpected phenomenon was observed: in accessing 
        and using the data, individuals have consistently detected errors, reported 
        them, and even volunteered corrections. However, these individuals were 
        not directly involved in the task of searching for errors; they were instead 
        working on other tasks.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">For example, a curator working remotely 
        over the Internet to plan a new exhibit for the museum retrieved several 
        artifact records using his web browser. In one of these records, he found 
        an erroneous entry and sent an email to the museumís registrar, noting 
        the error and suggesting a correction.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Because of examples such as these, 
        we decided to undertake a pilot study of error detection and correction 
        of museum databases at the Spurlock Museum. This study has revealed a 
        number of important issues which lead to the development of research questions 
        and design implications. These issues will now be discussed.</span></p>
      <p class="heading"><b>Error Detection</b></p>
      <p align="JUSTIFY"><span class="normal">The detection of errors may occur 
        in various ways: </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal"><i>Example</i>. While browsing a 
        particular data record, the museum registrar notices that an artifact 
        has no weight entered in the database. This problem must be noted, the 
        artifact retrieved, weighed, and value entered into the database. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal"><i>Example</i>. A museum staff member 
        notices that the length of a Merovingian brooch is entered in the database 
        as 3000 cm. Clearly this is an error; brooches are never 30m long. The 
        artifact must be remeasured and the correct value entered into the database.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal"><i>Example</i>. A undergraduate 
        senior in anthropology working at the museum reads an artifact record 
        for an Egyptian earthenware bowl. She notes that the time period designation 
        is entered as &quot;predynastic.&quot; However, the manufacturing process 
        field notes that the artifact was wheel-thrown. Because of her knowledge 
        of Egyptology, she is aware that this represents an inconsistency in the 
        data: predynastic Egyptian pots were not wheel-thrown. However, not being 
        an expert, she doesnít know which field is incorrect, nor (because of 
        her junior status) is she allowed to change them. She reports the problem, 
        but a specialist in Egyptian archaeology must examine the pot to resolve 
        the contradiction. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">From examples such as these, several 
        different types of errors can be identified. </span></p>
      <p class="heading"><b>Input Errors</b></p>
      <p align="JUSTIFY"><span class="normal">The most obvious kind of error is 
        a data input error, where the correct value was known (or knowable) at 
        the time of input, but that value is not what was entered. Such errors 
        can be slips or misconceptions. A slip is when the person entering the 
        data knows the correct value but accidentally enters another one. This 
        is frequently due to typing errors, but can also be caused by interruptions 
        or distractions. A misconception occurs when the person entering the data 
        believes that she knows the correct value and actually enters that value, 
        but it is not correct. The likelihood of a misconception varies with the 
        expertise of the person making the decision. Even an expert may have misconceptions, 
        but a novice is likely to have more.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">In general terms, appropriate ways 
        of dealing with input errors include the use of controlled vocabularies 
        (Lancaster 1986), which can detect some, but not all, typing errors. Selection 
        from menus eliminates the problem of the entry of an invalid entry, but 
        not the problem of an incorrect entry ñ an accidental tremor with the 
        mouse can lead to the selection of the wrong value. Data dictionaries 
        can be used to forbid or flag values outside expected norms and therefore 
        suspicious. Although careful checking will help to reduce the problem, 
        it will not eliminate it. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Jacs&oacute; (1993) notes the contribution 
        that effective use of controlled vocabularies can make to the problem. 
        Nevertheless, problems remain with existing commercial databases, including 
        typographic and spelling errors and the use of some fields as a dumping 
        ground for values that don't fit into the databaseís current field structure. 
        Further problems are caused by legitimate variant spellings, especially 
        in cases where names change over time. In such cases, cross-references 
        are a powerful solution, but problems occur when they are not used universally 
        in the database.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">One possible solution is to have 
        the data entered independently by two people and to check for discrepancies. 
        If errors are rare and randomly distributed, this is effective. It is 
        also, however, expensive, which is the reason it is not done for every 
        entry at the Spurlock, and we would presume, at many other locations.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Another possibility is to split 
        tasks into relative layers of expertise. For example, a novice can be 
        assigned the task of entering already extant data from paper forms into 
        the database system. More advanced students trained in artifact handling 
        procedures can measure the museumís artifacts and record this data on 
        paper forms for later data entry. Finally, the museumís most experienced 
        students are set to such advanced tasks as assigning nomenclature classifications, 
        identifying the provenance, or analyzing the material composition of a 
        given artifact.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">However, even apparently relatively 
        simple activities can lead to confusion. For example, at the Spurlock, 
        the length, width, height, and other dimensions of each artifact is entered 
        into the database. While measuring these values is not difficult, deciding 
        which value goes with which field is often confusing because this presupposes 
        knowledge of the artifactís correct physical orientation. An error here 
        could lead to confusion for exhibit designers about the intended orientation 
        of the piece, resulting in incorrectly shaped exhibit cases or display 
        mounts. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">It is important to note that this 
        sort of error could not be solved by the implementation of controlled 
        vocabularies. Moreover, although ideally all such data should be entered 
        by an expert, this is not always feasible due to financial constraints. 
        In the following sections we shall consider how collaborative approaches 
        can make the best use of the limited resource of expertise.</span></p>
      <p class="heading"><b>Emergent Errors</b></p>
      <p align="JUSTIFY"><span class="normal">Even if a value is known to be correct 
        and entered correctly into the database, it can become incorrect later. 
        Over time, if nothing is done to maintain accuracy a database will degrade 
        in quality. In short, data exhibits entropy. Where that is acknowledged, 
        for example, with cash balance accounts in commercial databases, policies 
        can be put in place to address it. Problems arise, however, when the change 
        is so slow that it is considered imperceptible or not worth the bother 
        of tracking. Furthermore, complicating the problem and leading to what 
        we are calling emergent errors is the phenomenon that scholarship and 
        knowledge evolves. What was once considered a correct attribution may 
        become incorrect in the future.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal"><i>Example</i>. The museum owns 
        many artifacts from the former Soviet Union. The &quot;country&quot; designation 
        for these artifacts was originally entered as &quot;USSR.&quot; As the 
        museum staff re-inventories the collection, these artifacts are being 
        re-classified as coming from &quot;Russia,&quot; &quot;Georgia,&quot; 
        etc. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal"><i>Example</i>. The Spurlock has 
        a sizeable collection of Paleolithic chipped stone tools. In the museumís 
        older ledgers, these are all classified as weapons. Subsequent scholarship 
        has revealed that many of these so-called weapons were actually multi-purpose 
        tools, also used for cooking, scraping, cleaning hides, etc. Therefore, 
        many of these artifacts originally classified as a weapon may now need 
        to be re-classified. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Thus the data in the database has 
        to refer not just to the artifact within the walls of the museum but also 
        in its larger context; whether in the world at large or in the scholarly 
        community. As external events change, whether they are geopolitical in 
        nature or represent an evolving state of knowledge, the database needs 
        to be updated.</span></p>
      <p align="JUSTIFY" class="heading"><b>Collaborative Approaches to Data Quality 
        Improvement</b></p>
      <p align="JUSTIFY"><span class="normal">Given the problems outlined above, 
        we believe that automated and semi-automated solutions, such as the use 
        of controlled vocabularies, although important, and to be strongly advocated, 
        will not solve all the problems. Careful manual checking of every item 
        is infeasibly expensive, not least because data entropy means that it 
        requires an ongoing commitment. We need to consider ways to improve quality 
        that are not prohibitive and which are capable of fitting with existing 
        work practices. The proposed solutions must also acknowledge the wide 
        variation in expertise of the people involved in the process. Our approach 
        has been informed by consideration of several observed cases such as the 
        following illustrative example.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal"><i>Example</i>. A student entering 
        cultural data on a particular Mesopotamian ceramic vase, did not know 
        whether this artifact was Sumerian or Akkadian in origin. Rather than 
        guessing at random or failing to enter anything, she entered the data 
        as Sumerian but appended a note to the record (using a specially designed 
        problems field) indicating her uncertainty in this matter, the nature 
        of the problem, and her recommendation that a specialist in Mesopotamian 
        history double-check the record. This example illustrates that even relatively 
        low status employees, with presumably less institutional commitment, may 
        be willing or can be encouraged to flag problems, provided that we give 
        them suitable mechanisms to do so easily (see below).</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Error checking and creative volunteering 
        of information have been seen in various different ways throughout the 
        museumís system: it is almost commonplace to see phrases such as &quot;I 
        donít know,&quot; &quot;Needs further research,&quot; or even simple question 
        marks scattered throughout the records. Thus, even though this process 
        of recording errors has not been formalized as a required duty of the 
        student employees, these examples serve as evidence that people are not 
        only willing but will find their own ways to indicate quality problems 
        of their own accord. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">The relative frequency of this kind 
        of activity raises a question: can web-accessible museum collections incorporate 
        mechanisms that will allow people browsing the data to report errors they 
        detect and, should they wish, volunteer corrections? Although clearly 
        this will not happen for every database record, we need to understand 
        more about the process of error recognition and correction in order to 
        facilitate it when it does occur. Consequently, it is our hope that if 
        we encourage this activity, both managerially and through appropriate 
        systems re-design, we can produce usage based data quality improvement.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">In general terms, this leads to 
        the following questions:</span></p>
      <ul>
        <p align="JUSTIFY"> 
        <li><span class="normal">Is it possible to take advantage of the use of 
          the data and the expertise of users to identify and remediate errors?</span></li>
        <p></p>
        <p align="JUSTIFY"> 
        <li><span class="normal">What kind of computer support would facilitate 
          this?</span></li>
        <p></p>
        <p align="JUSTIFY"> 
        <li><span class="normal">What kind of organizational and managerial arrangements 
          would facilitate this?</span></li>
        <p></p>
      </ul>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">This approach has precedent elsewhere. 
        Gasser (1986) noted the various workarounds that occurred as workers devised 
        mechanisms to cope with the inflexibility of a computer system, including 
        dealing with erroneous information. Hughes et al. (1992), in a study of 
        air traffic control work, observed that the less experienced assistants 
        picking up flight strips from a computer and moving them to a display 
        sometimes spotted errors in the printouts and reported them. This was 
        an important part of the informal error checking that was integral to 
        the safety culture of air traffic control. The study emphasized the social 
        mechanisms employed to create a total system that was far more reliable 
        than its very fallible components ñ exactly the situation needed for data 
        quality. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Mintz (1990) proposed the FIXIT 
        feature to enable quality control by online searchers of a fee-based database. 
        This attempted to address the barriers to giving feedback to database 
        producers, and to provide a credit for the accessing of erroneous data. 
        FIXIT would switch the user to a free database (avoiding further online 
        charges) and then ask the user for details about the faulty record that 
        had been detected. She further proposed that the search services should 
        publish details about how many corrections were sent to each provider 
        and how many linked records were added to correct the FIXITs sent. Rather 
        than just giving a credit for each erroneous record the user had retrieved, 
        Mintz suggested that the database providers pay double for error reports 
        in order to encourage them. </span></p>
      <p class="heading"><b>Encouraging Data Quality Feedback</b></p>
      <p align="JUSTIFY"><span class="normal">In particular, one very important 
        question is whether people will actually bother to volunteer this information. 
        It seems reasonable to assume that the more difficult it is to volunteer 
        the information, the less likely it is that a person would bother. Thus 
        a design challenge is to develop mechanisms that are as low cost as possible 
        while also exploring the potential benefits of use. What are the incentives 
        for people to volunteer their help? These can include a commitment to 
        scholarship, irritation with an observed error, and internal altruistic 
        motives. For some, the intrinsic reward is to see that their actions have 
        an impact. Thus it is important that reports be rapidly acted on since 
        the volunteers of information may check on the results of their suggestions. 
        If they choose to provide contact details they can be thanked and told 
        the consequences of their report.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">This question deserves a much more 
        detailed study. However, we can provide some evidence in the form of observed 
        examples of it occurring. Examples of this form of altruistic error correction 
        behavior date back to the initial stages of the museumís re-inventory 
        process. In late 1997, when digital records for the museumís collections 
        were first made available for review over the Internet, a specialist in 
        African art located in Phoenix, Arizona, examined over 200 records and 
        sent detailed comments on each to the museumís registrar via email. These 
        comments were evaluated by the museum staff for accuracy and subsequently 
        entered into the museumís database systems. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">This type of feedback has continued 
        throughout the re-inventory process. For example, recently, a University 
        of Illinois professor examined from his office a number of on-line records 
        of cuneiform tablets and emailed the registrar with his suggestions and 
        corrections for the records. On occasion, he noted that although the textual 
        data within the record was accurate, the digital photograph which accompanied 
        the record was upside-down!</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">In a related example of end user 
        feedback, Davis (1989) describes work by OCLC (The Online Computer Library 
        Center) to elicit response from the users of their Online Union Catalog 
        about its quality. The process for the users (librarians at affiliated 
        libraries) was laborious, involving the postal mailing of Change Request 
        forms along with photocopied supporting documentation. Accordingly, user 
        feedback generated less than half of the 125,000 records replaced per 
        year, 31% of respondents said they never reported errors, and 42% only 
        reported a few errors. However, what to us is remarkable is that <i>anyone</i> 
        bothered to report errors. We use this as evidence for the feedback approach 
        being feasible, provided that ease of use issues are addressed: 70% of 
        the librarians surveyed said that they would increase their reporting 
        of errors if a more accessible online error reporting system were available.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Orr (1998) also advocates user feedback 
        as a way to maintain data quality (this is the closest in approach to 
        the mechanisms outlined in this paper). He notes the impossibility of 
        perfect data quality and rather focuses concern on quality that is good 
        enough. Where the database is considered part of a feedback control system, 
        there is the possibility of usage statistics leading to the detection 
        and correction of errors. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Raymond (1998b) explores the development 
        of Open Source Software (such as Linux) as a kind of gift economy to account 
        for the apparently altruistic behavior of the participants. Although seemingly 
        very remote from the world of museums and even museum informatics, we 
        believe that it can shed light on the mechanisms that are necessary to 
        encourage collaborative errors detection and recovery, as well as serving 
        as an existence proof for the plausibility of the idea. The gift economy 
        analysis emphasizes that the altruistic behavior needs to be acknowledged, 
        probably publicly. Furthermore, the community of testers and developers 
        needs to be acknowledged, along with continuing feedback that overall 
        improvement is resulting from the collective efforts of that community. 
        We would expect that museum curators and scholars would form the most 
        likely members of an error-correcting community, but in addition to studying 
        how to support them, we believe that it would be worthwhile to at least 
        investigate the possibility of contributions from a wider set of visitors.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">These examples show how data quality 
        may be improved by its usage, that as people look at the data for their 
        own purposes, they may spot errors. This is a valuable resource, but one 
        that is difficult to exploit. Information can come from different kinds 
        of people and involve different degrees of knowledge and detail. It may 
        involve users both within and without of the organization. It may involve 
        experts who can identify the problem and its correct solution, or people 
        who just have a hunch that there may be a problem. The complicating factor 
        is that error correction and remediation may itself involve errors: people 
        who believe there is an error when the data is in fact correct, and people 
        whose suggested corrections to real errors are also erroneous.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">We believe that research into the 
        following two areas can help solve these problems: error metadata and 
        error probability.</span></p>
      <p class="heading"><b>Error Metadata</b></p>
      <p align="JUSTIFY"><span class="normal">Once we move from a desire solely 
        to get data right the first time towards a consideration of continual 
        change in data quality, it becomes important to consider the issues of 
        error metadata. Much can be learned from the use of an earlier data recording 
        technology, the index card. There are many instances where index cards 
        reveal not just the information that was first recorded on them, but how 
        that information has been modified, annotated and added to over time. 
        Often this information includes details of the date and person who made 
        the change or addition, and even why. The resultant information, although 
        often created for reasons of expediency, can in certain cases be more 
        useful than that conventionally obtained by just entering the final values 
        on the index card into a database and destroying the older incorrect one. 
        All this error metadata is of great practical interest in managing error 
        checking and management. It helps to know who has considered the data, 
        when, and why, and what they did. This metadata can help in deciding how 
        much credit to give to the resultant data and how much checking time to 
        devote to it (Baker 1994). </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">We can contrast this with a conventional 
        (computer) database. Certain people have permission to make changes to 
        the data, but this is often by overwriting, obliterating the old information, 
        often with no record of who did it, when, and why. It would seem that 
        such a database ironically suffers from amnesia about itself. Each field 
        in the database should itself have associated with it data about its creation, 
        checking, and modification. This would include date and time information, 
        who was involved and a continuing record of the previous values that have 
        subsequently been updated. Rothenberg (1996) outlines a range of metadata 
        fields that can be used in improving data quality. Our proposals, focusing 
        on aspects of usage and evolution, form a subpart of his wider analysis.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Based on some of our preliminary 
        suggestions, the Spurlock has implemented two systems for tracking error 
        metadata. First, a &quot;modification history&quot; is maintained for 
        each record in the computer system that tracks the digital evolution of 
        the data contained within the record. Every time a modification is made 
        to any field in any record, this fact is recorded in the modification 
        history for that record. Each entry in the modification history logs the 
        nature of the change, who did it, when and why. By tracking the contents 
        of this history throughout the lifespan of a record, the museumís Registrar, 
        among others, is able to gain an understanding of the interaction over 
        time between museum staff members and the computer system. Second, a &quot;problems 
        field&quot; in each record exists for the recording of any inconsistencies, 
        difficulties, or uncertainties that a user of the system might encounter. 
        When a museum employee finds a problem of any type with a given record, 
        he or she records this in the problem field and sets a flag that alerts 
        the museumís Registrar to the existence of a new problem with this record. 
        </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Looking to future developments, 
        we would advocate extending the index card idea into the online world; 
        we could add information about the usage of the database, such as incrementing 
        a counter every time a user (or a certain category of user) looked at 
        it. This is much harder to do in the physical world, although its inspiration 
        is the different physical condition of documents: papers and index cards 
        that are used a lot become dirtier and dog-eared. This idea, termed by 
        Hill &amp; Hollan (1992) &quot;read wear and write wear&quot;, can contribute 
        to improving data quality. As a working hypothesis we propose that heavily 
        used data is in general of higher quality, because the chances of error 
        detection, reporting and correction are higher than that for rarely used 
        data. If this is so, then usage information, although unlikely to be definitive 
        in identifying errors, can contribute to a probabilistic approach to data 
        quality management (as outlined later). We must acknowledge though that 
        the hypothesis is only concerned with an overall trend; a single heavily 
        used data item may still contain an error and indeed by virtue of its 
        heavy use, that erroneous value may be widely believed to be especially 
        reliable. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">There are many different ways in 
        which such metadata might be recorded and considerable variation in the 
        quantity that might be retained. For example, for each field one might 
        provide an ever growing list of its earlier values, when they were entered 
        and who entered them, along with an optional text field for annotations 
        about the updating process. Similarly there are many different kinds of 
        usage data that might be collected. If, for example, the use of a database 
        requires logging in, it would be possible to record a user identity along 
        with each usage incident, although this raises questions of privacy.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">All these ideas need further research 
        to explore their computational and social consequences as well as the 
        degree to which they support cost effective data quality improvement. 
        Of all the costs, that of storing what appears to be substantially more 
        information than is conventionally recorded for a database is perhaps 
        of the least concern. The costs of computer storage continue to fall rapidly 
        and compared to image data (such as one or more high quality photographs 
        of an artifact), this proposed additional textual information will likely 
        be only a marginal increase. The ongoing costs of collection should also 
        be relatively low, since most of the data described can be collected automatically 
        by a suitably designed system. The main additional costs are in adapting 
        the systems in place to include the collection of this information, and 
        the development and use of the systems to make use of the data, including 
        ensuring agreed privacy safeguards.</span></p>
      <p class="heading"><b>Error Probability</b></p>
      <p align="JUSTIFY"><span class="normal">Data mining is a well established 
        technique for discovering useful unexpected features in existing data. 
        We can consider the search for errors in our data as an example of data 
        mining. If we extend the mining metaphor, the spontaneous altruistic error 
        reporting described above can be considered as analogous to the happenstance 
        of a natural outcropping. This can then be used as an indicator of a cluster 
        of linked errors. Drilling for oil is an expensive endeavor; one attempts 
        to improve one's probabilities of a strike by accumulating multiple pieces 
        of contributing evidence. In the same way, once an error is brought to 
        oneís notice it can be used to adjust the probability field of likely 
        error clusters. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">For example, if one Mesopotamian 
        pot is mis-classified as Sumerian instead of Akkadian, one begins to suspect 
        other Mesopotamian pots. In particular, one suspects those classified 
        at about the same time and by the same person. One is also especially 
        suspicious of data items that have been rarely looked at. The linkages 
        between the data that adjust the probabilities can be both aspects of 
        the data itself (pot, Sumerian) and about its metadata (who entered it, 
        when, if it has ever been checked for this or any other error). The probabilistic 
        nature of this approach is important to consider. One could in theory 
        check everything, but usually resources do not permit this. It becomes 
        useful to maximize the potential of identifying errors even when this 
        means that not all errors are detected. Multiple factors can contribute 
        to this information. </span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">For example, in recording details 
        about the geographical provenance of a given artifact, students are asked 
        to enter the artifactís continent, country, region, city, etc. When undertaking 
        a random check of artifacts from Africa, it was noticed that one artifact 
        had in the fields continent, country and region, the values Africa, Kenya 
        and East Africa, respectively. This is incorrect. The region field should 
        be a sub-part of a country (e.g. South Kenya), not a sub-part of a continent. 
        Subsequent analysis led to the discovery that the same individual was 
        responsible for this misinterpretation of the region field in other instances. 
        This fact led the museumís registrar to suspect all geographical entries 
        by this individual and allowed her to search the system for all such entries 
        by this individual and correct them. It is easy to see the cause of the 
        misconception, and consequently worth checking (if time permits) whether 
        the same error occurs elsewhere.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">Another example can be found in 
        the analysis of records pertaining to maps. It was discovered that some 
        students cataloguing these items incorrectly recorded in these same geographic 
        provenance fields the details of the country depicted on the map rather 
        than the geographical information about where the map was created (i.e. 
        a map of West Africa made in Spain would have a value of Africa in the 
        continent field rather than Europe). As in the previous example, once 
        one such error was detected, suspicions were raised about other records. 
        Those map records entered by the same student seem most likely to be in 
        error, followed by (to a lesser extent) all records about maps and other 
        artifacts that depict countries, which could cause the same kind of confusion. 
        The example of the misclassified Paleolithic chipped stone tools discussed 
        above shows the same issue. Once it was realized that one given record 
        of this type was misclassified as a weapon, it was necessary to reassess 
        all similar artifacts.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">These examples serve to show how 
        the data can be regarded as a probability field, with each data item having 
        a certain chance of being incorrect. It is not the case that each item 
        has the same error probability. We can draw on information about prior 
        experience to ascribe higher probabilities based on a range of factors, 
        such as how difficult it is to obtain the correct value for a certain 
        category of artifact, and how expert the person making those decisions 
        is. As new information is obtained (such as discovering the map misconception), 
        the probability field shifts. We can use the information in the probability 
        field to help decide where to allocate scarce expert error-checking resources, 
        that is, where to drill for errors.</span></p>
      <p align="JUSTIFY" class="heading"><b>Conclusions</b></p>
      <p align="JUSTIFY"><span class="normal">Based on an ongoing study of the 
        re-inventorying practices of a particular museum, we have attempted to 
        identify existing collaborative activities that can serve as the inspiration 
        for future technical innovations. By linking those findings with current 
        CSCW research, we have outlined a new collaborative approach to data quality 
        management.</span></p>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">This analysis of the Spurlock Museumís 
        data quality has highlighted the following two approaches:</span></p>
      <ul>
        <p align="JUSTIFY"> 
        <li><span class="normal">Spontaneous volunteering of data quality problems, 
          sometimes accompanied by proposed solutions; and</span></li>
        <p></p>
        <p align="JUSTIFY"> 
        <li><span class="normal">The use of usage information and data quality 
          metadata to inform quality control activities by addressing an evolving 
          error probability field.</span></li>
        <p></p>
      </ul>
      <p align="JUSTIFY"></p>
      <p align="JUSTIFY"><span class="normal">We believe that these approaches 
        offer great potential benefits not only for museum and bibliographic databases, 
        but also for commercial ones. We intend to continue this work by a combination 
        of observational studies and the development and testing of prototype 
        systems embodying the features outlined in this paper.</span></p>
      <p align="JUSTIFY" class="heading"><b>References</b></p>

          
      <p><span class="workscited">Baker, N. (1994). Discards. <i>The New Yorker, 
        70</i>(7), 64-86.</span></p>
      <p><span class="workscited">Davis, C. C. (1989). Results of a survey on 
        record quality in the OCLC database. <i>Technical Services Quarterly, 
        7</i>(2), 43-53.</span></p>
      <p><span class="workscited">Gasser, L. (1986). The integration of computing 
        and routine work. <i>ACM Transactions on Information Systems, 4</i>(3), 
        205-225.</span></p>
      <p><span class="workscited">Hill, W. C., &amp; Hollan, J. D. (1992). Edit 
        wear and read wear. Paper presented at <i>CHI '92: Proceedings of the 
        Conference on Human Factors in Computing Systems,</i> Monterey, CA.</span></p>
      <p><span class="workscited">Hughes, J. A., Randall, D., &amp; Shapiro, D. 
        (1992). Faltering from ethnography to design<i>.</i> Paper presented at 
        <i>CSCW'92: Proceedings of the Conference on Computer-Supported Cooperative 
        Work, </i>Toronto.</span></p>
      <p><span class="workscited">Jacs&oacute;, P. (1993a). Searching for skeletons 
        in the database cupboard Part I: errors of omission. <i>Database, 16</i>(1), 
        38-49.</span></p>
      <p><span class="workscited">Jacs&oacute;, P. (1993b). Searching for skeletons 
        in the database cupboard Part II: errors of commission. <i>Database, 16</i>(2), 
        30-36.</span></p>
      <p><span class="workscited">Lancaster, F. W. (1986). <i>Vocabulary control 
        for information retrieval</i> (Second ed.). Arlington, VA: Information 
        Resources Press.</span></p>
      <p><span class="workscited">Marty, P. (1999). Museum informatics and collaborative 
        technologies. <i>Journal of the American Society for Information Science,</i> 
        50 (12), 1083-1091.</span></p>
      <p><span class="workscited">Mintz, A. P. (1990). Quality control and the 
        zen of database production. <i>Online, 14</i>(6), 15-23.</span></p>
      <p><span class="workscited">Orr, K. (1998). Data quality and systems theory. 
        <i>Communications of the ACM, 41</i>(2), 66-71.</span></p>
      <p><span class="workscited">Raymond, E. S. (1998). Homesteading the Noosphere. 
        <i>First Monday, 3</i>(10).<u> </u><a href="http://www.firstmonday.dk/issues/issue3_10/raymond/">http://www.firstmonday.dk/issues/issue3_10/raymond/</a></span></p>
      <p><span class="workscited">Rothenberg, J. (1996). Metadata to support data 
        quality and longevity<i>.</i> Paper presented at the <i>1st IEEE Metadata 
        Conference,</i> Silver Spring, MD.</span></p>
  
      <p class="author">&nbsp;</p>
      <!-- #EndEditable --></td>
  </tr>
</table>
<!--htdig_noindex-->
<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script>
<script type="text/javascript" src="http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript>
</noscript>
<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>
<!--/htdig_noindex-->
</body>
<!-- #EndTemplate -->
<!-- Mirrored from www.museumsandtheweb.com/mw2000/papers/marty/marty.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:50:15 GMT -->
</html>