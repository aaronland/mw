<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><!-- InstanceBegin template="/Templates/mw2009-papers.dwt" codeOutsideHTMLIsLocked="true" -->

<!-- Mirrored from www.museumsandtheweb.com/mw2009/papers/smith/smith.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 16:56:09 GMT -->
<head>
<!-- InstanceBeginEditable name="HeadPageTitle" -->
<title>Archives &amp; Museum Informatics: Museums and the Web 2009: Paper: Smith, K., The Future of Mobile Interpretation</title>
<!-- InstanceEndEditable --><!-- InstanceBeginEditable name="metaTitle" -->
<meta name="title" content="Archives  &amp; Museum Informatics: Museums and the Web 2009: Proceedings" />
<!-- InstanceEndEditable --><!-- InstanceBeginEditable name="Keywords" -->
<meta name="Keywords" content=", paper, research, MW2009, conference, museum, museums and the web, mw, matw, museums &amp; the web, archives &amp; museum informatics, archives, museums, informatics, digital museums, digital archives, digital art, museums online, archives online, libraries online, world wide web, www, web site, website, museum web site, museum website, howto,  conferences, professional papers, peer-reviewed, peer reviewed, peer review, digital libraries, online exhibits, online exhibitions, on-line" />
<!-- InstanceEndEditable --><!-- InstanceBeginEditable name="Description" -->
<meta name="Description" content="Museums and the Web 2009: the international conference for culture and heritage on-line" />
<!-- InstanceEndEditable --><!-- InstanceBeginEditable name="copyright" -->
<meta name="copyright" content="Archives &amp; Museum Informatics, 2009" />
<!-- InstanceEndEditable -->
<meta name="document-class" content="Published" />
<meta name="document-rating" content="General" />
<meta http-equiv="Content-Language" content="EN" />
<meta name="document-rights" content="Copyrighted Work" />
<!-- InstanceBeginEditable name="charset" -->
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<!-- InstanceEndEditable -->
<link rel="stylesheet" href="../../css/mw2009.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../css/papers.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../css/mw2009-noNews.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../css/print.css" type="text/css" media="print" />
<!--htdig_noindex-->
<!-- InstanceBeginEditable name="ImageRotateScript" --><!-- InstanceEndEditable -->
<!--/htdig_noindex-->
<!--htdig_noindex-->
<!-- InstanceBeginEditable name="PageScript" --><!-- InstanceEndEditable -->
<!--/htdig_noindex-->
</head>
<body>
<div id="container">
  <div id="header" onclick="location.href='../../index.html';" style="cursor:pointer;"> </div>
  <!--htdig_noindex-->
  <div id="top-nav">
    <ul id="top-nav-list">
      <li><a href="../../register/index.html">Register</a></li>
      <li><a href="../../dates/index.html">Key Dates</a></li>
      <li><a href="../../sescal/index.html">Schedule</a></li>
      <li><a href="../../local/index.html">Local Info</a></li>
    </ul>
    <!--/htdig_noindex-->
  </div>
  <div id="header-photo"> 
	 <!-- InstanceBeginEditable name="header-photo" --> <img src="../../images/rotate/header-1.jpg"  height="132" width="229" alt="MW-photo" />
	 <!-- InstanceEndEditable --></div>
<div id="print-title">
	<h1>Museums and the Web 2009: the international conference for culture and heritage on-line</h1>
	<h2>produced by Archives &amp; Museum Informatics</h2>
	<h3>site at http://www.archimuse.com/mw2009/</h3>	 
</div> 
 <div id="date"> <a href="../../sescal/index.html">April 15-18, 2009</a><br />
    <a href="../../local/index.html">Indianapolis, Indiana, USA</a> </div>
  <div id="title">
    <h1> 
		<!-- InstanceBeginEditable name="PageTitle" --> 

		The Future of Mobile Interpretation<!-- InstanceEndEditable --> </h1>
  </div>
  <!--htdig_noindex-->
  <div id="main-content">
    <div id="news">
    </div>
    <!--/htdig_noindex-->
    <div id="intro-paragraph"> 
		<!-- InstanceBeginEditable name="IntroParagraph" --> 
		<h2 class="Author"><a href="../../bios/au_395015498.html">Koven
		  J. Smith</a>, The Metropolitan Museum of Art, USA</h2>
		<h3 class="AbstractTitle">Abstract</h3>
		<p class="AbstractText">The last
    several years have seen museums carefully moving away from outmoded audio
    technology towards richer multimedia devices. However, while there have been a
    handful of successful museum installations of multimedia guides, these devices still
    have yet to take hold in museums in the same way that audio guides have.  This may have less to do with the
    technology itself, and more with the mindset that produces content for the
    technology.  This paper discusses
    the means by which museums might break through these old ways of working and
    begin producing truly next-generation mobile content.</p>
  <p class="AbstractText">Keywords:
    mobiles, handhelds, audio guides, interpretation, multimedia</p>

		<!-- InstanceEndEditable --> 
	</div>
    <div id="body-text"> 
		<!-- InstanceBeginEditable name="BodyText" -->
	    <h2 class="PaperTitle">Introduction</h2>
	    <p>The last
		    several years have seen museums carefully moving away from outmoded audio
		    technology towards richer multimedia devices. However, while there have been a
		    handful of successful museum installations of multimedia guides, these devices still
		    have yet to take hold in museums in the same way that audio guides have. The
		    failure of the majority of handheld projects to date has been blamed on their
		    trying to do too much, using technology that is too complex, too expensive, or
	      &quot;not ready for prime time.&quot; The resulting best practices, as
		    witnessed in the recent symposium on handheld devices at Tate Modern
		    (<a href="../http://tatehandheldconference.pbwiki.com/">http://tatehandheldconference.pbwiki.com/</a>), have emphasized simplifying
		    handheld applications and devices, in effect bringing them into line with
		    traditional audio tours but adding a few visuals. Although a few of these
		    devices may have individually failed as a result of poorly executed complexity,
		    simplification as a broad solution is not the answer. If anything, the failure
		    of these devices to find a voice in museums is because museums are, by and
		    large, not taking full advantage of the capabilities of this new generation of
	    multimedia devices.</p>
	    <p>Multimedia
		    devices represent a break, a sea change, in both content and platform, from
		    audio guides. That is to say, if one thinks of the evolution of mobile
		    interpretive devices as a straight line from AM/FM devices through personal
		    cassette players to the now-ubiquitous random-access mp3 players, multimedia
		    guides do not represent the logical endpoint of that evolution, but rather a
		    parallel and altogether different development. Multimedia guides bring
		    with them a suite of opportunities and difficulties that only <i>occasionally</i> overlap with the
		    opportunities and difficulties associated with audio guides.  Although
		    the technology has changed, the mindset that produces content for the
	    technology has not. </p>
	    <p>It is
		    therefore becoming increasingly apparent that museums need to divert efforts
		    away from an approach in which the device itself drives the content that is
		    created to one in which the mobile platform is merely an endpoint of a given
		    content development effort. Doing this requires first re-analyzing
		    and re-thinking assumptions about mobile interpretation that museums have long
		    since taken for granted,  then using that analysis to take advantage of
		    existing or emergent possibilities, and then settling on a development
	    framework that ensures continuous evolution.  </p>
	    <h2>Question Assumptions</h2>
	    <h3>&quot;A market
	    response to inefficient distribution&quot; </h3>
	    <p>One pervasive
		    notion that has largely been taken for granted is that a &quot;tour&quot; model
		    (selected &quot;stops&quot; with narrative content, accessed either randomly or
		    in sequence) is the appropriate one for a mobile interpretive device. This framework evolved naturally from the traditional docent-led tour,
		    but the methods used themselves evolved not out of <i>preference</i>, but rather out of <i>necessity</i>  &ndash; the medium determined the approach. Audio
		    guides were originally created to replace (or at least make more readily available)
		    the kind of content that was at that time being delivered to visitors via docents
	    leading tours in galleries.</p>
	    <p>In the earliest days of mobile interpretation, audio was the only medium
		    that could reliably deliver that kind of narrative content in a small, portable
		    package. Most
		    museums did not have this content readily available in aural form, meaning that
		    it had to be produced from scratch, involving either a significant investment
		    in production personnel and equipment or the engagement of an outside vendor. All the content was, in effect, hand-made; for each object or exhibition,
		    new content had to be written, edited, recorded, and transferred to the given
		    device. In the end, that high per-object cost, combined with limited
		    storage capacity on a given device, forced museums to be highly selective about
		    which objects (or exhibitions) would be included on a given device. Thus
		    the emphasis was heavy on special exhibitions (in which content development is
		    often funded by an exhibition budget) and carefully-selected &quot;greatest
	    hits&quot; from the permanent collection. </p>
	    <p>The
		    limitations placed on mobile interpretation both by the medium and the
		    high-cost production chain meant that certain practices became ingrained in
	    museums as being inseparable from the very idea of mobile interpretation. These practices include:</p>
	    <ul>
	      <li>Content
	      is developed specifically for the mobile device; </li>
      <li>Content
        is typically tied to specific stops within the physical space (usually objects
        or architectural features); </li>
      <li>Objects
        from the permanent collections are under-represented, in favor of special
        exhibition features; </li>
      <li>Contextual
        material, beyond gallery introductions, is largely absent. </li>
          </ul>
	    <p>Museums have taken what was originally a practical response
	        &ndash; audio  &ndash; to a very real problem  &ndash; how to provide mobile interpretation  &ndash; and
		    have come to assume that the content model that grew out of that practical
		    response is what suits visitors best. To quote Chris Anderson in <i><a
href="../http://www.wired.com/wired/archive/12.10/tail.html">The Long Tail</a>:</i>  &quot;Many of our assumptions about
		    popular taste are actually artifacts of poor supply-and-demand matching  &ndash; a
		    market response to inefficient distribution&quot;  (Anderson, 2004). One
		    sees this already in the few multimedia guides that have been put together. Although the applications developed for these devices are admittedly
		    significantly more sophisticated than even the top-of-the-line audio player,
		    they still preserve its model. The device forces the visitor to consume
		    content via &quot;stops&quot; on a &quot;tour.&quot;  At each stop, the
		    visitor is provided narrative content (typically still audio, often now
		    accompanied by text, images, or video). A user of the device is reduced
		    to being a consumer of information. The device does not react to choices
		    the user makes, nor does it respond to the user's input. Because most of
		    the content is still made by hand, the user is  limited to listening to or
		    viewing content predominantly from special exhibitions and some &quot;greatest
	    hits&quot; from the permanent collection.</p>
	    <h3>&quot;Don't be stingy&quot; </h3>
	    <p>This is not to
		    say that there is anything inherently wrong with the tour model; there is a portion
		    of the museum-going public who will probably always crave this
		    led-by-the-hand, explicitly curated approach. The problem, however, is
		    that the tour model appeals <i>only</i> to
		    this relatively small segment of the museum-going public. The remainder,
		    who might crave the ability to do more than
		    passively consume information, is out of luck. With each new
		    generation of museum-goers able to consume and filter greater quantities of
		    information more quickly and efficiently than the last, the greatest hits model
		    starts to look quite, in the words of <a
href="../http://smithsonian20.si.edu/schedule_webcast6b.html">Colleen Macklin</a>, &quot;stingy&quot;  (Macklin, 2009). Audio guides remove
		    the ability to skim large quantities of information. An entire stop must
		    be consumed, from start to finish, or not at all. The audio is either on,
		    or it's off. The net effect is that museums are forcing people who are
		    accustomed to digesting a lot, and quickly, to digest very little at a snail's
	    pace. </p>
	    <p>Significantly,
		    the greatest hits approach preserves a way of looking at collections that
		    has essentially been discarded in most Web presentations. On the Web,
		    museums are moving away from the &quot;curated highlights&quot; approach
		    towards a model in which the entire collection is available for searching,
		    browsing, and filtering. The curatorial facilitator is no longer the sole
		    means by which a visitor might experience an institution  &ndash; museums now
		    encourage users to self-curate their own groupings from an entire museum's
		    collection. In fact, a whole list of possibilities is available to a
		    user browsing a museum's collection on the Web &ndash; ways that are not available in the
		    physical space &ndash; not least among which is the ability to acquire depth of
		    context (whether via translations, maps, or encyclopedias). Instead
		    of multiple sources of information, the visitor has access only to a single
		    curated 'voice' (even when multiple narrators are used). Instead of
		    viewing information about the entire collection, the visitor must get by with a
		    small slice of information about a few highlighted objects. Instead of
		    slicing-and-dicing an entire collection in multiple ways, visitors must stick
		    to the physical layout of the museum  &ndash; American Decorative Arts, Asian Art,
		    19th Century Paintings, etc. In short, anyone hoping to carve out an
		    experience in the galleries as information-rich as the one on the museum's
	    Web site will leave the building hopelessly frustrated. </p>
	    <h3>Know your audience</h3>
	    <p>All of this
		    speaks to one of the most important un-addressed problems facing museums
		    developing mobile interpretive platforms: <b>the audience for multimedia guides has never been properly defined.</b>  In
		    hundreds of pilot multimedia projects conducted in museums over the last
		    ten years, this basic question has not been answered: is the target audience
		    for multimedia devices the <i>same</i> as
		    the target audience for traditional audio guides?  If it is, then best
		    practices developed to this point still apply. If it is not, then museums
		    must focus further research and development towards understanding this new (or
		    at least different) audience. Museums need to begin asking themselves
		    tough questions. For whom are these devices intended?  What are
		    museums hoping to learn by pursuing pilot projects involving mobile
		    interpretation?  Are these pilot projects pushing the development of
		    mobile media in such a way that specific hypotheses are proved or disproved?
	       As more and more museums begin pursuing these pilot projects, new
		    directions and new means of enhancing the visit must be explored. But
		    what specifically can be done to break museums out of the audio tour
	    mold?  </p>
	    <h2>Do More With What You Have</h2>
	    <h3>Make the entire collection available </h3>
	    <p>First things first. A mobile interpretive device
		    should have some kind of searchable content available for <b>every single</b> object on display (and preferably even for those
		    objects that aren't).  Generally, the core of this content would most
		    likely come from a museum's collections management system. Because this
		    content is often already being used for presentation on the Web and is
		    structured, the threshold for usage is low. Rather than having works in special
		    exhibitions and highlights represent the entirety of content on the device,
		    these objects can be called out as a subset of the larger data pool. The
		    implication here is that textual collections data now represents the core of
		    content on a given platform. This does not mean that handmade audio or
		    video content needs to be purged from the handheld device; it is simply that
		    that content would be augmentative instead of core. When available, that
		    content would be displayed. Indeed, in a 2005 study of the &quot;Tate
		    Collections Guide&quot; pilot, it was concluded that visitors viewed audio/video
		    content when available, but found text to be satisfactory otherwise (personal
	    correspondence between the author and Nancy Proctor, 2009).  </p>
	    <p>Having textual data available for every single object opens
		    up the possibility to search, filter, and group objects. Our users have
		    come to expect this ability on the Web; now give them that same ability in the
		    physical space. <i>Ad hoc</i> grouping
		    means that visitors are no longer restricted to highlights constructed by
		    museum personnel  &ndash; visitors can, in effect, create their own highlights, based
		    on criteria <i>they</i> set. If a
		    visitor wants to see every object in your collection created in 1892, they can
		    do that. If a visitor wants to see every object in your collection
		    donated by a particular benefactor, they can do that, too. The grouping
		    could be arbitrary, as well   &ndash; a visitor could construct their own group by hand
	    as a result of performing multiple search or browse operations. </p>
	    <p>Even this simple step of taking content that already exists
		    and making it available to a handheld device fundamentally transforms the
		    nature of the handheld experience. With the ability to search, group, and
		    filter every object, the device becomes a digital surrogate, an assistant,
		    rather than a tour guide. The device has transformed from merely a content-delivery
	    system to a means of helping to turn a visitor's preferences into action.  </p>
	    <h3>Locate </h3>
	    <p>To truly
		    translate the Web-like experience of discovery into the physical space,
		    however, the handheld device <i>must</i> provide
		    appropriate wayfinding. If the visitor is able to discover objects potentially
		    of interest, but not successfully locate those objects in the gallery space,
		    the handheld device has failed. Object locations should be explicitly
		    mapped, appropriate travel vectors within the gallery space could be defined,
		    and the devices themselves should be location-aware. Currently,
		    wayfinding, whether via maps or text directions, is a problem that might be
		    best tackled by several museums working together. As mapping technology
		    evolves, lightweight (and yet sophisticated) means of solving this problem may
	    present themselves. </p>
	    <h3>Recommend</h3>
	    <p>Once the visitor has successfully navigated his or her way
		    to an object, it is important to ensure that that object doesn't become a
		    navigational dead end. When a user is viewing an object in the physical
		    space, the device should always suggest additional (possibly related) objects
		    that may be of interest. Doing this means that the end result of a given
		    search is actually the beginning of another potential browsing path. The device
		    must therefore incorporate a recommendation engine.  Ideally, this engine would work at the intersection of three
	    vectors: content, location, and preference.</p>
	    <p>Content recommendations would primarily be based on content
		    contained within a museum's collections management system or in related
		    curatorial scholarship. Content recommendations would involve the
		    clustering together of objects based on similar characteristics. In this
		    scenario, objects could be clustered algorithmically based on co-occurrence of terms/phrases
		    in those objects' collection records, or clustered manually based on objects'
		    inclusion in known groups (such as the Hudson River School, for instance). Potentially, content recommendations could also be made via an
	    Amazon-like &quot;others who viewed this object also viewed&quot; scenario.  </p>
	    <p>The location-aware nature of modern handheld devices allows
		    location to also be a factor in making successful recommendations in several
		    ways.  A museum spread out over a
		    large area (such as a sculpture park) might elect to limit additional
		    recommendations to objects that are within comfortable walking distance of the
		    object being currently viewed. A traditional art museum might elect to do
		    exactly the opposite, in that a visitor already in a given gallery might not
		    need to have objects from that same gallery recommended to him or her. If
		    a visitor is traveling along a pre-selected or pre-determined route, the
		    recommendations might be limited to objects with a proximity relation to that
		    route.  Additionally, a museum may
		    wish to feature certain content in its gallery space, and indicate a preference
	    to recommend that content when the visitor is nearby.</p>
	    <p>Preference-based recommendations would involve more active
		    user participation. Internet radio sites like as <a
href="../http://www.pandora.com/">Pandora</a> (<a href="../http://www.pandora.com/">http://www.pandora.com/</a>) and <a href="../http://www.last.fm/">Last.fm</a> (<a
href="../http://www.last.fm/">http://www.last.fm/</a>) derive much of
		    their value by understanding a user's preferences over time and suggesting new
		    content based on that understanding. Similarly, a museum handheld device
		    could track content viewed by a given visitor and recommend additional content
		    based on the cumulative understanding gleaned from that information. The
		    visitor could aid in this process by indicating whether the object, content, or
		    location is relevant to them or not (similar to Pandora's &quot;thumbs
		    up/thumbs down&quot; approach). Recommendations could also be based on a
		    stated preference by the user at any time during the visit. Perhaps an
		    initial search on the handheld device brings up a list of results, but also a
		    list of preference options (which might be derived from a museum's own faceted
		    categorization schemas). For example, a search for &quot;Thomas
		    Cole&quot; might bring up a list of objects on display created by Thomas Cole,
		    but also a list of questions such as, &quot;Are you interested in work by
		    artists of the Hudson River School?&quot; or &quot;Are you interested in works
		    created between 1800 and 1850?&quot;  These preferences could be then used
	    to determine what types of recommendations are made later in the user's visit.   </p>
	    <h3>Contextualize</h3>
	    <p>Beyond this three-pronged recommendation engine, the mobile
		    platform should provide valuable context to the visitor as well. An
		    obvious way to do this would be to give the visitor the ability to compare and
		    contrast objects. The nature of museums is to physically locate objects
		    together by time period, style, or other common themes, removing the ability to
		    see how these objects might compare to other objects within the building, in
		    the same way that two objects might be contrasted in a print publication. It is unlikely that you would see two depictions of the same scene
		    painted 100 years apart on different continents in the same gallery, but you
	    could easily place these two works side-by-side in the device for comparison. </p>
	    <p>The point here is that once the visitor finds a subject of
		    interest, he or she should be able to know as much as the museum knows about
		    that subject <i>while still inside the
		      building and even in front of the artwork that sparked the interest.</i>  A
		    visitor may stumble on that work by Thomas Cole, but find that more so than the
		    individual work itself, he or she is interested in the Hudson River School and
		    artists related to that movement. If he or she so desires, the visitor
		    should then be able to find publication excerpts, artist letters, newspaper
		    articles, and bibliographic citations, then group works by related artists
		    together and map where they are in the building. The platform becomes a
	    portal into the museum's knowledge base. </p>
	    <h2>Provide A Multitude of Unique Experiences</h2>
	    <p>Two of the
		    complaints often lodged against traditional audio guides are that the
		    headphones (or phone receivers) cut off the user from interactions with those
		    around them, and that audio guides force the user to digest information at the <i>guide's</i> pace, rather than the
		    user's. Both of these complaints tie into a much larger problem, which is
		    that the traditional audio guide (and its multimedia descendants) allow for
		    only one type of experience, and that that experience is, by and large,
		    dictated by the institution with little to no true interaction on the part of
		    the visitor. That experience should continue to remain a part of any interpretive
		    device for those visitors who desire it, but the devices of the future should
	    also allow for a multitude of other experiences to occur alongside it.</p>
	    <h3>User-generated content</h3>
	    <p>One way in
		    which museums have attempted to encourage a less passive experience via
		    multimedia guides is through the use of so-called &quot;user-generated
		    content.&quot;  In this scenario, content is actively solicited from the
		    visitor by the museum, usually in the form of comments or responses. The
		    benefit to the user for submitting this type of content generally falls into
	    one of three categories: </p>
	    <ul>
	      <li>The
		      benefit to the user is <b>delayed </b>  &ndash; visitors
		      can bookmark favorite objects as they move through the galleries, accessing
	      further information about those objects post-visit via a Web link; </li>
      <li>The
        benefit to the user is <b>abstract </b>  &ndash; visitors
        can &quot;tag&quot; artworks with terms or add comments or reactions, but are
        unable to do anything specific with this content once it's been entered; </li>
      <li>The
        benefit to the user is <b>nonexistent </b>  &ndash; visitors enter reactions into a guestbook-style application, but these
        reactions are never seen again. </li>
        </ul>
	    <p>Unfortunately, much of this content tends to benefit the
		    institution much more than the visitor. Very little of it
		    fundamentally alters the nature of the visit in any way. Handhelds of the
		    future have to make allowances for content coming from users that truly
		    benefits the users themselves, <i>during the
        visit itself</i>.</p>
	    <h3>Content streams</h3>
	    <p class="MsoListBullet">Facebook, Twitter, FriendFeed, and the like have made the
		    concept of micro-updates in a public forum commonplace. It is not a
		    stretch to imagine incorporating that experience into the gallery environment. Users could enter their reactions, thoughts, and responses into the
		    device via a texting-style interface with a defined character limit. These
		    entries could then be posted to a public stream that everyone in the museum
		    could see. As with most microblogging clients, public replies, direct
		    messages, and pictures taken by the handheld devices themselves could all be
		    incorporated into the stream. This stream could potentially become a
		    valuable discovery tool, particularly when coupled with the location-aware
		    features of the device. As the visitor wanders around the museum, someone
		    posts a photo of a work he or she is interested in. Viewing the stream
		    overlaid onto a map of the museum (&agrave; la <a
href="../http://twittervision.com/">twittervision</a> [<a href="../http://twittervision.com/">http://twittervision.com/</a>]) enables the visitor to figure out in which gallery
		    that photo was taken. The device then provides the visitor with
	    directions to that gallery from the visitor&rsquo;s current location. </p>
	    <p class="MsoListBullet">The stream could be used in a number of other ways, as well. First, the aggregate stream could be parsed to group posts into content
		    or emotional areas. For instance, the institution could encourage the use
		    of hashtags to identify posts about certain types of items, which would
		    separate those posts into a usable stream, as with this Twitter stream showing
		    all posts tagged with #mw2009:  <a
href="../http://search.twitter.com/search?q=MW2009">http://search.twitter.com/search?q=MW2009</a> . An institution could encourage the use of tags like #portrait,
		    #landscape, #still-life or similar to aid in helping other visitors locate
		    certain types of work in the galleries. Additionally, the stream could be
		    parsed for certain emotional key phrases, as with <a
href="../http://twistori.com/">twistori</a>.  Tate Britain has already begun creating emotionally-themed tours (i.e.,
		    a tour of objects if you're feeling blue, happy, depressed, etc.) that are
		    distributed as paper brochures; parsing the stream in this way could
		    potentially be a means for creating these kinds of tours on-the-fly. As a
		    further extension, the emotionally-themed posts could be grouped on a map as
		    well, allowing a visitor to gauge the &quot;emotional health&quot; of the
		    institution at any given moment (i.e., &quot;it certainly appears that there
		    are a lot of angry people in the Rothko exhibition right now!&quot;).  The public microblog stream could also be a way for individual
		    staff voices to interact with visitors in real time. This could take the
		    form of announcements such as posts from docents announcing the start of a tour
		    or an educational program, but it could also encourage interaction with staff
		    at a deeper level. For instance, a conservator could post a message indicating
		    that he is about to deinstall artwork in a particular gallery, and will be
	    available to answer questions.  </p>
	    <h3>Community-based discovery</h3>
	    <p>Beyond actively posting content, users could create other
		    kinds of content that would be helpful for other users as well. We have
		    already witnessed the utility of users using social networks for resource
		    discovery on the Web, via social tagging sites like <a
href="../http://delicious.com/kovensmith">Delicious</a> (<a
href="../http://delicious.com/kovensmith">http://delicious.com/kovensmith</a>) or
		    feed sharing services like <a
href="../http://www.google.com/reader/shared/01569013051303405917">Google Reader</a> (<a href="../http://www.google.com/reader/shared/01569013051303405917">http://www.google.com/reader/shared/01569013051303405917</a>). Moving this type of utility to a mobile device is an obvious next step. Once a visitor has mapped out a self-selected tour or grouping of
		    objects or locations, he or she could publish that collection to the mobile
		    platform for use by other visitors. Lists of these user-generated collections
		    could be voted upon by users as well, giving users the ability to later select
		    from &quot;most traveled&quot; user-generated tours or &quot;most highly
	    rated&quot; object groupings.</p>
	    <p>These types of user-generated content allow the device to go
		    beyond simply being an information delivery vessel, and become a platform for
		    all types of experiences, many of which may yet be unforeseen by the
		    institution that provides the device. The cumulative effect of all of
		    this posting, picture-taking, interacting, and talking is both a more social
		    experience, and one which cannot ever be repeated in the same way twice. The visitor has left the building feeling that he or she has participated
		    in something that both could have <i>only </i>occurred
		    inside the institution (as opposed to on the Web) and could have <i>only</i> occurred on that particular
		    day, at that particular time. The visit has been personalized, and made
	    unique. </p>
	    <h2>Build For the Future</h2>
	    <p>As can be seen
		    from the small number of suggestions posed here, implementing the interpretive
		    device of the future will require that museums challenge their preconceived
		    notions about what these devices should (or can) do, while simultaneously
		    understanding that their visitors are becoming <i>more</i> savvy about technology, not less. Museums will need to
	    build for the future by cycling through a series of steps:   </p>
	    <ol>
	      <li><b>Audience
		      research.</b>  Museums
		      should doggedly focus on understanding how to improve device adoption rates
		      among their visitors. It is simply no longer tenable for a museum to not
		      know why a majority of its audience doesn't use interpretive technology. Results of audience research should be targeted towards specific
	      development goals on the device itself.</li>
      <li><b>Application-centered
        design.</b>  With
        older handheld technologies, there was always the problem that a museum was
        introducing a device that was almost inevitably going to be unfamiliar to most
        visitors. This led to an approach to application design in which the
        emphasis was on simplicity and making a single &quot;foolproof&quot;
        application. iPhones and their ilk, however, have led users to expect a
        complex device with simple applications  &ndash; most iPhone applications do only one
        thing, in the most straightforward way possible. Museums should take
        advantage of this by separating functions into different applications that work
        in concert.  </li>
      <li><b>Pilot
        projects.</b>  To
        date, most pilot projects involving mobile multimedia devices have focused on
        determining, basically, whether or not the technology is viable in the museum
        environment. The next wave of pilot projects must move past this and begin
        to determine <i>what kinds</i> of
        applications and frameworks work best for museums.  Application-centered design means, for instance, that a
        museum could have stable &quot;object finder&quot; applications running on its
        devices while <i>piloting</i> &quot;recommender&quot; applications at the same time, on the same devices. Because the application is being piloted on devices that already have a
        user base, the collated user statistics will better aid audience research
        efforts.</li>
          </ol>
	    <p>Often these steps might be concurrent. In fact, a
		    stable device could introduce a new application as a pilot project, and gauge
		    user feedback and statistics as a form of audience research. Understanding the audience, creating separate applications, and running
		    better pilot projects also mean that museums no longer need to develop mobile
		    experiences in isolation. One institution might develop a good wayfinding
		    application, another a recommendation engine, still another a microblogging
		    client. All of these applications could be made available to all institutions
        &ndash; the community benefits from the community.  </p>
	    <p>In any case,
		    it is clear that museums must begin to push harder to develop mobile
		    experiences that challenge traditional notions of interpretation if they don't wish
		    to  be left designing for yesterday's audience. Doing so means that
		    museums must be willing to discard outmoded approaches when appropriate,
		    incorporate new ideas and content when available, and recognize that the only
		    steady state is that of constant improvement.  Adopting that framework
		    will ensure that mobile interpretation continues to be a vital and important
	    component of the museum experience well into the future. </p>
	    <h2>References</h2>
	    <p class="ReferencesText">Anderson,
	    Chris (2004). The Long Tail. <i>Wired, </i>12.10. <a href="../http://www.wired.com/wired/archive/12.10/tail.html">http://www.wired.com/wired/archive/12.10/tail.html</a></p>
	    <p class="ReferencesText">Macklin,
		    Colleen (2009). Closing Plenary. In <i>Smithsonian
        2.0. </i><a href="../http://smithsonian20.si.edu/schedule_webcast6b.html">http://smithsonian20.si.edu/schedule_webcast6b.html</a></p>
    <!-- InstanceEndEditable --> </div>
  			<div id="citation">
            		
                <h4>Cite as:</h4>
				<p class="references"><!-- #BeginEditable "OnlineCitation" -->Smith, K., The Future of Mobile Interpretation<!-- #EndEditable -->. In J. Trant and D. Bearman (eds). <em>Museums and the Web 2009: Proceedings</em>.
				 Toronto: Archives &amp; Museum Informatics. Published March 31, 2009. Consulted  

                 <script language="JavaScript" type="text/javascript">
               <!-- <![CDATA[
// current date - based on http://rainbow.arch.scriptmania.com/scripts
// Array of day names
var dayNames = new Array("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday");

var monthNames = new Array("January","February","March","April","May","June","July",
                           "August","September","October","November","December");

var dt = new Date();
var y  = dt.getYear();

// Y2K compliant
if (y < 1000) y +=1900;

document.write(monthNames[dt.getMonth()] + " " + dt.getDate() + ", " + y + ". ");
	                // ]]> -->
				  </script>

http://www.archimuse.com/mw2009/papers/<!-- #BeginEditable "URL" -->smith/smith.html
				 <!-- #EndEditable --></p>
	</div>
</div>
  <!--htdig_noindex-->
  
  <div id="sidebar">
    <div id="side-nav">
      <ul id="side-nav-list">
        <li id="workshops"><a href="../../workshops/index.html" class="menu">Workshops</a></li><li id="sessions"><a href="../../sessions/index.html" class="menu">Sessions</a></li><li id="speakers"><a href="../../speakers/index.html" class="menu">Speakers</a></li><li id="interactions"><a href="../../interact/index.html" class="menu">Interactions</a></li><li id="demonstrations"><a href="../../demos/index.html" class="menu">Demonstrations</a></li><li id="events"><a href="../../events/index.html" class="menu">Events</a></li><li id="exhibits"><a href="../../exhibit/index.html" class="menu">Exhibits</a></li><li id="best"> <a href="../../best/index.html" class="menu">Best of the Web </a></li><li id="pc"><a href="../../thanks/index.html" class="menu">Committees</a></li><li  id="community"><a href="../http://conference.archimuse.com/" class="menu">Community On-line</a></li>
      </ul>
    </div>
    <div id="search-site">
<form method="get" id="searchform" action="http://wp.museumsandtheweb.com/">
<input type="text" class="field" name="s" id="s" placeholder="Search">
<input type="submit" class="submit" name="submit" id="searchsubmit" value="Search">
</form>
    </div>
        <div id="addthis">
    <!-- AddThis Button BEGIN -->
	<script type="text/javascript">var addthis_pub="archimuse";</script>
	<script type="text/javascript">var addthis_brand = "archimuse";</script>
	<script type="text/javascript">var addthis_options = 'twitter, facebook, linkedin, delicious, email, digg, favorites, more';</script>
	<a href="../http://www.addthis.com/bookmark.php?v=20" onmouseover="return addthis_open(this, '', '[URL]', '[TITLE]')" onmouseout="addthis_close()" onclick="return addthis_sendto()"><img src="../../../external.gif?link=http://s7.addthis.com/static/btn/lg-share-en.gif" width="125" height="16" alt="Share"/></a><script type="text/javascript" src="../http://s7.addthis.com/js/200/addthis_widget.js"></script><!-- AddThis Button END -->
</div>
    
    <div id="sidebar-links">
      <ul id="sidebar-links-list">
        <li><a href="../../../index.html" class="produced-by">produced by<br />
          <img src="../../images/sidelogo.png" width="144" height="37" alt="AMI logo" /></a></li>
        <li><a href="../../../index.html">Join our mailing list</a></li>
        <li><a href="../http://search.museumsandtheweb.com/search">Search A&amp;MI<br />
          <img src="../../images/search.gif" alt="search" width="24" height="25" border="0" /></a></li>
      </ul>
    </div>
    <div id="sidebar-acknowledge">

      <div id="sponsor">
</div>
    </div>
   <!--/htdig_noindex-->
  </div>
  <div id="footer">
    <div id="last-updated"> last updated: <!-- #BeginDate format:Am1a -->October 27, 2010 4:57 PM<!-- #EndDate -->
      <br />
[analytics code]<br />
<div id="cc">
        <!--htdig_noindex-->
        <a rel="license" href="../http://creativecommons.org/licenses/by-nc-nd/3.0/"><img src="../http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0  License" width="88" height="31" style="border-width:0" /></a>
        <!--/htdig_noindex-->
      </div>
    </div>
    <!--htdig_noindex-->
    <div id="footer-content">
      <p>Archives &amp; Museum Informatics, 158 Lee Avenue, Toronto, Ontario, M4E
        2P3 Canada<br />
        Telephone: +1 416 691 2516 | Fax: +1 416 352 6025 | E-mail:
        <script language="JavaScript" type="text/javascript">
               <!-- <![CDATA[
	           user = "mw2009";
	           site = "archimuse.com";
	           position = "Page Footer";
	           document.write('<a href=\"mailto:' + user + '@' + site + '\?subject=Response from MW2008 Web Site: ' + position + '\">');
	           document.write( user  + ' @ ' + site + '<\/a>');
	                // ]]> -->
				  </script>
        <br />
        Copyright &copy; 2009 &ndash; Archives &amp; Museum Informatics &ndash; All
        rights reserved.<br />
      </p>
    </div>
    <div style="clear:both"></div>
  </div>
</div>
<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script> 
<script type="text/javascript" src="../http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript></noscript>

<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>
<!--/htdig_noindex-->
</body>
<!-- InstanceEnd -->
<!-- Mirrored from www.museumsandtheweb.com/mw2009/papers/smith/smith.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 16:56:09 GMT -->
</html>
