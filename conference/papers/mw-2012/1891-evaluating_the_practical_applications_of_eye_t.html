<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">

<!-- Mirrored from www.museumsandtheweb.com/mw2012/papers/evaluating_the_practical_applications_of_eye_t by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 15:47:50 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Evaluating the Practical Applications of Eye Tracking in Museums | museumsandtheweb.com</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<script type="text/javascript">var _sf_startpt=(new Date()).getTime()</script>
<link rel="alternate" type="application/rss+xml" title="Comments for &quot;Evaluating the Practical Applications of Eye Tracking in Museums&quot;" href="../../crss/node/16871html.html" />
<link rel="shortcut icon" href="../../files/favicon0cc1.ico?a" type="image/x-icon" />
  <link type="text/css" rel="stylesheet" media="all" href="../../files/ctools/css/fc1c48c5ca33c8171b281d62bf027dd0e4da.css?5" />
<link type="text/css" rel="stylesheet" media="all" href="../../files/css/css_f7e2be0e9735a90610f82c98117dc429.css" />
<link type="text/css" rel="stylesheet" media="print" href="../../files/css/css_be733149ff93a70329b24a75cac048ce.css" />
  <style type="text/css">#container{width:100%;}.two-sidebars .content-inner{margin-left:240px; margin-right:300px;}.sidebar-first .content-inner{margin-left:240px; margin-right:0;}.sidebar-last .content-inner{margin-right:300px; margin-left:0;}#sidebar-first{width:240px;margin-left:-100%;}#sidebar-last{width:300px;margin-left:-300px;}</style>  <script type="text/javascript" src="../../files/js/js_c85e31df0ca6eeac4ecb6a1f43c7ac18.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","block_edit":{"hover_links":1},"chartbeat":{"uid":3385,"domain":"museumsandtheweb.com"},"fivestar":{"titleUser":"Your rating: ","titleAverage":"Average: ","feedbackSavingVote":"Saving your vote...","feedbackVoteSaved":"Your vote has been saved.","feedbackDeletingVote":"Deleting your vote...","feedbackVoteDeleted":"Your vote has been deleted."},"googleanalytics":{"trackOutgoing":1,"trackMailto":1,"trackDownload":1,"trackDownloadExtensions":"7z|aac|arc|arj|asf|asx|avi|bin|csv|doc|exe|flv|gif|gz|gzip|hqx|jar|jpe?g|js|mp(2|3|4|e?g)|mov(ie)?|msi|msp|pdf|phps|png|ppt|qtm?|ra(m|r)?|sea|sit|tar|tgz|torrent|txt|wav|wma|wmv|wpd|xls|xml|z|zip"}});
//--><!]]>
</script>
</head>
<body class="not-front not-logged-in article-type-paper one-sidebar sidebar-first section-mw2012 page-node-16871">
  <div id="container">

    <div id="skip-nav" class="show-on-focus">
      <!-- To adjust the display of the skip link see the Advanced theme settings (General settings), and never use display:none! -->
      <a href="#main-content">Skip to main content</a>
    </div>

        
    
          <div id="leaderboard">  <div id="block-panels_mini-leader" class="block block-panels_mini odd block-region-leaderboard block-count-1">
  <div class="block-inner">

    
    <div class="content"><div class="panel-display panel-3col clear-block" id="mini-panel-leader">
  <div class="panel-panel panel-col-first">
    <div class="inside"></div>
  </div>

  <div class="panel-panel panel-col">
    <div class="inside"><div class="panel-pane pane-custom pane-1 " >
  
      <h2 class="pane-title"><a href="../../index.html">Museums and the Web </a></h2>
  
  
  <div class="pane-content">
    <font color="white">An annual conference exploring the social, cultural, design, technological, economic, and organizational issues of culture, science and heritage on-line.</font>  </div>

  
  </div> <!-- /panels-pane -->
</div>
  </div>

  <div class="panel-panel panel-col-last">
    <div class="inside"><div class="panel-pane pane-custom pane-2 " >
  
      <h2 class="pane-title"><a href="http://mw2013.museumsandtheweb.com/">Museums and the web 2013</a></h2>
  
  
  <div class="pane-content">
    <ul>
<li><a href="../../mw2012.html">2012</a></li>
<li><a href="../../mw2011.html">2011</a></li>
<li><a href="../../mw2010/index.html">2010</a></li>
<li><a href="../../mw2009/index.html">2009</a></li>
<li><a href="../../mw2008/index.html">2008</a></li>

<li><a href="../../mw/museums_and_the_web.html">...</a></li>
</ul>
  </div>

  
  </div> <!-- /panels-pane -->
</div>
  </div>
</div>
</div>

    
  </div>
</div> <!-- /block -->
</div> <!-- /leaderboard -->
    
    <div id="header" class="clearfix">

              <div id="branding">

                                    <div class="logo-site-name"><strong>
                <span id="logo"><a href="../../mw2012.html" title="Museums and the Web 2012: the international conference for culture and heritage on-line"><img src="../../sites/all/themes/at_mw2012/logo.png" alt="museumsandtheweb.com logo" title="Home page"/></a></span>                <span id="site-name"><a href="../../mw2012.html" title="Museums and the Web 2012: the international conference for culture and heritage on-line"><img src="../../sites/all/themes/at_mw2012/css/theme/images/mw2012-title.png" alt="Museums and the Web 2012: the international conference for culture and heritage on-line" /></a></span>              </strong></div> <!-- /logo/site name -->
                        <div id="site-slogan"><div class="when-where">April 11-14, 2012, San Diego, CA, USA</div><div class="by">produced by Archives &amp; Museum Informatics</div><div class="link">http://conference.archimuse.com/mw2012</div></div>                    
        </div> <!-- /branding -->
            
        <div id="social">
                    <div class="custom-user-menu"><ul class="item-list clear-block">
<li class="first"><a href="../../user/register.html">New Account</a></li>
<li class="last"><a href="../../user.html">Login</a></li>
</ul></div>          <div class="social-media clear-block"><ul>
<li><a class="linkedin" href="http://www.linkedin.com/shareArticle?url=http://museumsandtheweb.com/mw2012/papers/evaluating_the_practical_applications_of_eye_t&amp;source=Archimuse&amp;mini=true" rel="nofollow" target="_blank" title="Share on LinkedIn">LinkedIn</a></li>
<li><a class="facebook" href="http://www.facebook.com/share.php?u=http://museumsandtheweb.com/mw2012/papers/evaluating_the_practical_applications_of_eye_t" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a></li>
<li><a class="twitter" href="http://twitter.com/?status=http://museumsandtheweb.com/mw2012/papers/evaluating_the_practical_applications_of_eye_t" rel="nofollow" target="_blank" title="Share on Twitter">Twitter</a></li>
<li><a class="flickr" href="http://www.flickr.com/groups/mw2011-15th/" rel="nofollow" target="_blank" title="Join the Flickr group">Flickr</a></li>
</ul></div>
        </div> <!-- /social -->
        
              <div id="register"><a href="../register.html" target="_blank">Register</a></div> <!-- /register -->
            
      
    </div> <!-- /header -->

        
      <div id="primary" class="nav">
      </div> <!-- /primary link menu -->


    


    
    <div id="columns"><div class="columns-inner clearfix">

      <div id="content-column"><div class="content-inner">
        
                
        
        <div id="main-content">
          
                      <div id="breadcrumb">
              <h2 class="element-invisible">You are here:</h2>
              <a href="../../mw2012.html">MW2012</a> &#187; <a href="../sessions.html">Sessions</a> &#187; <a href="../programs/evaluating_the_practical_applications_of_eye.html">Evaluating the Practical Applications of Eye Tracking in Museums</a> &#187;             </div> <!-- /breadcrumb -->
              
                      <div id="main-content-header" class="clear-block">
              <h1 id="page-title">Evaluating the Practical Applications of Eye Tracking in Museums</h1>                          </div>
          
          
          <div id="content"><div id="article-16871" class="article paper-article clear-block">

    
    <div class="article-detail clear-block">
  <div class="node-edit-link" id="node-edit-link-16871"><ul class="links"><li class="0 first last active"><a href="evaluating_the_practical_applications_of_eye_t.html" class="active">[View]</a></li>
</ul>
</div><h3 class="Author">Edward Bachta, Robert J. Stein, Silvia Filippini-Fantoni, Tiffany Leason, Indianapolis Museum of Art, USA</h3>
<h2 class="AbstractTitle">Abstract</h2>
<p class="AbstractText">Funded by an Institute for Museum and Library Services Sparks! Ignition grant, the Indianapolis Museum of Art is exploring whether or not eye-tracking technology can be useful to museums seeking to better understand how in-gallery visitors actually “see” the objects in our collection. Through a set of three experiments, the project seeks to understand whether eye tracking can be used to measure visitor attention to artworks, understand the correlation between guided interpretation and visitor comprehension, and to trigger interpretive content delivery. In this paper, authors will review the relevant literature in the field that connects gaze detection and cognition; explain in detail the experimental methodology used in the first experiment to determine the practicality of adopting these techniques in museums; and report on initial conditions and factors discovered during the project’s initial research.</p>
<p class="keywords">Keywords: eye tracking, gaze tracking, visitor attention, user interface, research</p>
<h2>1.&nbsp;&nbsp;&nbsp;&nbsp; Introduction and background</h2>
<p>Every year, museums welcome millions of visitors to galleries and exhibitions hoping that many of them will find meaningful experiences there that will help them understand the world in new ways.&nbsp; Museum staff spends enormous amounts of time and effort studying the ways visitors experience works of art in their collections, hoping to increase the visitor’s engagement with them.&nbsp; Research conducted at the Indianapolis Museum of Art (IMA) and at other museums based on Falk’s model (2009) of identity-related motivations for visiting shows that visitors to art museums see them as places for inspiration and contemplation, among other things. However, it’s difficult to get to a more concrete understanding of what aspects of a patron’s visit are inspiring and how museums can actively promote and encourage those experiences without digging deeper.</p>
<p>While some visitors clearly have deep and engaging experiences, research shows that the average visitor spends only seconds in front of a work of art. In <em>Learning in the Museum</em>, Hein (1998) states:</p>
<p class="BlockQuoteAuthor">Empirical data supports the view that visitors spend little time at individual exhibit components (often a matter of a few seconds and seldom as much as one minute); seldom read labels; usually stop at less than half the components at an exhibit; are more likely to use trial-and-error methods at interactive exhibits than to read instructions; that children are more likely to engage with interactive exhibits than adults, and that attention to exhibits declines sharply after about half an hour.</p>
<p>In fact, studies of 150 visitors at the Metropolitan Museum of Art (Smith, 2001) found a mean time of less than 30 seconds to be typical, with most spending significantly less time. Worts (2003) summarizes this behavior as “grazing” and theorizes that the pattern may arise from a mismatch in the goals of curators and visitors:</p>
<p class="BlockQuoteAuthor">Audience research across the field commonly reveals the characteristic behavior of ‘grazing’ – or wandering slowly past many artworks, spending only seconds looking at any work in particular. It is relatively rare to watch a visitor spend more than a minute with any individual artwork.</p>
<p>These texts have also motivated IMA’s own examination of viewing patterns in the permanent collection galleries in a multi-year effort called the Viewing Project (<a href="http://www.imamuseum.org/art/research/viewing-project" title="http://www.imamuseum.org/art/research/viewing-project">http://www.imamuseum.org/art/research/viewing-project</a>), which seeks to encourage active looking to support visitor creativity and engagement, and to present objects from the permanent collection in new ways.</p>
<p>Evaluations from the project’s installations studied in-gallery viewing behaviors and found that “time spent looking” at an artwork resulted in median times between 4 and 31 seconds covering ten different installations. While improvements in engagement have been realized for some of the Viewing Project installations, a quantitative link between looking and engagement remains elusive, and measuring that “time spent looking” is a detailed and time-intensive human process.</p>
<p>Research by Housen (1999) and Yenawine (1997) asserts that deep looking, and the critical thinking that results from facilitated conversation about works of art, results in richer connections and increased engagement with those works. This deep looking is therefore an important skill to encourage in our visitors—but looking at what and in what context?&nbsp; Can understanding what a visitor actually sees when he or she looks at a museum object help us to provide more engaging experiences for that visitor, inform the creation of gallery interpretation, and have implications for the design of those in-gallery experiences?</p>
<h3>Potential for eye tracking</h3>
<p>Techniques for measuring gaze have been an important part of cognitive psychology and many other fields of study since the early 1960s. Environmental scans by Rayner (1978, 1998) summarize the scope and evolution of research linking eye tracking and cognition. Agreement in the research suggests that gaze and attention are tightly coupled (Hoffman, 1998), implying a direct relationship between how we look at museum objects and our thinking about them. Automated scientific equipment for eye tracking became more widely available in the 1970s but involved complex and expensive hardware and often constrained the user’s head movement.&nbsp; More advanced eye-tracking systems developed later were head-mounted and worn like goggles or glasses. These systems allowed users to move their heads freely and supported a more mobile study of eye tracking. While these systems were an important improvement over immobilizing the user’s head, they still required visitors to wear detailed calibration and cumbersome equipment to.</p>
<p>Research by Wooding (2002) examines the use of eye-tracking systems and art from the collection of the National Gallery in London. While the data seems promising, Wooding’s work focused more on a generalized method for visualizing eye-tracking data and not on specific applications of these techniques for art history or museology. Milekic&nbsp;(2010) published an overview of gaze tracking and its potential applications for museums, highlighting the advent of cheap and commercially available equipment to support the use of these tools in a museum setting.</p>
<p>While still somewhat expensive, eye-tracking technology offers the potential for museums to directly study what our visitors are looking at when they spend time with a work of art. Future eye-tracking technology will likely include software-only systems that will run on common laptops and desktop computers.&nbsp; Several academic software tools already exist that attempt to track gaze in this way. These systems are still largely experimental at this stage and lack the accuracy and ease of use for routine deployment in galleries.</p>
<p>Museums now have the opportunity to explore and model a number of ways in which eye-tracking techniques can be used to improve visitor experience, allowing them to exploit those advances as hardware costs continue to fall and software-based systems become more common. Eye tracking has the potential to transform the ways we understand visual processing in the arts and at the same time offers a direct way of studying several important factors of a museum visit.</p>
<h3>Project design</h3>
<p>Seeking to explore useful and practical means of applying eye-tracking technology to common problems faced by museums, the IMA proposed a series of three experiments to be conducted as part of a research project. The project’s work is funded in part by a 2010 Sparks! Ignition grant from the Institute for Museum and Library Services.</p>
<p>Staff from the museum’s technology, audience engagement, and media departments collaborated on the design, creation, execution, and evaluation of each experiment. This paper will describe the instrumentation to be used for these experiments and the methodology, analysis, and summary of the first experiment.</p>
<p>Museums have many different ways to measure attendance in our galleries. From hand clickers to beam counters and even thermal cameras, museums are quite sophisticated in how we count feet through the door; however, museums have made little progress towards understanding just what those visitors do once they enter that door. As we learned earlier, museums are already studying the amount of time visitors spend with works in their collections, but these studies require a set of observational rubrics that are labor intensive and prone to human error. The ability to measure the attention of visitors automatically in front of a museum object would be a transformational metric for gallery design, collection management, and interpretive development in museums. As a step in this direction, the goal of the first experiment is to assess whether an eye tracker can be used to accurately measure the amount of time a visitor spends looking at a painting.</p>
<h2>2.&nbsp;&nbsp;&nbsp;&nbsp; Instrumentation</h2>
<p>In determining which hardware technology to use for our research, head-mounted trackers were ruled out because one of the project goals is to determine whether an eye-tracking system can be used to detect the gaze of a visitor without encumbrance or requiring calibration. Of the models available, the EyeTech Digital Systems VT2 eye tracker was chosen for the experiments (<a href="http://www.eyetechds.com/"><em>http://www.eyetechds.com/</em></a>). This device is designed to sit on the included stand or be mounted on the bottom of a display. During operation, the eye tracker emits infrared light toward the viewer and captures a series of images (called frames) with a camera. Each frame is analyzed by the tracker to determine whether a pair of eyes is looking toward the device. When the tracker is calibrated with a reference rectangle, it is able to compute gaze coordinates relative to that area. The software library supplied can report these and other parameters during operation.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig1-rev.jpg" border="0" width="500" height="373" /></p>
<p style="text-align: center;"><strong>Figure 1: The EyeTech Digital Systems VT2</strong></p>
<p>The device was tested prior to planning experiments to determine the design constraints for setup in the gallery. Due to the attenuation of intensity from the infra-red emitters and the characteristics of the lens, the tracker has an ideal viewing range that was found to be approximately 25 inches from the front panel. It also is unable to detect the eyes if they are looking too far past the left or right edges of the device (the field of view is about 36 degrees at this distance), or too high above the device.</p>
<p>Unfortunately, it is not reasonable to attempt to detect the gaze at standing height with these limitations, as variations in height would likely place the eyes in a position that cannot be tracked. As a result, it was decided to have participants sit during the experiment to reduce variation in the position of the eyes. It is also important to consider that the area to be viewed must lie within the field of view that can be tracked once the tracker has been set up.</p>
<h2>3.&nbsp;&nbsp;&nbsp;&nbsp; Experiment One</h2>
<p>After considering the limitations of the device, the objective of this experiment was refined to assessing how accurately the amount of time a seated person’s eyes spend looking at a work of art can be derived by using an eye tracker. In a live deployment, it would be ideal to avoid disrupting a visitor’s normal patterns of viewing the art by requiring calibration, so a key component of the evaluation is to test whether the device performs well when not calibrated for each participant.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig2.png" border="0" width="500" height="420" /></p>
<p style="text-align: center;"><strong>Figure 2: Edward Hopper, American (1882–1967), Hotel Lobby, 1943, oil on canvas, 32 1/4 x 40 3/4 in. (image), </strong></p>
<p style="text-align: center;"><strong>40 1/2 x 48 1/2 in. (framed), William Ray Adams Memorial Collection, 47.4 © Edward Hopper.</strong></p>
<p>Edward Hopper’s <em>Hotel Lobby</em> was chosen for this experiment based on the parameters determined during preliminary testing. The eye tracker was set up in the gallery and calibrated once to register the position of the painting. The person for whom this calibration was performed was also a participant in the last session of the experiment.</p>
<p>The experiment consisted of two phases. In phase one, the seat was placed at the distance determined to be ideal based on the preliminary tests and was not moved from that position. In phase two, before beginning the session the location of the seat was adjusted for each participant to test whether results could be improved without tilting the camera. Twenty-two IMA staff participated in the experiment, representing a range of heights, and some wore glasses. Ten people participated in phase one of the experiment, and twelve people participated in phase two.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig3.png" border="0" width="500" height="454" /></p>
<p>&nbsp;</p>
<p style="text-align: center;"><strong>Figure 3: A session underway in the gallery</strong></p>
<p>Subjects’ standing and seated heights were measured, as well as the distance from the floor to mid-eye level when seated. Over a period of sixty seconds, participants were asked to look in the field of the painting including the frame (referred to here as “in bounds”), then outside, and then directed back inside. Simultaneously, two research assistants observed the participants and used stopwatches to manually track time of gaze in bounds. These recorded times were then averaged in order to compare to the time tracked by the device to gauge accuracy.&nbsp; Research assistants also noted any movement by the participants.<br />In phase two, the procedure was as detailed above, but if the device did not pick up participants’ gazes when first getting into position, participants were asked to move the seat until they could see their eyes reflected back in the device and the system reported that it could detect the eyes fairly consistently. The distance from the device to the new position of the seat was recorded, and the data recording session for the participant began. A Python script was written using the C API provided by EyeTech to record various data reported by the tracker for future analysis. The script produced auditory feedback (which was output only to the computer operator via headphones) and a simple summary at the end of each session so that the process could be monitored.</p>
<p>Some participants from phase two were also asked to take part in a secondary experiment in which the ability of the device to accurately track gaze location when not calibrated for each viewer was assessed. Participants were instructed to look at six different areas within the painting (i.e., the blond woman’s hair, the man behind the desk, the old man’s face, the old woman’s shoes, the old woman’s hat, and the painting on the wall) for 10 seconds each. Tracker data was logged in the same manner as the other sessions.</p>
<h2>4.&nbsp;&nbsp;&nbsp;&nbsp; Analysis of results</h2>
<h3>Gaze-duration measurements</h3>
<p>The logs generated during the experiment were first processed using Python scripts to calculate aggregate figures. In this aggregation, it was assumed that the eyes were looking at the location reported for frame n for the duration between frame n-1 and frame n, rather than interpolating. The calculated figures were then compared with the manually recorded data as outlined below.</p>
<h4>Quantity of valid gaze data</h4>
<p>The first measurement considered was the amount of valid gaze coordinate data that the tracker was able to collect during the session. If the tracker was unable to detect eyes in the frame, it was flagged as being an invalid frame in the reported metadata. To calculate the total period during which valid frames were captured, the sum of the intervals between each frame with valid gaze data and their previous frame was computed (i.e., each frame was taken to represent the interval of time since the previous frame).</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig4.png" border="0" width="500" height="300" /></p>
<p style="text-align: center;"><strong>Figure 4: Valid gaze data analysis results</strong></p>
<p>The figure above plots the difference between the total valid gaze period described above and the time spent looking in bounds as manually recorded. The results are normalized as a percentage of the session time. Positive results indicate that when instructed to look outside the painting, the participant may have still looked close enough to the painting that their eyes were detected by the tracker. Negative results indicate that invalid frames were reported during a greater amount of time than the period in which the participant was asked to look away from the painting. For reference, the average manually observed time spent looking out of bounds was 23 percent.</p>
<p>Overall, invalid frames surpassed the amount of time spent looking away by 20 percent of the session time for six of the participants. While this was the case during phase one (participants 1-10) for four people (in fact, no valid frames were reported during two of these sessions), an improvement can be seen in phase two (participants 11-22), where the seat position was adjusted.</p>
<h4>Comparing the quantity of valid gaze data to seated eye level</h4>
<p>The duration represented by invalid frames surpassed the amount of time spent looking away by over 40 percent of the session time for four of the six participants whose eye level was below 50 inches from the ground. The tracker was not able to register any valid data for two of these participants in phase one, and two participants in phase two had difficulty getting into a good position. There did not appear to be a correlation to the quantity of valid data for seated eye levels between 50 and 53.75 inches (the highest seated eye level in the study).</p>
<h4>Comparison of tracker data and manually recorded data</h4>
<p>The amount of time that the participants spent looking in bounds was calculated based on the raw data as described above for measuring the amount of valid data, but only including frames where the tracker data indicated that the gaze was within the outside edge of the frame. This value differed from the average manually observed time by less than 5 percent of the session time for five of the twenty-two participants. For nine participants, the tracker data differed from observation by over 20 percent of the session time.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig5.png" border="0" width="500" height="300" /></p>
<p style="text-align: center;"><strong>Figure 5: Results from the raw tracker data</strong></p>
<p>In an attempt to improve this performance, a gap-handling algorithm was implemented that included the period between a valid frame and the next valid frame if the gaze was directed in bounds in both frames, and if the frames occurred within a given period of time. This algorithm was applied to the recorded data with thresholds of 100 milliseconds, 500 milliseconds, and 1 second.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig6.png" border="0" width="500" height="300" /></p>
<p style="text-align: center;"><strong>Figure 6: Results from the gap-handling algorithm</strong></p>
<p>The algorithm appears to improve the accuracy if a threshold between 100 and 500 milliseconds is used. With the improvement seen when applying the 500-millisecond threshold, the difference from the manually recorded time comes within 5 percent of the session time for nine of the participants, and within 10 percent for half of the participants.</p>
<p>When participants were asked to adjust the position of the seat in phase two, the tracker produced results that more closely matched observation. An accuracy better than 5 percent was reported for five phase two participants, versus none for phase one. Improvement can be seen with the gap-handling algorithm applied as well.</p>
<p>It can also be seen that the last participant, who calibrated the system, was among those for whom the system was very accurate. This provides confidence that the tracker was not nudged out of alignment during the experiment.</p>
<h4>Considering glasses</h4>
<p>Ten of the twenty-two participants wore glasses. Of the seven for whom the raw tracker accuracy was within 10 percent, only one used glasses, and this was the person for whom the device was calibrated. Of the eleven for whom the accuracy was within 10% after applying the 500-millisecond threshold, only three wore glasses.</p>
<h3>Gaze location measurements</h3>
<h4>Results for the calibrated viewer</h4>
<p>To analyze the data from the secondary experiment, the gaze points from each session were plotted using a scatterplot technique. Shown below is the scatterplot for the participant with whom the system was calibrated:</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig7.png" border="0" width="500" height="385" /></p>
<p style="text-align: center;"><strong>Figure 7: Gaze points for the calibrated viewer</strong></p>
<p>When the gaze data is valid, the location of the gaze is reported as an x-y pair, normalized such that (0,0) corresponds to the upper left of the calibration boundary and (100,100) corresponds to the lower right. For these experiments, the system was calibrated such that the boundary corresponds to the outside edge of the frame of the painting. The image below shows the scatterplot superimposed on a photograph of the painting, such that the gaze locations appear brighter:</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig8.png" border="0" width="500" height="419" /></p>
<p style="text-align: center;"><strong>Figure 8: Composited gaze points with calibration</strong></p>
<p>As can be seen, the reported locations match with the actual locations that the participant was asked to look at: the blond woman’s hair, the man behind the desk, the old man’s face, the old woman’s shoes, the old woman’s hat, and the painting on the wall. The distribution of points in each cluster is roughly 5 percent of the width and height of the painting, which at the distance used in the experiment (70 inches from painting to tracker, and about 25 inches from tracker to eye), corresponds to 1.5 degrees of the field of view. As described by Rolfs (2009), the eye tends to microsaccade once or twice per second when fixating on a point, and it is common to use a threshold of 1 to 2 degrees to distinguish between these involuntary eye movements and voluntary saccades. Considering this, the results for the calibrated viewer are about as good as can be expected.</p>
<h4>Results for uncalibrated viewers</h4>
<p>Two approaches were used to compare the results for the calibrated participant with the others. The first was a visual comparison using the same scatterplot technique. Shown below is a composite of a typical uncalibrated participant’s results with the photograph of the painting.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig9.png" border="0" width="500" height="419" /></p>
<p style="text-align: center;"><strong>Figure 9: Composited gaze points without calibration</strong></p>
<p>The result shows that the horizontal component seems to be fairly accurate, but that the system has perceived the participant to be looking significantly lower than he or she actually was. There were no cases where the gaze was perceived to be higher. There was also some variation in the distribution of the clusters, but because this may be due to the viewer choosing to look fixedly at one point or at the general area described by the instruction, it is not possible to evaluate the precision.</p>
<p>To quantify the accuracy, a second approach was taken. Using the R statistical analysis package, the gaze points from each session were clustered with the k-means algorithm into seven groups (an extra group was required to contain invalid points reported at 0,0). Plots of the output validate that the algorithm identified the appropriate clusters.</p>
<p style="text-align: center;"><img src="../../files/ebachta-fig10.png" border="0" width="500" height="278" /></p>
<p style="text-align: center;"><strong>Figure 10: K-means clustering</strong></p>
<p>The centroids of the clusters for the uncalibrated participants were then compared with those for the calibrated viewer. Reported across all uncalibrated viewers as a percentage of the respective dimension of the painting, the horizontal component differed by 3.9 percent on average, and the vertical component differed by 24.4 percent on average. There was also variance between people with a similar seated eye level. Interestingly, the range of the vertical component of the cluster centroids also differed significantly between viewers. The range of the calibrated user was [37.9,63.8], or 25 percent, which corresponds to the proportion of the painting in which the gaze targets lie. The widest range among the other participants was 39.2 percent, while the narrowest was 15.2 percent. This means that it would not be possible to correct for the error in vertical position with only a translation operation (i.e., adding a constant value to the coordinate).</p>
<h2>5.&nbsp;&nbsp;&nbsp;&nbsp; Summary of results</h2>
<p>From the preliminary tests alone, it is apparent that the device used in the experiment is not capable of tracking the gaze for casual visitors walking through the gallery. The constraints on viewing distance and angle make it unlikely that the data recorded would correlate well with the attention paid to an object by the average visitor.</p>
<p>Analyzing the results of the experiment tells us more about the potential for learning from visitors who may casually sit in front of a tracking system while viewing an object (i.e., without requiring calibration). The results indicate that in general, the tracker was unable to continuously capture gaze location data for the duration of each session. Allowing the viewer to adjust the position of the seat improves the accuracy of computed gaze duration within a boundary, and for some participants the results were quite good. However, it does not appear possible to measure this reliably for all viewers. When having participants sit, there is still enough variation in height that the eyes may not be in a good position for tracking, and it is likely that the frames of glasses interfere with the ability of the tracker to detect the eyes.</p>
<p>From these results, it would seem that perhaps by giving the viewer more instructions to pay attention to his or her position while adjusting the height of the seat, it would be possible to improve the performance of the system. However, the gaze location results show that without being calibrated, the eye tracker would not be able to determine whether the gaze has crossed the upper or lower boundary of the physical target area. Because the horizontal measurement does appear to be accurate, it may be interesting to consider whether this is an issue in the gallery setting, as it is not common to place paintings above and below each other.</p>
<p>In conclusion, we find that it is not possible to compute gaze duration accurately within a specific zone without calibrating the tracker for each viewer, and it may not be possible even if calibrated unless an algorithm is applied to handle gaps in the data. Analyzing the results of the secondary experiment has shown that the tracker was able to report gaze location accurately in a gallery setting for the viewer who performed calibration. For uncalibrated viewers, the system was only accurate in tracking the horizontal component of gaze location.</p>
<h2>6.&nbsp;&nbsp;&nbsp;&nbsp; Future work</h2>
<p>To prepare for the second experiment proposed in the grant, a phase will be included in which each participant will first calibrate the system and then look at a set of points. The procedure will be similar to the gaze-location experiment described earlier and more precisely identified points will be included. The results of this phase will be analyzed both to compare with the results of the first experiment and to verify that the gaze location data will be accurate for each participant in the main phase of the experiment. If the reported gaze coordinates are still found to be unreliable after calibrating for each person, the plan for experiment two will be modified.</p>
<p>In the main phase of the second experiment, the eye-tracking equipment will be used to monitor a user’s gaze during a typical Visual Thinking Strategies (VTS) session facilitated by an IMA staff member. Using the VTS method, educators regularly engage groups in interactive discussions seeking to draw out visitor thoughts regarding a work of art.&nbsp; While such discussions often provide unique insight into an individual’s thoughts about a work of art, direct measurements of the connection between viewing and thinking are often difficult and subjective.</p>
<p>A video recording of the session will be made and synchronized with the data stream from the eye-tracking hardware, allowing museum staff to examine the connection between gaze and response.&nbsp; The experiment may reveal a valuable new technique to study the ways in which visitors with varying levels of art experience approach artworks in museum collections.</p>
<h2>7.&nbsp;&nbsp;&nbsp;&nbsp; Acknowledgements</h2>
<p>We would like to thank the representatives from EyeTech who informed us about the capabilities of the tracker and spent a day familiarizing our team with the system. We would also like to acknowledge the researchers and engineers who have brought eye-tracking technology to the point where cultural institutions can begin to consider its use in evaluation.</p>
<h2>8.&nbsp;&nbsp;&nbsp;&nbsp; References</h2>
<p class="ReferencesText">Falk, J.H. (2009). <em>Identity and the Museum Visitor Experience</em>. Walnut Creek, California, USA: Left Coast Press.</p>
<p class="ReferencesText">Hein, G.E. (1998). <em>Learning in the Museum</em>. London: Routledge, 138.</p>
<p class="ReferencesText">Hoffman, J.E. (1998). “Visual attention and eye movements”. In H. Pashler (ed.), <em>Attention</em>. Hove, UK: Psychology Press, 119–154.</p>
<p class="ReferencesText">Housen, A. (1999). “Eye of the Beholder: Research, Theory, and Practice.” Presented at the conference of <em>Aesthetic and Art Education: a Transdisciplinary Approach</em>. September 27–29, 1999, Lisbon, Portugal. Consulted January 20, 2012. Available at: <a href="http://www.vtshome.org/system/resources/0000/0006/Eye_of_the_Beholder.pdf"><em>http://www.vtshome.org/system/resources/0000/0006/Eye_of_the_Beholder.pdf</em></a></p>
<p class="ReferencesText">Milekic, S. (2010). “Gaze-Tracking and Museums: Current Research and Implications.” In J. Trant &amp; D. Bearman (eds.), <em>Museums and the Web 2010: Proceedings</em>. Toronto: Archives &amp; Museum Informatics. Consulted September 28, 2011. Available at: <a href="http://www.archimuse.com/mw2010/papers/milekic/milekic.html"><em>http://www.archimuse.com/mw2010/papers/milekic/milekic.html</em></a></p>
<p class="ReferencesText">Rayner, K. (1978). “Eye movements in reading and information processing.” <em>Psychological Bulletin. 85(3). </em></p>
<p class="ReferencesText">Rayner, K. (1998). “Eye movements in reading and information processing: 20 years of research.” <em>Psychological Bulletin</em>.<em> 124(3).</em></p>
<p class="ReferencesText">Rolfs, M. (2009). “Microsaccades: Small steps on a long way”. <em>Vision Research, 49.</em> Elsevier Ltd., 2415–2441. Consulted January 29, 2012. Available at: <a href="http://www.martinrolfs.de/Rolfs_MicrosaccadeReview.pdf"><em>http://www.martinrolfs.de/Rolfs_MicrosaccadeReview.pdf</em></a></p>
<p class="ReferencesText">Smith, J.K., &amp; L. F. <a href="file:///C:/Users/rcherry/Dropbox/3.3/3.2_bachta_mw2012_final/3.2_bachta_mw2012_final.docx#_msocom_1">[hc1]</a>&nbsp;(2001<a href="file:///C:/Users/rcherry/Dropbox/3.3/3.2_bachta_mw2012_final/3.2_bachta_mw2012_final.docx#_msocom_2">[EB2]</a>&nbsp;). “Spending Time on Art”. In <em>Empirical Studies of the Arts</em>, 19(2).</p>
<p class="ReferencesText">Wooding, D. (2002). “Fixation maps: quantifying eye-movement traces.” In <em>Proceedings of the 2002 symposium on Eye tracking research &amp; applications (ETRA '02).</em> New York, NY, USA: ACM, 31–36.</p>
<p class="ReferencesText">Worts, D. (2003). “On the Brink of Irrelevance? Art Museums in Contemporary Society.” In L. Tickle, V. Sekules, &amp; M. Xanthoudaki (eds.),<em> Researching Visual Arts Education in Museums and Galleries: An International Reader</em>. Dordrecht, Netherlands: Kluwer Academic Publishers.</p>
<p class="ReferencesText">Yenawine, P. (1997). “Thoughts on Visual Literacy.” In J. Flood, S.B. Heath, &amp; D. Lapp (eds.), <em>Handbook of Research and Teaching Literacy through the Communicative and Visual Arts</em>. Consulted January 20, 2012. Available at: <em><a href="http://www.vtshome.org/system/resources/0000/0005/Thoughts_Visual_Literacy.pdf">http://www.vtshome.org/system/resources/0000/0005/Thoughts_Visual_Literacy.pdf</a></em></p>
<div class="field nodereference field-paper-program-ref">
                                <h3 class="label inline">Program Item Reference:&nbsp;</h3>
          <div class="item inline odd"><a href="../programs/evaluating_the_practical_applications_of_eye.html">Evaluating the Practical Applications of Eye Tracking in Museums</a></div>
              </div>
 <!-- /content-field -->
  </div>
  
  <div class="article-meta">
      <p class="submitted">Posted <em class="time">March 29, 2012 - 12:49am</em> by <em class="author"><a href="../../user/ebachta.html" class="username" title="View Ed&#039;s profile.">Ed Bachta</a></em></p>
  
    
    </div>
  
  
</div> <!-- /article --></div>

        </div> <!-- /main-content -->

        
      </div></div> <!-- /content-column -->

              <div id="sidebar-first" class="sidebar">  <div id="block-menu-menu-mw2012" class="block block-menu odd block-region-sidebar-first block-count-2">
  <div class="block-inner">

          <h2 class="block-title">MW2012 Main Menu</h2>
    
    <div class="content"><ul class="menu">
 <li class="leaf first about-mw2012"><a href="../about.html" title="Museums and the Web 2012 (MW2012)">About MW2012</a></li>
<li class="leaf attendees"><a href="../attendees.html" title="Attendees">Attendees</a></li>
<li class="leaf workshops"><a href="../workshops.html" title="Museums and the Web 2012 (MW2012): Workshops">Workshops</a></li>
<li class="leaf sessions"><a href="../sessions.html" title="Museums and the Web 2012 (MW2012): Sessions">Sessions</a></li>
<li class="leaf speakers-amp-authors"><a href="../speakers.html" title="Museums and the Web 20121 (MW2012): Speakers &amp; Authors">Speakers &amp; Authors</a></li>
<li class="leaf demonstrations"><a href="../demonstrations.html" title="Demonstrations">Demonstrations</a></li>
<li class="leaf exhibits"><a href="../sponsor_exhibits.html" title="Museums and the Web 2012 (MW2012): Exhibits">Exhibits</a></li>
<li class="leaf sponsors"><a href="../sponsors.html" title="Museums and the Web 2012 (MW2012): Sponsors">Sponsors</a></li>
<li class="collapsed best-of-the-web"><a href="../best/nominees.html" title="Best of the Web Nominations 2012">Best of the Web</a></li>
<li class="leaf program-committee"><a href="../program-committee.html" title="">Program Committee</a></li>
<li class="leaf epub-proceedings"><a href="../epub.html" title="MW2012 Proceedings Digital Download">ePub Proceedings</a></li>
<li class="leaf mw-online"><a href="../mw2012_online_the_permanent_backchannel.html" title="Museums and the Web 2012 (MW2012): MW Online – The Permanent Backchannel">MW Online</a></li>
<li class="leaf registration"><a href="../registration.html" title="Museums and the Web 2012 (MW2012): Register">Registration</a></li>
<li class="leaf exhibitor-logistics"><a href="../mw2012_exhibitor_logistics.html" title="Museums and the Web 2012 (MW2012): Exhibitor Logistics">Exhibitor Logistics</a></li>
<li class="leaf scholarships"><a href="../scholarships.html" title="Museums and the Web 2012 (MW2012): Scholarships and Volunteers">Scholarships</a></li>
<li class="collapsed local-information"><a href="../local-information.html" title="Museums and the Web 2012 (MW2012): Local Information">Local Information</a></li>
<li class="leaf events"><a href="../events.html" title="Museums and the Web 2012 (MW2012): Events">Events</a></li>
<li class="leaf key-dates"><a href="../dates.html" title="Museums and the Web 2012 (MW2012): Key Dates">Key Dates</a></li>
<li class="leaf paper-guidelines"><a href="../paper-guidelines.html" title="Paper Guidelines">Paper Guidelines</a></li>
<li class="leaf presentation-guidelines"><a href="../presentation-guidelines.html" title="Presentation Guidelines">Presentation Guidelines</a></li>
<li class="leaf suggestions-"><a href="../../forum/suggestions_for_the_mw2012_program.html" title="Suggestions for the MW2012 Program?">Suggestions?</a></li>
<li class="leaf last call-for-participation"><a href="../call.html" title="">Call for Participation</a></li>
 </ul>
</div>

    
  </div>
</div> <!-- /block -->
</div> <!-- /sidebar-first -->
      
      
    </div></div> <!-- /columns -->

    
          <div id="footer">

                  <div id="footer-region">  <div id="block-panels_mini-footer" class="block block-panels_mini odd block-region-footer block-count-3">
  <div class="block-inner">

    
    <div class="content"><div class="panel-flexible panels-flexible-4 clear-block" id="mini-panel-footer">
<div class="panel-flexible-inside panels-flexible-4-inside">
<div class="panels-flexible-region panels-flexible-region-4-left panels-flexible-region-first left">
  <div class="inside panels-flexible-region-inside panels-flexible-region-4-left-inside panels-flexible-region-inside-first">
  </div>
</div>
<div class="panels-flexible-region panels-flexible-region-4-center center">
  <div class="inside panels-flexible-region-inside panels-flexible-region-4-center-inside">
<div class="panel-pane pane-custom pane-3 " >
  
      <h2 class="pane-title">Founded by Archives &amp; Museum Informatics</h2>
  
  
  <div class="pane-content">
    <p><a href="http://www.archimuse.com/" title="www.archimuse.com">www.archimuse.com</a></p>
  </div>

  
  </div> <!-- /panels-pane -->
  </div>
</div>
<div class="panels-flexible-region panels-flexible-region-4-right panels-flexible-region-last right">
  <div class="inside panels-flexible-region-inside panels-flexible-region-4-right-inside panels-flexible-region-inside-last">
<div class="panel-pane pane-custom pane-4 " >
  
      <h2 class="pane-title">Managed by Museums and the Web LLC</h2>
  
  
  <div class="pane-content">
    <p>703 Dale Drive<br />
Silver Spring MD 20910 USA<br />
info @ museumsandtheweb.com<br />
<a href="../../terms.html">Terms and Conditions</a> </p>
  </div>

  
  </div> <!-- /panels-pane -->
  </div>
</div>
</div>
</div>
</div>

    
  </div>
</div> <!-- /block -->
</div> <!-- /footer-region -->
        
        
      </div> <!-- /footer -->
    
  </div> <!-- /container -->

  <script type="text/javascript">
  var _sf_async_config=Drupal.settings.chartbeat;
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
         (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
         "static.chartbeat.com/js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
       loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script><script type="text/javascript">
<!--//--><![CDATA[//><!--
var _gaq = _gaq || [];_gaq.push(["_setAccount", "UA-26332456-1"]);_gaq.push(['_setAllowLinker', true]);
_gaq.push(['_setAllowHash', false]);_gaq.push(["_trackPageview"]);(function() {var ga = document.createElement("script");ga.type = "text/javascript";ga.async = true;ga.src = ("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js";var s = document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ga, s);})();
//--><!]]>
</script>

</body>

<!-- Mirrored from www.museumsandtheweb.com/mw2012/papers/evaluating_the_practical_applications_of_eye_t by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 15:48:43 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
</html>
