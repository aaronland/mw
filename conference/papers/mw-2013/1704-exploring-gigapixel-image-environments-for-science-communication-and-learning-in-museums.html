<!DOCTYPE html>
<!--[if IE 6]>
<html id="ie6" lang="en-US">
<![endif]-->
<!--[if IE 7]>
<html id="ie7" lang="en-US">
<![endif]-->
<!--[if IE 8]>
<html id="ie8" lang="en-US">
<![endif]-->
<!--[if !(IE 6) | !(IE 7) | !(IE 8)  ]><!-->
<html lang="en-US">
<!--<![endif]-->

<!-- Mirrored from mw2013.museumsandtheweb.com/paper/exploring-gigapixel-image-environments-for-science-communication-and-learning-in-museums/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 23 Apr 2022 21:31:34 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />
<title>Exploring Gigapixel Image Environments for Science Communication and Learning in Museums | MW2013: Museums and the Web 2013</title>
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="stylesheet" type="text/css" media="all" href="../../wp-content/themes/twentyeleven/style.css" />
<link rel="pingback" href="../../xmlrpc.php" />
<!--[if lt IE 9]>
<script src="https://mw2013.museumsandtheweb.com/wp-content/themes/twentyeleven/js/html5.js" type="text/javascript"></script>
<![endif]-->
<link rel='dns-prefetch' href='http://s.w.org/' />
<link rel="alternate" type="application/rss+xml" title="MW2013: Museums and the Web 2013 &raquo; Feed" href="../../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="MW2013: Museums and the Web 2013 &raquo; Comments Feed" href="../../comments/feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="MW2013: Museums and the Web 2013 &raquo; Exploring Gigapixel Image Environments for Science Communication and Learning in Museums Comments Feed" href="feed/index.html" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/mw2013.museumsandtheweb.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.6"}};
			!function(e,a,t){var r,n,o,i,p=a.createElement("canvas"),s=p.getContext&&p.getContext("2d");function c(e,t){var a=String.fromCharCode;s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,e),0,0);var r=p.toDataURL();return s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,t),0,0),r===p.toDataURL()}function l(e){if(!s||!s.fillText)return!1;switch(s.textBaseline="top",s.font="600 32px Arial",e){case"flag":return!c([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])&&(!c([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!c([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]));case"emoji":return!c([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}function d(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(i=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},o=0;o<i.length;o++)t.supports[i[o]]=l(i[o]),t.supports.everything=t.supports.everything&&t.supports[i[o]],"flag"!==i[o]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[i[o]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(r=t.source||{}).concatemoji?d(r.concatemoji):r.wpemoji&&r.twemoji&&(d(r.twemoji),d(r.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='wp-block-library-css'  href='../../wp-includes/css/dist/block-library/style.min40df.css?ver=5.6' type='text/css' media='all' />
<link rel="https://api.w.org/" href="../../wp-json/index.html" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../xmlrpc0db0.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 5.6" />
<link rel="canonical" href="index.html" />
<link rel='shortlink' href='../../indexa743.html?p=3323' />
<link rel="alternate" type="application/json+oembed" href="../../wp-json/oembed/1.0/embedd004.json?url=https%3A%2F%2Fmw2013.museumsandtheweb.com%2Fpaper%2Fexploring-gigapixel-image-environments-for-science-communication-and-learning-in-museums%2F" />
<link rel="alternate" type="text/xml+oembed" href="../../wp-json/oembed/1.0/embed1834?url=https%3A%2F%2Fmw2013.museumsandtheweb.com%2Fpaper%2Fexploring-gigapixel-image-environments-for-science-communication-and-learning-in-museums%2F&amp;format=xml" />
<style type="text/css" id="custom-background-css">
body.custom-background { background-color: #850000; }
</style>
	</head>

<body class="paper-template-default single single-paper postid-3323 custom-background two-column right-sidebar">
<div id="page" class="hfeed">
	<header id="branding" role="banner">
			<hgroup>
				<h1 id="site-title"><span><a href="../../index.html" title="MW2013: Museums and the Web 2013" rel="home">MW2013: Museums and the Web 2013</a></span></h1>
				<h2 id="site-description">The annual conference of Museums and the Web | April 17-20, 2013 | Portland, OR, USA</h2>
			</hgroup>

						<a href="../../index.html">
									<img src="../../wp-content/uploads/2012/09/portland_strip.jpg" width="1000" height="288" alt="" />
							</a>
			
								<form method="get" id="searchform" action="https://mw2013.museumsandtheweb.com/">
		<label for="s" class="assistive-text">Search</label>
		<input type="text" class="field" name="s" id="s" placeholder="Search" />
		<input type="submit" class="submit" name="submit" id="searchsubmit" value="Search" />
	</form>
			
			<nav id="access" role="navigation">
				<h3 class="assistive-text">Main menu</h3>
								<div class="skip-link"><a class="assistive-text" href="#content" title="Skip to primary content">Skip to primary content</a></div>
				<div class="skip-link"><a class="assistive-text" href="#secondary" title="Skip to secondary content">Skip to secondary content</a></div>
								<div class="menu-mw2013-menu-container"><ul id="menu-mw2013-menu" class="menu"><li id="menu-item-238" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-has-children menu-item-238"><a href="../../index.html">About</a>
<ul class="sub-menu">
	<li id="menu-item-603" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-603"><a href="http://www.museumsandtheweb.com/">MW Community</a></li>
	<li id="menu-item-6483" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-6483"><a href="http://www.museumsandtheweb.com/conferences/">Conferences</a></li>
</ul>
</li>
<li id="menu-item-2524" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-2524"><a href="../../news/index.html">News</a></li>
<li id="menu-item-262" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-262"><a href="../../attending/index.html">Attending</a>
<ul class="sub-menu">
	<li id="menu-item-2357" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2357"><a href="../../registration/index.html">Registration</a></li>
	<li id="menu-item-244" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-244"><a href="../../key-dates/index.html">Key Dates</a></li>
	<li id="menu-item-243" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-243"><a href="../../local-information/index.html">Local Information</a></li>
	<li id="menu-item-242" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-242"><a href="../../call-for-proposals/index.html">Call for Proposals</a>
	<ul class="sub-menu">
		<li id="menu-item-508" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-508"><a href="../../call-for-proposals/guidelines-for-proposals/index.html">Guidelines for Proposals</a></li>
		<li id="menu-item-509" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-509"><a href="../../terms-and-conditions/index.html">Terms and Conditions</a></li>
		<li id="menu-item-510" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-510"><a href="../../submit-proposal/index.html">Submit Proposal</a></li>
		<li id="menu-item-2574" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2574"><a href="../../paper-guidelines/index.html">Paper Guidelines</a></li>
		<li id="menu-item-5875" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-5875"><a href="../../call-for-proposals/presentation-guidelines/index.html">Presentation Guidelines</a></li>
	</ul>
</li>
	<li id="menu-item-714" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-714"><a href="../../scholarships-volunteering/index.html">Scholarships &#038; Volunteering</a></li>
	<li id="menu-item-3146" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3146"><a href="../../exhibitor-rates/index.html">Sponsorship</a></li>
</ul>
</li>
<li id="menu-item-526" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-526"><a href="../../exhibiting/index.html">Exhibiting</a>
<ul class="sub-menu">
	<li id="menu-item-3196" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3196"><a href="../../exhibitor-rates/index.html">Exhibitor Rates</a></li>
	<li id="menu-item-527" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-527"><a href="../../exhibiting/exhibition-terms/index.html">Exhibition Terms</a></li>
	<li id="menu-item-263" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-263"><a href="../../registration/exhibitor-registration/index.html">Exhibitor Registration</a></li>
	<li id="menu-item-1935" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1935"><a href="../../guidelines-for-advertisers/index.html">Guidelines for Advertisers</a></li>
</ul>
</li>
<li id="menu-item-689" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-689"><a href="../../conference/index.html">Conference</a>
<ul class="sub-menu">
	<li id="menu-item-4186" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-4186"><a href="../../program/index.html">Program</a></li>
	<li id="menu-item-700" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-700"><a href="../../sessions/index.html">Sessions</a></li>
	<li id="menu-item-697" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-697"><a href="../../workshops/index.html">Workshops</a></li>
	<li id="menu-item-6745" class="menu-item menu-item-type-post_type menu-item-object-post menu-item-6745"><a href="../../introducing-mwx2013-new-exhibition-program-for-museums-and-the-web/index.html">MWX</a></li>
	<li id="menu-item-3220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-3220"><a href="../../best-of-the-web/index.html">Best of the Web</a>
	<ul class="sub-menu">
		<li id="menu-item-6424" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6424"><a href="../../best-of-the-web-winners/index.html">Best of the Web Winners</a></li>
		<li id="menu-item-3221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3221"><a href="../../best-of-the-web-criteria/index.html">Best of the Web Criteria</a></li>
		<li id="menu-item-3228" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3228"><a href="../../best-of-the-web-nominees/index.html">Best of the Web Nominees</a></li>
	</ul>
</li>
	<li id="menu-item-701" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-701"><a href="../../demonstrations/index.html">Demonstrations</a></li>
	<li id="menu-item-6255" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6255"><a href="../../salons/index.html">Salons</a></li>
	<li id="menu-item-528" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-528"><a href="../../exhibits/index.html">Exhibits</a></li>
	<li id="menu-item-702" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-702"><a href="../../sponsors/index.html">Sponsors</a></li>
	<li id="menu-item-6478" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6478"><a href="../../papers/index.html">Papers</a></li>
	<li id="menu-item-719" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-719"><a href="../../proceedings/index.html">Proceedings</a></li>
</ul>
</li>
<li id="menu-item-730" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-730"><a href="../../people/index.html">People</a>
<ul class="sub-menu">
	<li id="menu-item-731" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-731"><a href="../../speakers-authors/index.html">Speakers &#038; Authors</a></li>
	<li id="menu-item-732" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-732"><a href="../../attendees/index.html">Attendees</a></li>
	<li id="menu-item-257" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-257"><a href="../../program-committee/index.html">Program Committee</a></li>
	<li id="menu-item-3135" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-3135"><a href="../../bow-committee/index.html">BoW Committee</a></li>
</ul>
</li>
<li id="menu-item-6564" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-6564"><a href="http://www.museumsandtheweb.com/publication-order/">Publications</a></li>
</ul></div>			</nav><!-- #access -->
	</header><!-- #branding -->


	<div id="main">

<style type="text/css">
#full_width_content {
	margin-left: 76px;
	margin-right: 76px;
}

h2, h3, h4 {
	font-weight: bold;
}
</style>

<div id="primary">
<div id="full_width_content">
			

<h1 class="entry-title">Exploring Gigapixel Image Environments for Science Communication and Learning in Museums</h1>
<br />

<a href="../../user/indexc41d.html?u=ahmed_ansari">Ahmed Ansari</a>, USA, <a href="../../user/indexf433.html?u=illah">Illah Nourbakhsh</a>, USA, <a href="../../user/index92b5.html?u=chrisbartley">Chris Bartley</a>, USA, <a href="../../user/index2f4c.html?u=Marti">Marti Louw</a>, USA<br /><br /><b>Abstract</b><br />
<p>This paper describes an ongoing collaboration between the Carnegie Museum of Natural History, Carnegie Mellon University, and the University of Pittsburgh to explore and study how emerging gigapixel image technology can be used to support science communication and learning in natural history museums. Multitouch based explorations of dynamic visual information spaces present unique user interface and interaction design opportunities and challenges. This case study focuses on a human centered design approach to technology innovation, by describing the design research methodologies used and the series of prototypes developed. We share findings from a series of user studies to optimize navigation, annotation and exploration in a zoomable user interface, and highlight the unique affordances of multiscalar image platforms to scaffold disciplinary observation and engagement with scientific content. Lastly, we discuss future design directions and offer some frameworks that can be adopted by museums when seeking to incorporate explorable, high-resolution imagery into exhibit experiences, both on the museum floor and on-line.</p>
<p><b>Keywords: </b>Interaction design, high-resolution images, gigapixel, multiscalar, zoomable user interfaces, (ZUIs), annotations, petroglyphs, natural history museums, collections, science communication</p>
<h2><strong>1. Background</strong></h2>
<h3>Creating rich learning environments with gigapixel technology</h3>
<p>Gigapixel image technology is an emerging class of explorable interactive media. High-resolution images embedded in zoomable visual environments enable users to move from full panoramic views to incredibly deep close-ups often comprising of billions of pixels. These multiscalar image environments are most familiar in online mapping and geographic browsing applications such as <i>Yahoo! Maps</i> and <i>Google Earth</i>, but the information they can organize extends well beyond maps, including composite photographic images and even computationally rendered data visualizations. Deep zooming, or zoomable user interfaces (ZUIs), offer a spatial way to display and organize large amounts of information in a single interface using scroll, pan, and zoom controls (Bederson, 2011). Text, images, graphics, audio, and video can be embedded at spatial locations and zoom levels within an image, creating localized sites for commenting and conversation (Drucker et al., 2008<i>).</i>  Compared to earlier forms of interactive media, this technology affords users deeper navigational agency and exploratory control in a shared visual space.<b></b></p>
<p>Gigapixel image technology initially gained popularity with the photography and scientific communities enthused by a newfound ability to easily document and present subject matter with an extremely high degree of image resolution and fidelity (see the <a href="http://fofsgigapan.blogspot.com/" target="_blank" rel="noopener noreferrer">Fine Foundation Blog for Science Research</a>). Increasingly, gigapixel image platforms are being used by museums and media professionals to facilitate public engagement and learning experiences with visual data. Media design groups like <a href="http://ideum.com/blog/2011/01/gigapixel-kinect-controller/">Ideum</a> and <a href="http://www.xrez.com/tag/gigapixel/">xRez</a> are developing multitouch viewers and gesture-based interfaces for navigating and exploring deep zoom image spaces. Through its <a href="http://www.digitalnarratives.net/">Digital Narratives</a> initiative, Microsoft Research is developing new user-driven narrative controls and viewers that enhance the storytelling aspects of inherently nonlinear rich interactive media. The Applied Laboratory for Interactive Visualization and Embodiment (ALiVE) of Hong Kong&#8217;s City University is designing and testing  large-scale architectural solutions to the problem of presenting  immersive, interactive, whole-body kinesthetic experiences to museum-goers using museums&#8217; large-scale visual datasets of landscapes, archaeological sites and sacred texts (Kenderdine, 2010). Central to all these projects is the effort to invent new interfaces for interacting with explorable images, pushing beyond unmediated free exploration and into the realm of rich mediation and socially engaged, shared exploration.</p>
<p>Our project team has focused on design research that uncovers the affordances of gigapixel environments to support deep observation, engagement and learning. In a recent paper, Louw and Crowley describe the potential for multiscalar image platforms to support participatory learning experiences around visual evidence, highlighting the distinctive attributes of these networked visual environments for engaged science communication and learning (Louw &amp; Crowley, 2013). In terms of museum applications, gigapixel image-based platforms for learning exhibit five promising affordances:</p>
<ol>
<li><em>Deep looking and noticing in a shared observational space: </em>Users can move seamlessly from the macro to the micro within an image, enabling unseen details to emerge, and encouraging exploration and observation. In her article discussing the impact of the Google Art Project, which showcases high-resolution online reproductions of artworks from over 46 museums around the world, The Smithsonian&#8217;s Nancy Proctor points to the incredibly positive responses from museums, and notes that these online engagements with gigapixel images offer “intimate encounters with images at visual depths not possible even in the galleries” (Proctor, 2011).</li>
<li><i>Democratizing a tool of science: </i>Gigapixel image technology makes the process of taking, storing, sharing, and interpreting high-resolution images—a domain once the purview of surveillance and defense industries—available to the general public. The hardware and software required to easily create high-resolution images is now commercially available, with much of the code open source. Websites like <a href="http://gigapan.com/">GigaPan.org</a> and <a href="http://photosynth.net/">Photosynth.net</a> invite gigapixel image makers from all over the world to upload their content to be viewed, annotated, geolocated, commented on, and shared globally.</li>
<li><i>Encouraging participatory learning: </i>As a networked platform, the technology generates new possibilities for participatory learning and direct communication between and among disciplinary experts and the public, through crowdsourcing, open forums, and sharing of observations and interpretation. In 2010, the North Carolina State University Insect Museum began using gigapixel image technology to document their extensive insect collection, and has released over 2,700 panoramas of rare and fragile specimen drawers to the general public <a href="http://gigapan.com/profiles/ncsuinsectmuseum">online</a>. In a paper documenting the results and insights from the project, the NCSU team reported that the platform afforded them a new kind of public engagement opportunity whereby museum scientists and users could interact and have conversational exchanges about insect biology. (Bertone et al., 2012).</li>
<li><i>Offering new visuospatial ways to curate collections and environments: </i>Multiscalar images enable content to be organized and accessed visually along an explorable spatial dimension. Thus, depth and distance cues in visual perception can be used to order information, in addition to more traditional means of organizing exhibit content (thematic, chronological, geographic, hierarchical, etc.) to guide and encourage close looking. The <a href="http://www.naturevalleytrailview.com/">Nature Valley Trail View</a> is a prime example of an explorable visuospatial environment made possible by combining underlying image browsing technologies such as Streetview and GigaPan with geospatial data maps in unified visual information space. This project enables users to virtually explore and walk along trails at the Grand Canyon, Great Smoky Mountains and Yellowstone National Parks. Along the way contextual “call outs” provide additional, interactive media overlays for a more dynamic experience.</li>
<li><i>Enabling context-dependent annotations and mediation: </i>Embedded information can be revealed depending on user interactions and locations within a three-dimensional image space, dynamically tying information to user exploration (Luan et al. 2008). A playful example of this type of context-dependent mediation can be found on the portfolio website of the Canadian design firm <a href="http://castordesign.ca/">Castor</a>, which displays clever media-rich annotations as a user zooms in to see details of various inspirational objects on display.</li>
</ol>
<p>Clearly, a gigapixel media environment has the potential to extend how a museum visitor approaches and engages with interactive media. But to date there has been little research or evaluation work done assessing the merits of this type of technology platform for learning and engagement in museum contexts. Facilitating scientific or disciplinary based forms of observation so that museumgoers begin to engage with artifacts and collections more critically, analytically, and systematically is a challenging prospect that requires specific scaffolding and mediation (Eberbach &amp; Crowley, in press; Louw &amp; Crowley, 2013), and there are considerable interaction and information design challenges that zoomable user interfaces present (Bederson, 2008).</p>
<p>Museum exhibit experiences must also account for myriad interests, needs, goals and expectations that the public brings to these settings (Falk and Sheppard, 2006). As<i> ‘</i>natural’ and<i> </i>gesture-based user interfaces become the standard (Gammon &amp; Burch, 2008), the question becomes vitally important of how to develop intuitive interaction spaces that cater to disparate types of users, giving them deeper agency and choice in how to move through content in ways that are personally relevant and support coherent meaning making (Kidd, Ntalla &amp; Lyons, 2011). The following case study describes an effort to meet these design challenges in the context of a specific gigapixel application: the exploration, observation and appreciation of petroglyphs in a natural history museum setting.</p>
<h2><strong>2. Case Study: <em>Stories in the Rock</em></strong></h2>
<h3>Exploring the design of gigapixel image viewer for a museum environment</h3>
<p>The University of Pittsburgh Center for Learning in Out-of-School Environments (UPCLOSE), in collaboration with Carnegie Mellon University and Carnegie Museum of Natural History, is developing an NSF-funded set of demonstration projects to study the affordances of gigapixel based image technology for science communication and learning.  Our work is guided by three distinct approaches to designing engaging public-science learning experiences: Public Understanding of Science, Public Participation in Scientific Research, and Public Engagement in Science, each of which are described in the report “<i>Many experts, many audiences: public engagement with science and informal science education</i>” published by the Center for the Advancement of Informal Science Education (McCallie et al. 2009).  The following case study presents the first of three demonstration projects and highlights the ‘Public Understanding of Science’ engagement approach which emphasizes knowledge building and activities designed to enable learners to shape their own inquiry and actively construct meaning.</p>
<p>Dr. Sandra Olsen, Head of Anthropology, and Director of the Center for World Cultures at the Carnegie Museum of Natural History, leads the <a href="http://saudi-archaeology.com/">Arabian Rock Art Heritage Project</a> with a multinational team of researchers investigating a set of little known petroglyph sites found in remote regions of the Saudi Arabian desert. Etched into large rock faces are remarkable scenes of prehistoric people hunting wild animals, herding cattle, riding camels, and engaging in warfare on horseback. Some of these ancient rock art engravings date back to the Holocene Wet Phase (8,000 BCE–3,500 BCE), when the environment in the Arabian Peninsula was more like a savannah. During these expeditions, Dr. Olsen’s team documented dozens of rock art sites using GigaPan composite photography, texture mapping, and three-dimensional laser scanning. These visual data resources function not only as research evidence, but also work as fascinating windows into remote and inaccessible cultural sites of great importance; and they served as the inspiration for our <i>Stories in the Rock</i> project.</p>
<p>The <i>Stories in the Rock</i> demonstration project has three central aims. First, to study applications of multiscalar image environments to communicate and engage visitors in museum research and collections. Second, to support disciplinary-based ways of looking, noticing and observing features in an ancient petroglyph site. Third, to support user engagement through self-directed exploration and agency.</p>
<p>We next present a summary of our design research process and findings. The activities build towards fulfilling project goals, framing the possibilities and limits of gigapixel image interactions in a museum exhibit setting, generating frameworks and design concepts that meet both usability and learning objectives, and finally developing an iterative series of working prototypes that we deployed for testing and evaluation.</p>
<h3>Phase I | Exploratory research</h3>
<p>The initial project phase focused on formative design research to understand user and stakeholder goals, needs, and expectations, as well as research to define user interaction scenarios by observing engagement with existing gigapixel viewer-based technology. We used a combination of methods including individual interviews, think-aloud protocols <i>(Lewis, 1982),</i> a card sort activity<i> (Nielsen, Sano, 1994)</i>, heuristic evaluation techniques <i>(Nielsen, 1992),</i> and contextual inquiry methods <i>(Holtzblatt, 2004)</i> to elicit and understand user needs and desires and to evaluate existing systems. The collected data was then synthesized and analyzed to establish key findings and insights, and several theoretical design frameworks were applied to help generate a set of solutions for an interactive exhibit experience based on explorations, interactions and mediation of high-resolution images.</p>
<h4>Understanding curator needs &amp; goals</h4>
<p>A central goal of this project lies in facilitating communication between scientists and publics. As a starting point, we needed to learn more about the subject matter, its interpretation, and the curator’s communication and public engagement goals. To do this we conducted several semi-structured interviews with Dr. Olsen, and used a modified contextual inquiry method to understand her observational and interpretation practices, as well as how she utilizes gigapixel image data in her research. To support this inquiry, we printed a large 3” x 6” photograph of the Shuwaymis West petroglyph site as a prompt, and used post-it notes to spatially mark points of interest and identify important topics. This physical exercise functioned both as a way to capture rich information, and as means to begin thinking about translating scientific interpretation into a mediated exhibit experience</p>
<h4>Findings</h4>
<p>One of the insights that emerged from this work was the curator’s need to not only examine petroglyph scenes contained within a single panel, but also to make connections between images of different sites to illustrate, for example, carving tools and techniques, evolving representational strategies, and changing depictions of animal assemblages over time. To interpret the sometimes ambiguous figures depicted in the rock art, she frequently referenced modern-day equivalents to help differentiate a leopard from a cheetah figure, or to compare the great size and shape of ancient aurochsen horns to domesticated descendants of long-horn cattle. She noted the importance of finding ways to help museum visitors notice details, pick out salient features and make personal connections to topics of interest. These conversations provided ideas for interesting ways to mediate, tell stories, and help users observe these petroglyph images with the eye of an archaeologist.</p>
<p>Understanding user interactions with a gigapixel image viewer</p>
<p>To study user interactions with an explorable high-resolution image environments we began by examining an existing gigapixel image viewer.  Drawing on GigaPan images of petroglyph sites selected from Dr. Olsen’s <a href="http://saudi-archaeology.com/imaging-techniques/gigapan-imaging/">Arabian Rock Art Heritage</a> project, we set-up a 50” plasma display with an attached touch panel module to support single-touch inputs.  The user interface included a set of simple navigation elements affording zooming and panning on one side, and a series of snapshots at the bottom that would zoom into key hotspots within the image.</p>
<p>To assess the appeal and usability of this design, we used a standard think-aloud technique while observing and logging user interactions. Museum visitors were encouraged to talk about their experience while using the gigapixel viewer, and respond to questions about the interface, navigation, and content presentation. Each user group was also invited to participate in a subsequent card-sort activity to explore content themes, potential taxonomies and structure. Participants were given a number of topic cards showing a petroglyph figure associated with a subject theme (e.g. hunting, art &amp; craft, animals, culture) and then asked to rank the cards from most to least interesting. On a provided form, participants wrote down any questions that they had relating themes, and suggested additional topic areas of interest.</p>
<div>
<p>Findings</p>
<p>These user studies generated several key insights about the interaction and information design challenges associated with gigapixel image viewers. We found that participants, in particular older individuals and groups, found navigating the single touchscreen-based image viewer to be non-intuitive and somewhat frustrating. Younger audiences expected the touchscreen to support familiar multitouch scroll, pinch, swipe inputs. These younger users adapted quickly to the interface navigation elements. Nevertheless, poor touchscreen responsiveness led most users to rely on zoom sliders, snapshots, and button controls for navigation, which many felt were cumbersome and sluggish. Many users resorted to serially using snapshot selections to move around. Users, especially younger ones, would often walk away from the exhibit display if they found it difficult to navigate.</p>
<p>The existing user interface we tested did not encourage free exploration of the petroglyph scenes, nor support observational talk and gesturing about what was seen in the image. Users reported that the system gave them no sense of how to explore the image, or any kind of contextual information about the petroglyph site. In general, most of our participants wished for more interpretive and explanatory content rather than descriptions of what was being seen visually. Younger audiences repeatedly expressed a desire for more engaging audio-visual or interactive media to obtain information on particular image details or themes, while older audiences indicated that more in-depth textual information and visual supplements like photographs or illustrations would help make the exhibit more engaging.</p>
<p>Several important insights emerged from the card sort activities. Many of the responses revolved around background information and interpretation. For instance when shown an image of a bow hunter with leashed dogs pursuing wild cattle, some users wanted to know more about hunting in the Neolithic period. Others raised questions about rock art carving techniques, ancient cultures, or the geology of the site. We were able to identify popular and particular themes of interest (e.g. nomadic culture and lifestyle). Users also provided suggestions for additional topics of interest, such as rock art carving tools and techniques. During this activity, users indicated a clear desire to have more control over how to explore content. They made requests for a more organized or structured approach to the information, and they wanted to hear from the curator about points of interest within the petroglyph image. These findings would guide our content development and information design process in the next phase.</p>
</div>
<h4>Conducting a heuristics evaluation of the museum’s current gigapixel viewer</h4>
<p>Next, we conducted a heuristic evaluation to test to the functionality, usability, aesthetics, and content presentation of an existing kiosk-based gigapixel image viewer that had been developed for the Museum’s <i>M is for Museum</i> exhibition.  The viewer was designed to showcase seven GigaPan images taken by museum scientists, including both artifacts and specimens from the museum’s collections. The gigapixel image viewer provided simple navigation elements, including a zoom/pan slider control on the left side of the image &#8211; below the image was a row of snapshots with short text annotations that when selected zoomed into a key frame within the image, and a set of arrow buttons to select between images <em>(Fig. 1)</em>.</p>
<div id="attachment_3736" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/cmnh.fig1a.jpg.jpg"><img aria-describedby="caption-attachment-3736" loading="lazy" class="size-large wp-image-3736" alt="Figure 1a: The M is for Museum Gigapixel Image Viewer, Carnegie Museum of Natural History" src="../../wp-content/uploads/2013/01/cmnh.fig1a.jpg-1024x573.jpg" width="584" height="326" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/cmnh.fig1a.jpg-1024x573.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/cmnh.fig1a.jpg-300x168.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/cmnh.fig1a.jpg-500x280.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/cmnh.fig1a.jpg.jpg 1408w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3736" class="wp-caption-text">Figure 1: The M is for Museum Gigapixel Image Viewer, Carnegie Museum of Natural History</p></div>
<h4>Findings</h4>
<p>The heuristic evaluation technique confirmed many of the findings from earlier studies about the general lack of user friendliness, content issues, and the problems users had navigating through the visual environment. Additionally, we felt that the user interface elements took up too great a portion of screen real estate (a full third of the screen area was used), and there was redundancy in the way multiple interface elements performed similar tasks. Moreover, the snapshot annotations were external to the image, separating the close-up detail from the text annotations and other potential mediation. Again, we found that the text descriptions associated with the snapshots were overly brief, giving little scientific explanation, background or interpretive clues about what to look for in the image.</p>
<h4>Insights &amp; design implications</h4>
<p>The above findings would guide our content development and information design process in the next phase, and are summarized below:</p>
<ul>
<li>Develop smooth and responsive touchscreen elements with support for multitouch gestures;</li>
<li>Explore different approaches to navigation and content control elements;</li>
<li>Streamline user interface elements in order to maximize the screen area available for image viewing and exploration;</li>
<li>Add multimedia content (audio, video, animations, graphics, diagrams) that supports user observations and engagement with the subject matter;</li>
<li>Establish a clear visual hierarchy and ordering of information, revealing what type of content is available, and where it lies spatially within the gigapixel image; and</li>
<li>Frame interactions with the viewer as a mediated conversation with a knowledgeable guide.</li>
</ul>
<h3>Phase 2 | Concept development &amp; design</h3>
<p>The objective for the next phase of work was to generate design solutions that would address many of the issues and opportunities identified during the exploratory phase. Using a set of guiding frameworks to develop user experience models, we produced a number of wireframes and high fidelity mockups that we tested with museumgoers utilizing A/B testing protocols (Nielsen, 2005). Further refinements were made based on feedback, narrowing down various iterations to a final prototype that would undergo full development and implementation. The prototype was then subjected to further pilot testing in think-aloud protocols, before being deployed for an external summative evaluation.</p>
<p>Before diving into a description of some of the choices made in designing and developing the mockups and prototypes, it would be useful to draw attention to the guiding frameworks that we used to better frame the design challenges we faced. In particular, we identified three core opportunities that became the cornerstones of our design process moving forward.</p>
<h4>Translating user mental models into an engaging user experience cycle</h4>
<p>In our user studies, we found that the biggest hurdle to getting users to use the image viewer and immerse themselves within the content lie in the lack of a properly managed and streamlined user experience. We found that the best way to bridge our project goals with user and stakeholder expectations was to treat the user experience as a form of mediated conversation between an expert (i.e. the museum scientist or curator) and the learner. For this, we turned to studies on conversation analysis, particularly work drawing on Erving Goffman’s model of interpersonal communication (Goffman, 1963, Tang, 2007). An effective communication loop based on this model follows an “initiate—sustain—leaving-taking” cycle of interaction. In terms of a mediated interactive system, it requires an effort by the system to <i>initiate</i> contact by presenting the user with just enough information about a potential engagement to create awareness and attract a response, then <i>sustain</i> an engagement by creating an engaging user experience that provides a sense of agency and exchange in interaction without overwhelming the user with choices or content, and lastly, offering a proper <i>leave-taking</i> sequence that closes the loop while presenting opportunities for future interactions and dialogue.  This model informed several aspects of our design.<i></i></p>
<p>To entice users to approach the gigapixel kiosk, we developed a start screen <em>(Fig. 2)</em> that presented basic with contextual information (title, site location). A/B testing revealed that some users had difficulties in reading the start screen approach as a prompt for interaction, and they preferred the actual gigapixel panorama to be shown as a better indicator of the experience to follow.</p>
<div id="attachment_3739" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/home_screen.jpg"><img aria-describedby="caption-attachment-3739" loading="lazy" class=" wp-image-3739 " alt="Figure 2. Initial start screen mockup, © CREATE Lab    " src="../../wp-content/uploads/2013/01/home_screen-1024x768.jpg" width="584" height="438" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen-1024x768.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen-300x225.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen-400x300.jpg 400w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen.jpg 1600w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3739" class="wp-caption-text">Figure 2. Initial start screen mockup, © UPCLOSE</p></div>
<p>These changes were reflected in the final prototype <em>(Fig. 3)</em>. We also introduced a feedback loop within the viewer code, whereby the interface would revert back to the start screen after 90 seconds of inactivity.</p>
<div id="attachment_3741" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/home_screen2.jpg"><img aria-describedby="caption-attachment-3741" loading="lazy" class=" wp-image-3741 " alt="Figure 3. Final prototype start screen, © CREATE Lab   " src="../../wp-content/uploads/2013/01/home_screen2-1024x522.jpg" width="584" height="297" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen2-1024x522.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen2-300x153.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen2-500x255.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/home_screen2.jpg 1920w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3741" class="wp-caption-text">Figure 3. Final prototype start screen, © CREATE Lab</p></div>
<p>&nbsp;</p>
<p>To sustain engagement, we wanted to find ways to provide more navigational agency, support visual inquiry and offer content choices based on user interests. We began by finding ways to refine and streamline the user interface. By iteratively simplifying the kinds of overlays that we were using by eliminating the need for explicit onscreen pan-zoom controls given the multitouch screen interface and adding a dual function “navmap” that tracks a user’s spatial location and zoom depth, we were able to reduce the number of onscreen elements and allocate the entire screen to the gigapixel imag<i>e. </i>We were also able to add the ability to embed rich media content, i.e. video and audio, into the information overlays <i>(Fig. 4).</i></p>
<div id="attachment_3742" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/Untitled-3.jpg"><img aria-describedby="caption-attachment-3742" loading="lazy" class="size-large wp-image-3742" alt="Figure 4. An example of text and video playing in the final protoype, © CREATE Lab" src="../../wp-content/uploads/2013/01/Untitled-3-1024x525.jpg" width="584" height="299" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/Untitled-3-1024x525.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/Untitled-3-300x154.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/Untitled-3-500x256.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/Untitled-3.jpg 1920w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3742" class="wp-caption-text">Figure 4. An example of text and video playing in the final prototype, © CREATE Lab</p></div>
<p>Furthermore, as the design solution enabling tactile navigation, we settled on three multitouch gestures: a ‘tap’ to select, ‘swipe’ to pan, and a two-finger ‘pinch’ gesture to control zoom level. As an additional support, we provided an explanatory graphic beside the touchscreen describing how to use the multitouch gesture controls.  We found these gestures intuitive to many museumgoers across a wide range of ages, and the ability to lay hands on the image to virtually dive deep into it provides a compelling visceral experience unmatched by using traditional proxy devices such as a mouse or track pad.</p>
<h4>Scaffolding user observation through nested content structures<i><br />
</i></h4>
<p>Our initial findings had outlined that users wanted to have a sense of agency in identifying and choosing both what kinds of information was available, and how it was presented to them. This need aligned both with the goal to design the kinds of mediated conversations that sustain interest-based exchanges, and the stakeholder’s requests to find ways to scaffold and direct user attention to facilitate observation. In the first viewer that the museum was using, there was no way to determine what was important about the selected snapshots within an image, what kinds of information different snapshots linked to, and how different regions within the gigapixel image were related to each other.</p>
<p>Because of the way in which information can be located in the visual plane of a high-resolution multiscalar image, in the form of point annotations or hotspots, it affords two distinct modes of organization: taxonomic, and visuospatial. The gigapixel image space offer two orthogonal dimensions for structuring content – one lies at the surface of the screen, and lends itself ordered taxonomies based on the nature of the subject matter. The second dimension lies within the image space, where the spatial location of the content embedded in x-y-z spaces gives a perspective where and on how the taxonomic structure(s) is distributed within the image. So how themes or hotspots clustered within regions of the image becomes an additional visual information cue. Activating these modes of content organization enables a powerful way for users to more deeply explore based on personal interest.</p>
<p>Using Richard Saul Wurman’s information organizing principles (Wurman, 1989), we identified <em>location</em> (i.e. petroglyph sites), <em>chronology</em> (i.e. Neolithic, Holocene), <em>categories</em> (i.e. disciplinary topics or themes), <em>hierarchy</em> (i.e. alphabetical, scalar) as basic ways to structure the information design. In addition, we identified as additional organization filters such as <em>media types</em> (text, audio, video) or user type (i.e. information tailored for kids, adults etc.) as further ways we could structure content. While sorting by location, time periods, categories and themes are central to the ways in which scientists observe and identify important patterns depending on their specific discipline, allowing content to be accessed through forms of hierarchy like popularity, or through the annotation mediums in which it is presented could also be appealing to certain kinds of subject matter and users.</p>
<p>We prototyped several design solutions under different content structuring regimes to see how annotations, and the corresponding ancillary information, would be used. For example, we created mockups testing the use of a top menu bar that would give users a choice in how they wanted to sort through content, by allowing them to select a taxonomic meta-structure following one of our identified organizing principles, as well as determining how content could be accessed from within that structure. <em>(Fig. 5)</em>.</p>
<div id="attachment_3744" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/3.png"><img aria-describedby="caption-attachment-3744" loading="lazy" class=" wp-image-3744 " alt="Figure 5. Mockup showing chronological sorting using a timeline, © UPCLOSE" src="../../wp-content/uploads/2013/01/3-1024x768.png" width="584" height="438" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/3-1024x768.png 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/3-300x225.png 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/3-400x300.png 400w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/3.png 1600w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3744" class="wp-caption-text">Figure 5. Mockup showing chronological sorting using a timeline, © UPCLOSE</p></div>
<p>For thematic content, we created several different solutions testing different modes of interaction, the layout and interactivity of interface elements, and exploring design variables like color, shape, and icon to denote distinct categories of information<em> (Fig. 6)</em>.</p>
<div id="attachment_3745" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/mockup_theme2.jpg"><img aria-describedby="caption-attachment-3745" loading="lazy" class="size-large wp-image-3745" alt="Figure 6. An example of the user interface mockups using themed content structures, © UPCLOSE" src="../../wp-content/uploads/2013/01/mockup_theme2-1024x589.jpg" width="584" height="335" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/mockup_theme2-1024x589.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/mockup_theme2-300x172.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/mockup_theme2-500x287.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/mockup_theme2.jpg 1600w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3745" class="wp-caption-text">Figure 6. An example of the user interface mockups using themed content structures, © UPCLOSE</p></div>
<p>After evaluating the different prototypes through A/B testing with museumgoers, we found that users responded most favorably to a retractable ‘content drawer’ on the side of the left side of the screen, with a single color denoting a ‘selected/unselected’ theme state, so the corresponding annotations would reflect the chosen theme color and indicate the location and distribution of that theme within the image. This solution was adopted for the final prototype <i>(Fig. 7).</i></p>
<div>
<div id="attachment_3746" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/final_drawers.jpg"><img aria-describedby="caption-attachment-3746" loading="lazy" class=" wp-image-3746 " alt="Figure 7. Final prototype content drawers showing a selected theme, © CREATE Lab" src="../../wp-content/uploads/2013/01/final_drawers-1024x523.jpg" width="584" height="298" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/final_drawers-1024x523.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/final_drawers-300x153.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/final_drawers-500x255.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/final_drawers.jpg 1918w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3746" class="wp-caption-text">Figure 7. Final prototype content drawers showing a selected theme, © CREATE Lab</p></div>
<h4>Redefining annotations</h4>
</div>
<p>Finally, we focused how to best annotate content in a dynamic multiscalar image. We propose that annotations, whether textual or otherwise are a powerful tool to give users further control over information within a zoomable visual environment. The first museum gigapixel viewer used a ‘snapshot&#8217; metaphor where a sub-sampled frame within the image served as the navigation and annotation element. The problem with snapshot interface elements is that by separating snapshots from their spatial location within the image, it dislocates the information from its context, giving users little sense of how the spread of information and its location within the image matters. Framed snapshots also act as poor indicators of significance or markers of hard-to-see details within an image.</p>
<p>We chose to explore an ‘interest spot’ metaphor as a more precise way to pinpoint and contextualize annotations within a multiscalar zoomable user interface. Interest spots act as both a specific indicator to where visual details lie as one zooms in, and become the point from which content overlays can be centered and presented on screen. Our experiences suggest that these point annotations offered a flexible and compelling information design strategy.</p>
<p>Across several design iterations, we explored the idea of layering interest points with additional functionality and choice, giving users control over content modality, links to external sources, and access to social media via share buttons <em>(Fig. 8)</em>.We also considered supporting “interest spot” based conversations between users and the museum staff. These strategies would enhance the “mediated conversation” and leaving-taking aspects of the interaction cycle we aspired to. However, due to project scoping requirements and time constraints, we had to leave out development of these enhanced interest points in the final prototype. It is worth noting that over the course of our A/B testing participants generally liked the idea of added functionality, and this suggests a promising area for future investigation.</p>
<div id="attachment_3748" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/anotes.jpg"><img aria-describedby="caption-attachment-3748" loading="lazy" class="size-large wp-image-3748" alt="Figure 8: Enhancing interest points with added functionality, © UPCLOSE" src="../../wp-content/uploads/2013/01/anotes-1024x414.jpg" width="584" height="236" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/anotes-1024x414.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/anotes-300x121.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/anotes-500x202.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/anotes.jpg 1124w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3748" class="wp-caption-text">Figure 8: Enhancing interest points with added functionality, © UPCLOSE</p></div>
<h3>Phase 3: Prototype implementation, testing &amp; evaluation</h3>
<p>Throughout the design process, we worked closely with our technology partner, the CREATE Lab at Carnegie Mellon University, to vet ideas and understand the software engineering implications and feasibility of various design choices. The final prototype system needed to withstand the demands of a museum setting, which imposed several technical constraints: the system needed to run with minimal maintenance, it had to run locally without an Internet connection and cloud storage, it needed to be able to self-recover from a browser crash, and finally require minimal effort for the museum staff to start-up and shut down.</p>
<p>The final design was implemented in JavaScript and HTML5, rendered in the Mozilla Firefox web browser, running on Windows 7 Pro 64-bit. Since all assets are stored locally, the kiosk computer is running a local instance of the Apache HTTP Server to which the web browser connects to load all content for the interactive. Running the assets locally resulted in faster access times, and meant fewer memory leaks and crashes, which ensured that the viewer ran with minimal maintenance for the duration of the study. The gigapixel image viewer was created in a modified version of Microsoft Seadragon Ajax, which supported loading gigapixel images, making use of several open-source JavaScript libraries like jQuery, Hogan, and Less. We also chose hardware which would provide an optimal, lag free user experience, utilizing a quad core Intel i7 CPU, 8 gigabytes of RAM, and a solid-state hard drive for reliable and fast access to the image files, and a 24” multitouch touchscreen which was installed in a customized freestanding kiosk case.</p>
<p>Furthermore, the viewer incorporated an event tracking system which records every user action, so we have exact statistics on which parts of the gigapixel image are most explored, which audio/video elements are most popular, active usage time vs. idle time, etc. The statistics on usage patterns gathered from the data logs gathered through the interactive viewer have been very useful in complementing our observational data. These statistics have been compiled and visualized using Google’s Fusion Tables software.</p>
<p>In order to test the effectiveness of the new viewer in the context of an exhibition setting, we worked with Laser Labs, a local design and fabrication studio, to develop the supporting Stories in the Rock exhibit environment <em>(Fig. 9), </em>which housed both the interactive kiosk testing, and showcased a large print GigaPan print, wall panel graphics, and “stone phone” audio handsets. We also developed a set of tabletop rock art tracing and card matching activities for families to help looking at petroglyphs with an archaeologist’s eye.</p>
<div id="attachment_3749" style="width: 842px" class="wp-caption alignleft"><a href="http://gigapan.com/gigapans/119442/options/nosnapshots/iframe/2.0/flash.html"><img aria-describedby="caption-attachment-3749" loading="lazy" class=" wp-image-3749 " alt="Figure 9: Panorama of the exhibit space, © UPCLOSE" src="../../wp-content/uploads/2013/01/image003.png" width="832" height="327" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/image003.png 832w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/image003-300x117.png 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/image003-500x196.png 500w" sizes="(max-width: 832px) 100vw, 832px" /></a><p id="caption-attachment-3749" class="wp-caption-text">Figure 9: Gigapixel image of the Stories in the Rock exhibit space, © UPCLOSE</p></div>
<p>The exhibit was launched as a demonstration project in late November 2012.</p>
<h4>Case study results</h4>
<p>While we are currently in the process of synthesizing both qualitative and quantitative findings, initial observations and our customized analytics program yield promising results.  As of January 2013, we logged 1136 user sessions over a 57-day period, with a mean time of 4 minutes logged per session.  This average session duration shows a considerable improvement over the mean time observed with the interactive museum viewer that existed at the start of the project. We were also able measure which interest points and themes were most frequently selected by users, giving us an indication navigation preferences and the kinds of content users preferred. Interestingly, the ratio of theme to interest spot clicks is quite low, suggesting that users are favoring interest-spot over theme-based explorations <i>(Fig. 10)</i>. We were also able to track playable media usage &#8211; 37.4% of users watched or listened to media in the content overlays, indicating that multimedia content is clearly something that users find engaging. Audio was played slightly more frequently than the videos.</p>
<div id="attachment_3952" style="width: 594px" class="wp-caption alignleft"><a href="../../wp-content/uploads/2013/01/sitr.jpg"><img aria-describedby="caption-attachment-3952" loading="lazy" class="size-large wp-image-3952" alt="Figure 10: The ratio of interest point (blue) to theme selection (red) activations, © UPCLOSE" src="../../wp-content/uploads/2013/01/sitr-1024x438.jpg" width="584" height="249" srcset="https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/sitr-1024x438.jpg 1024w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/sitr-300x128.jpg 300w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/sitr-500x214.jpg 500w, https://mw2013.museumsandtheweb.com/wp-content/uploads/2013/01/sitr.jpg 1885w" sizes="(max-width: 584px) 100vw, 584px" /></a><p id="caption-attachment-3952" class="wp-caption-text">Figure 10: The ratio of interest point (blue) to theme selection (red) activations, © UPCLOSE</p></div>
<h2><strong>3. Future directions</strong></h2>
<p>Our design research findings indicate that the new developed <i>Stories in the Rock </i>gigapixel viewer has  successful meet many of the project goals.  User engagement with the gigapixel image viewer has increased across the board, both in terms of the average length of user session times, and in the use of supporting text and media annotations.  Based on preliminary observational data tracking conversations and gestures, the experience appears to be increasing scientific forms of observation and talk, and users are better able to explore the subject matter based on their personal interests. The summative evaluation currently being conducted by Rockman et al. will further deepen our understanding of whether engagement, observation and learning goals have been met.</p>
<p>We also want to highlight several promising future directions. Enhancing annotations with additional functionality is certainly something that requires deployment and further user testing. We also feel that the current prototype did not fully exploit the opportunity to use the gigapixel image platform as a social media space. As we continue to develop this system, we will look for ways to support more direct engagement and discussion between museum experts and a communities of users/learners—and following Goffman’s cycle of engagement—to <i>sustain</i> dialogic exchanges. Currently no functionality exists for leaving comments or observations within the multiscalar image space, or for sharing information via social media channels, and these possibilities merit further exploration. Adding the ability to share or tag content that users found particularly interesting, or would like others to connected with, would also help to strengthen the <i>leave-taking</i> aspect of the user experience cycle that we have proposed, giving users an incentive to revisit or connect to new content.</p>
<p>While our current project focused mainly on illuminating non-linear navigational control and content structuring within a single gigapixel image, we recognize another area of need based on our meetings with the museum scientists; that is developing ways to navigate and structure content relationships between gigapixel images, and for that matter, other coordinating with other forms of high-resolution visual data (e.g. geospatial, simulations, visualizations).  Moreover, research groups are inventing evermore powerful interactive visual browsing tools for high resolution media. Our CREATE Lab technology partner has developed <a href="http://www.cmu.edu/homepage/computing/2011/spring/gigapan-time-machine.shtml">GigaPan Timemachine</a> which extends gigapixel image exploration over time – adding another dimension of animation and complexity to the design space.</p>
<p>Through our work and conversations with museum staff, we have been made aware that there is considerable interest using gigapixel image technology to bring other aspects of museum research and collections to the public. Our next demonstration project focuses on working with the entomology section to develop a “digital teaching collection” to help overcome learning challenges inherent in aquatic macroinvertebrate identification and classification activities. “Taxonomic ID bottlenecks” are endemic to many citizen science and biomonitororing programs, and our work here will look for ways to use explorable, annotatable high-resolution macrophotography to make identifying and learning about the insects found in freshwater streams easier and more engaging.</p>
<p>Our hope is that the compelling visual nature of these networked explorable image environments evolve into a fully realized cyberlearning infrastructure that deepens observational practices and rich exchanges between experts and publics in a shared visual environment.</p>
<h2>Acknowledgements</h2>
<p>Gigapan images courtesy of Dr. Sandra Olsen, Richard T. Bryant and the Layan Cultural Foundation.  We would also like to thank Mary Ann Steiner, Kevin Crowley, Camellia Sanford, and the scientists and staff at the Carnegie Museum of Natural History for their many contributions to this project.</p>
<p>This material is based upon work supported by the National Science Foundation under <a href="http://nsf.gov/awardsearch/showAward.do?AwardNumber=1114476http://nsf.gov/awardsearch/showAward.do?AwardNumber=1114476">Grant No. 1114476</a>.<b> </b>Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2><strong>References</strong></h2>
<p>Bederson, B. B. (2011). “The promise of zoomable user interfaces”. <i>Behaviour &amp; Information Technology</i><i>, </i>30(6), 853-866.</p>
<p>Bertone, M. A., Blinn, R. L., Dew, T. M. S. J., Seltmann, K. C., &amp; Deans, A. R. (2012). “Results and insights from the NCSU Insect Museum GigaPan project”. <i>ZooKeys</i><i>,</i> (209), 115.</p>
<p>Beyer, H., &amp; Holtzblatt, K. (1997). “Contextual Design: Defining Customer Centered Systems”, Morgan Kaufmann: San Francisco.</p>
<p>Eberbach, C.<cite>, </cite>Crowley, K<cite>. </cite>(In Press)<cite> </cite>From seeing to observing: How parents and children learn to see science in a botanical garden<cite>. </cite><i>Journal of the Learning Sciences.</i><cite><br />
</cite></p>
<p>Falk, J.H., &amp; Sheppard, B. (2006). “Thriving in the Knowledge Age: New Business Models for Museums and Other Cultural Institutions”, Lanham: Altamira Press.</p>
<p>Gammon, B., &amp; Burch, A. (2008). “Designing mobile digital experiences”. <i>Digital technologies and the museum experience: Handheld guides and other media</i>, Lanham: Altamira Press, 35.</p>
<p>Goffman, E. 1963. Behavior in Public Places: Notes on the Social Organization of Gatherings, Glencoe: The Free Press</p>
<p>Kenderdine, S. (2010). &#8220;Immersive visualization architectures and situated embodiments of culture and heritage&#8221;, <i>Information Visualisation (IV), 2010 14th International Conference</i> (pp. 408-414). IEEE.</p>
<p>Kidd, J., Ntala, I., &amp; Lyons, W. (2011). &#8220;Multi-touch interfaces in museum spaces: reporting preliminary findings on the nature of interaction”, <i>Re-thinking Technology in Museums: Emerging Experiences, Ciolfi, Scott and Barbieri,</i> University of Limerick.</p>
<p>Louw, M., &amp; Crowley, K. (2013). “New Ways of Looking and Learning in Natural History Museums: The Use of Gigapixel Imaging to Bring Science and Publics Together”. <i>The Museum Journal</i><i>,</i> 56(1), 87-104.</p>
<p>Lewis, C., &amp; Rieman, J. (1993). “Task-centered user interface design: a practical introduction”,<b> </b>University of Colorado, Boulder,<i> </i>(This shareware book is available at ftp.cs.colorado.edu).</p>
<p>Luan, Q., Drucker, S. M., Kopf, J., Xu, Y. Q., &amp; Cohen, M. F. (2008). “Annotating gigapixel images”. <i>Proceedings of the 21st annual ACM symposium on User interface software and technology</i>, ACM., 33-36.</p>
<p>McCallie, E., Bell, L., Lohwater, T., Falk, J. H., Lehr, J. L., Lewenstein, B. V., &amp; Wiehe, B. (2009). “Many experts, many audiences: Public engagement with science and informal science education”. A CAISE Inquiry Group Report, 1.</p>
<p>Nielsen, J. &amp; Sano, D. (1995). “<em>SunWeb: User interface design for Sun Microsystem&#8217;s internal web”</em>, <i>Computer Networks and ISDN Systems</i>, 28(1), 179-188.</p>
<p>Nielsen, J. (1992, June). “Finding usability problems through heuristic evaluation”. <i>Proceedings of the SIGCHI conference on Human factors in computing systems, </i>ACM, 373 – 380.</p>
<p>Nielsen, J. (2005). “Putting A/B Testing in Its Place”, available at http://www.useit.com</p>
<p>Proctor, N. (2011). “The Google Art Project: A New Generation of Museums on the Web?”,  <i>The Museum Journal</i><i>,</i> 54(2), 215-221.</p>
<p>Tang, J. C. (2007). “Approaching and leave-taking: Negotiating contact in computer-mediated communication”, <i>ACM Transactions on Computer Human Interaction</i>, 14(1).</p>
<p>Wurman, R. S. (1989), Information Anxiety, New York: Doubleday</p>

<hr>

<script type="text/javascript">
var d=new Date();
var month=new Array();
month[0]="January";
month[1]="February";
month[2]="March";
month[3]="April";
month[4]="May";
month[5]="June";
month[6]="July";
month[7]="August";
month[8]="September";
month[9]="October";
month[10]="November";
month[11]="December";

var day_number=d.getDate();
var month_name=month[d.getMonth()];
var year=d.getFullYear();
var full_date = month_name+" "+day_number+", "+year;
</script>

Cite as:<br /> 

A. Ansari, I. Nourbakhsh, C. Bartley and M. Louw, Exploring Gigapixel Image Environments for Science Communication and Learning in Museums. In <i>Museums and the Web 2013</i>, N. Proctor & R. Cherry (eds). Silver Spring, MD: Museums and the Web. Published January 30, 2013. Consulted <script type="text/javascript">document.write(full_date);</script> <!-- April 23, 2022 -->.<br />
https://mw2013.museumsandtheweb.com/paper/exploring-gigapixel-image-environments-for-science-communication-and-learning-in-museums/<br /><br /><hr>
<!-- 
<script type="text/javascript" src='http://cdnjs.cloudflare.com/ajax/libs/jstimezonedetect/1.0.4/jstz.min.js'></script>
<script type="text/javascript">
var tz = jstz.determine();
tz.name();
</script>


<script type="text/javascript">document.write(tz.name()); 
</script>UTCApril 23, 2022-->

	<div id="comments">
	
	
	
		<div id="respond" class="comment-respond">
		<h3 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="index.html#respond" style="display:none;">Cancel reply</a></small></h3><p class="must-log-in">You must be <a href="../../wp-loginf851.html?redirect_to=https%3A%2F%2Fmw2013.museumsandtheweb.com%2Fpaper%2Fexploring-gigapixel-image-environments-for-science-communication-and-learning-in-museums%2F">logged in</a> to post a comment.</p>	</div><!-- #respond -->
	
</div><!-- #comments -->

</div><!-- #content -->
</div><!-- #primary -->



	</div><!-- #main -->

	<footer id="colophon" role="contentinfo">

			
<div id="supplementary" class="two">
		<div id="first" class="widget-area" role="complementary">
		<aside id="text-2" class="widget widget_text">			<div class="textwidget">Founded by Archives & Museum Informatics<br>
<a href="http://www.archimuse.com/">www.archimuse.com</a></div>
		</aside>	</div><!-- #first .widget-area -->
	
	
		<div id="third" class="widget-area" role="complementary">
		<aside id="text-3" class="widget widget_text">			<div class="textwidget"><p style="text-align: right; ">
Managed by Museums and the Web LLC<br>
703 Dale Drive<br>
Silver Spring MD 20910 USA<br>
info@museumsandtheweb.com</div>
		</aside>	</div><!-- #third .widget-area -->
	</div><!-- #supplementary -->
			<div id="site-generator">
								<a href="http://wordpress.org/" title="Semantic Personal Publishing Platform" rel="generator">Proudly powered by WordPress</a>
			</div>
	</footer><!-- #colophon -->
</div><!-- #page -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-26332456-1"></script>
<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-26332456-1', {
  		'linker': {
    	'domains': ['museweb.net', 'mwconf.org', 'museumsandtheweb.com']
  }
});
</script>

<script type='text/javascript' src='../../wp-includes/js/comment-reply.min40df.js?ver=5.6' id='comment-reply-js'></script>
<script type='text/javascript' src='../../wp-includes/js/wp-embed.min40df.js?ver=5.6' id='wp-embed-js'></script>

</body>

<!-- Mirrored from mw2013.museumsandtheweb.com/paper/exploring-gigapixel-image-environments-for-science-communication-and-learning-in-museums/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 23 Apr 2022 21:32:46 GMT -->
</html>