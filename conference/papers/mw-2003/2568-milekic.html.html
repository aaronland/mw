<html><!-- #BeginTemplate "/Templates/mw2003-papers.dwt" --><!-- DW6 -->

<!-- Mirrored from www.museumsandtheweb.com/mw2003/papers/milekic/milekic.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 20:33:17 GMT -->
<head>
<!-- #BeginEditable "doctitle" --> 
<title>Museums and the Web 2003: Papers: Milekic</title>
<!-- #EndEditable --> 
<meta name="keywords" content="Museums and the Web 2003, Archives & Museum Informatics, museums online, on-line, cultural heritage online, museum digitization, internet, conference, symposium, workshop, meeting, international, papers, presentations, multimedia, interactive, education, exhibits, evaluation, virtual reality, digitization, information architecture, information design, interface design, digital library, digital libraries ">
<!-- #BeginEditable "page keywords" --> 
<meta name="keywords" content="eye tracking, gaze tracking, intention-based interface">
<!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=">
<!-- #BeginEditable "script" --><!-- #EndEditable --> 
<link rel="stylesheet" href="../../Library/mw2003.css" type="text/css">
<script language="JavaScript">
<!--
function MM_swapImgRestore() { //v3.0
  var i,x,a=document.MM_sr; for(i=0;a&&i<a.length&&(x=a[i])&&x.oSrc;i++) x.src=x.oSrc;
}

function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.0
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && document.getElementById) x=document.getElementById(n); return x;
}

function MM_swapImage() { //v3.0
  var i,j=0,x,a=MM_swapImage.arguments; document.MM_sr=new Array; for(i=0;i<(a.length-2);i+=3)
   if ((x=MM_findObj(a[i]))!=null){document.MM_sr[j++]=x; if(!x.oSrc) x.oSrc=x.src; x.src=a[i+2];}
}
//-->
</script>
</head>

<body bgcolor="#FFFFFF" background="../../../mw2002/images/mw2002.bg.gif" text="#000000" link="#660099" vlink="#000066" onLoad="MM_preloadImages('../../images/register_on.gif','../../images/workshops_on.gif','../../images/sessions_on.gif','../../images/speakers_on.gif','../../images/interact_on.gif','../../images/demos_on.gif','../../images/exhibits_on.gif','../../images/events_on.gif','../../images/best_on.gif','../../images/dates_on.gif','../../images/charlotte_on.gif','../../images/sponsors_on.gif')">
<table width="600" border="0" cellspacing="2" cellpadding="5">
  <tr> 
    <td width="145" align="LEFT" valign="TOP"> 
      <p><a href="../../index.html"><img src="../../images/mw.gif" width="112" height="155" border="0" alt="/mw/"></a></p>
      <p> <a href="../../register/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('register','','../../images/register_on.gif',1)"><img name="register" border="0" src="../../images/register_off.gif" width="112" height="18"></a><br>
        <a href="../../workshops/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('workshops','','../../images/workshops_on.gif',1)"><img name="workshops" border="0" src="../../images/workshops_off.gif" width="112" height="18"></a><br>
        <a href="../../sessions/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('sessions','','../../images/sessions_on.gif',1)"><img name="sessions" border="0" src="../../images/sessions_off.gif" width="112" height="18"></a><br>
        <a href="../../speakers/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('speakers','','../../images/speakers_on.gif',1)"><img name="speakers" border="0" src="../../images/speakers_off.gif" width="112" height="18"></a><br>
        <a href="../../interact/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('interactions','','../../images/interact_on.gif',1)"><img name="interactions" border="0" src="../../images/interact_off.gif" width="112" height="18"></a><br>
        <a href="../../demos/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('demonstrations','','../../images/demos_on.gif',1)"><img name="demonstrations" border="0" src="../../images/demos_off.gif" width="112" height="16"></a><br>
        <a href="../../exhibit/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('exhibits','','../../images/exhibits_on.gif',1)"><img name="exhibits" border="0" src="../../images/exhibits_off.gif" width="112" height="19"></a><br>
        <a href="../../events/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('events','','../../images/events_on.gif',1)"><img name="events" border="0" src="../../images/events_off.gif" width="112" height="18"></a><br>
        <a href="../../best/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('best','','../../images/best_on.gif',1)"><img name="best" border="0" src="../../images/best_off.gif" width="112" height="18"></a><br>
        <a href="../../dates/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('dates','','../../images/dates_on.gif',1)"><img name="dates" border="0" src="../../images/dates_off.gif" width="112" height="18"></a><br>
        <a href="../../charlotte/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('charlotte','','../../images/charlotte_on.gif',1)"><img name="charlotte" border="0" src="../../images/charlotte_off.gif" width="112" height="18"></a><br>
        <a href="../../sponsor/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('sponsors','','../../images/sponsors_on.gif',1)"><img name="sponsors" border="0" src="../../images/sponsors_off.gif" width="112" height="21"></a> 
        <br>
        <br>
        <a href="http://www.archimuse.com/" target="_top"><img src="../../images/nav_ami.gif" width="135" height="25" border="0" alt="A&amp;MI home"></a> 
        <br>
        <span class="small">Archives & Museum Informatics<br>
        158 Lee Avenue<br>
        Toronto Ontario<br>
        M4E 2P3 Canada</span></p>
      <p class="small">ph: +1 416-691-2516<br>
        fx: +1 416-352-6025</p>
      <p><span class="small">info @ archimuse.com</span><span class="small"><br>
		<a href="http://www.archimuse.com/">www.archimuse.com</a></span></p>
      <table width="74">
        <tr> 
          <td> <a href="http://search.museumsandtheweb.com/search" target="_top"> <img src="../../images/search.gif" width="24" height="25" alt="Search" border="0" name="Search"></a> 
          </td>
          <td valign="MIDDLE"> <a href="http://search.museumsandtheweb.com/search"> 
            <span class="verysmall">Search<br></span></a> </td>
        </tr>
      </table>
      <p><font face="Arial, Helvetica, sans-serif" class="verysmall"><span class="small">Join 
        our <a href="http://search.museumsandtheweb.com/mailinglist/"> Mailing List</a>. 
        <br>
        <a href="http://search.museumsandtheweb.com/terms-of-use-privacy/"> Privacy</a>.</span></font> 
      </p>
      <p>&nbsp; </p>
 <p><span class="verysmall">published: March 2004<br>
        analytic scripts updated:<br>
		  <!-- #BeginDate format:Am1 -->October 28, 2010<!-- #EndDate -->
        </span>
         </p>
     <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img src="http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0  License" width="88" height="31" style="border-width:0" /></a>
     </td>
    <td width="455" align="LEFT" valign="TOP" class="normal"><a href="../../speakers/index.html"><img src="../../images/PAPERS.gif" width="390" height="55" border="0" alt="Museums and the Web 2003 Papers"></a> 
	  <br>
	  <p>&nbsp;</p>
	 <!-- #BeginEditable "Body of Page" --> 
	  <p class=PaperTitle>The More You Look the More You Get: Intention-based 
		Interface using Gaze-tracking</p>
	  <p class=Author>Slavko Milekic, The University of the Arts, USA</p>
	  <p class=URL><a href="http://www.uarts.edu/" target="_blank">http://www.uarts.edu</a></p>
	  <p class=AbstractTitle>Abstract</p>
	  <p class=AbstractText>Only a decade ago eye- and gaze-tracking technologies 
		using cumbersome and expensive equipment were confined to university research 
		labs. However, rapid technological advancements (increased processor speed, 
		advanced digital video processing) and mass production have both lowered 
		the cost and dramatically increased the efficacy of eye- and gaze-tracking 
		equipment. This opens up a whole new area of interaction mechanisms with 
		museum content. In this paper I will describe a conceptual framework for 
		an interface, designed for use in museums and galleries, which is based 
		on non-invasive tracking of a viewer's gaze direction. Following the simple 
		premise that prolonged visual fixation is an indication of a viewer's 
		interest, I dubbed this approach intention-based interface.</p>
	  <p class=keywords>Keywords: eye tracking, gaze tracking, intention-based 
		interface</p>
	  <h1 class="SubHeader">Introduction</h1>
	  <p class=normal>In humans, gaze direction is probably the oldest and earliest 
		<i>means of communication at a distance. </i>Parents of young infants 
		are often trying to 'decode' from an infant's gaze direction the 
		needs and interest of their child. Thus, gaze direction can be viewed 
		as a first instance of <i>pointing</i>. A number of developmental studies 
		(Scaife and Bruner 1975; Corkum and Moore, 1988; Moore 1999 ) show that 
		even very young infants actively follow and respond to the gaze direction 
		of their caregivers. The biological significance of eye movements and 
		gaze direction in humans is illustrated by the fact that humans, unlike 
		other primates, have visible white area (sclera) around the pigmented 
		part of the eye (iris, covered by transparent cornea, see Figure 1). This 
		makes even discrete shifts of gaze direction very noticeable (as is painfully 
		obvious in cases of 'lazy eye'). </p>
	  <p class=normal align="center"><img src="milekicFig01.jpg" width="398" height="140"></p>
	  <p class=MsoCaption><b>Figure 1. Comparison of human and non-human eye (chimpanzee). 
		Although many animals have pigmentation that accentuates the eyes, the 
		visible white area of human eye makes it easier to interpret the gaze 
		direction</b></p>
	  <p class=normal>Eye contact is one of the first behaviors to develop in 
		young infants. Within the first few days of life, infants are capable 
		of focusing on their caregiver's eyes (Infants are physiologically shortsighted 
		with the ideal focusing distance of 25-40 cm. This distance corresponds 
		to the distance between the mother's and infant's eyes when the baby is 
		held at the breast level. Everything else is conveniently a blur. Within 
		the first few weeks, establishing eye contact with the caregiver produces 
		a smiling reaction (Stewart &amp; Logan, 1998). Eye contact and gaze direction 
		continue to play a significant role in social communication throughout 
		life. Examples include:</p>
	  <ul>
		<li class="normal"> <span class="normal"> regulating conversation flow;</span></li>
		<li class="normal"> <span class="normal"> regulating intimacy levels; 
		  </span></li>
		<li class="normal"> <span class="normal"> indicating interest or disinterest; 
		  </span></li>
		<li class="normal"> <span class="normal"> seeking feedback; </span></li>
		<li class="normal"> <span class="normal"> expressing emotions; </span></li>
		<li class="normal"> <span class="normal"> influencing;</span></li>
		<li class="normal"> <span class="normal"> signaling and regulating social 
		  hierarchy;</span></li>
		<li class="normal"> <span class="normal"> indicating submissiveness or 
		  dominance;</span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>Thus, it is safe to assume that humans have a large number 
		  of behaviors associated with eye movements and gaze direction. Some 
		  of these are innate (orientation reflex, social regulation), and some 
		  are learned (extracting information from printed text, interpreting 
		  traffic signs).</p>
		<p class=normal>Our relationship with works of art is essentially a social 
		  and intimate one. In the context of designing a gaze tracking-based 
		  interface with cultural heritage information, innate visual behaviors 
		  may play a significant role precisely because they are social and emotional 
		  in nature and have the potential to <i>elicit a reaction</i> external 
		  to the viewer. In this paper I will provide a conceptual framework for 
		  the design of gaze&#8209;based interactions with cultural heritage information 
		  using the digital medium. Before we proceed, it is necessary to clarify 
		  some of the basic physiological and technological terms related to eye- 
		  and gaze-tracking.</p>
		<h1 class="SubHeader">Eye Movements and Visual Perception</h1>
		<p class=normal>While we are observing the world, our subjective experience 
		  is that of a smooth, uninterrupted flow of information and a sense of 
		  the wholeness of the visual field. This, however, contrasts sharply 
		  with what actually happens during visual perception. Our eyes are stable 
		  only for brief periods of time (200-300 milliseconds) called <i>fixations</i>. 
		  Fixations are interspersed by rapid, jerky movements called <i>saccades</i>. 
		  During these movements no new visual information is acquired. Furthermore, 
		  the information gained during the periods of fixations is clear and 
		  detailed only in a small area of the visual field  -  about 2&#176; of 
		  visual angle. Practically, this corresponds to the area covered by one's 
		  thumb at arm's length. The rest of the visual field is fuzzy but provides 
		  enough information for the brain to plan the location of the next fixation 
		  point. The problems that arise because of the discrepancy between our 
		  subjective experience and the data gained by using eye-tracking techniques 
		  can be illustrated by the following example:</p>
		<p class=normal align="center"><img src="MilekicIllustration01.jpg" width="398" height="111"></p>
		<p class=MsoCaption>Figure 1</p>
		<p class=normal>The sentence above is a classical example of a "garden 
		  path" sentence that (as you probably have experienced) initially leads 
		  the reader to a wrong interpretation (Bever, 1970). The eye-tracking 
		  data provide information about the sequence of fixations (numbered 1 
		  to 7) and their duration in milliseconds. The data above provide some 
		  clues about the relationship between visual analysis during reading 
		  and eye movements. For example, notice the presence of two <i>retrograde 
		  saccades</i> (numbered 6 and 7) that happened after initial reading 
		  of the sentence. They more than double the <i>total fixation time</i> 
		  of the part of the sentence necessary for disambiguation of its meaning. 
		  Nowadays there is a general consensus in the eye-tracking community 
		  that the number and the duration of fixations are related to the cognitive 
		  load imposed during visual analysis. </p>
		<p class=normal align="center"><img src="milekicFig02.jpg" width="373" height="290"></p>
		<p class=MsoCaption>Figure 2. Illustration of differences in gaze paths 
		  while interpreting I. Repin's painting "They did not expect him." </p>
		<p class=normal>Path (1) corresponds to free exploration. Path (2) was 
		  obtained when subjects were asked to judge the material status of the 
		  family, and path (3) when they were asked to guess the age of different 
		  individuals. Partially reproduced from Yarbus, A. L. (1967<b>)</b></p>
		<p class=normal>Eye-tracking studies of reading are very complex but have 
		  the advantage of allowing fine control of different aspects of the visual 
		  stimuli (complexity, length, exposure time, etc.). Interpretation of 
		  eye movement data during scene analysis is more complicated because 
		  visual exploration strategy is heavily dependent on the <i>context of 
		  exploration</i>. Data (Figure 2) from an often-cited study by Yarbus 
		  (1967) illustrate differences in visual exploration paths during interpretation 
		  of Ilya Repin's painting "They did not expect him, or "the unexpected 
		  guest".</p>
		<h2 class="SubHeader">Brief History of Eye- and Gaze-Tracking </h2>
		<p class=normal>The history of documented eye- and gaze-tracking stu<span class="normal">dies 
		  is over a hundred years old (Javal, 1878). It is a history of technological 
		  and theoretical advances where progress in either area would influence 
		  the other, often producing a burst of research activity that would subsequently 
		  subside due to the uncovering of a host of new problems associated with 
		  the practical uses of eye-tracking. </span></p>
		<p class=normal><span class="normal">Not surprisingly, the first eye-tracking 
		  studies used other humans as tracking instruments by utilizing strategically 
		  positioned mirrors to infer gaze direction. Experienced psychotherapists 
		  (and socially adept individuals) still use this technique, which, however 
		  imperfect it may seem, may yield a surprising amount of useful information. 
		  Advancements in photography led to the development of a technique based 
		  on capturing the light reflected from the cornea on photographic plate 
		  (Dodge &amp; </span>Cline, 1901). Some of these techniques were fairly 
		  invasive, requiring placement of a reflective white dot directly onto 
		  the eye of the viewer (Jud, McAllister &amp; Steel, 1905) or a tiny 
		  mirror, attached to the eye with a small suction cup (Yarbus, 1967). 
		  In the field of medicine a technique was developed (electro-oculography, 
		  still in use for certain diagnostic procedures) that allowed registering 
		  of eyeball movements using a number of electrodes positioned around 
		  the eye. Most of the described techniques required the viewer's head 
		  to be motionless during eye tracking and used a variety of devices like 
		  chin rests, head straps and bite-bars to constrain the head movements. 
		  The major innovation in eye tracking was the invention of a head-mounted 
		  eye tracker (Hartridge &amp; Thompson, 1948). With technological advances 
		  that reduced the weight and size of an eye tracker to that of a laptop 
		  computer, this technique is still widely used.</p>
		<p class=normal>Most eye tracking techniques developed before the 1970s 
		  were further constrained by the fact that data analysis was possible 
		  only <i>after</i> the act of viewing. It was the advent of mini- and 
		  microcomputers that made possible real-time eye tracking. Although widely 
		  used in studies of perceptual and cognitive processes, it was only with 
		  the proliferation of personal computers in the 1980s that eye tracking 
		  was applied as an instrument for the evaluation of human-computer interaction 
		  (Card, 1984). Around the same time, the first proposals for the use 
		  of eye tracking as a means for user-computer communication appeared, 
		  focusing mostly on users with special needs (Hutchinson, 1989; Levine, 
		  1981). Promoted by rapid technological advancements, this trend continued, 
		  and in the past decade a substantial amount of effort and money was 
		  devoted to the development of eye- and gaze-tracking mechanisms for 
		  human-computer interaction (Vertegaal, 1999;Jacob, 1991; Zhai, Morimoto 
		  &amp; Ihde, 1999). Detailed analysis of these studies is beyond the 
		  scope of this paper, and I will refer to them only insofar as they provide 
		  reference points to my proposed design. Interested readers are encouraged 
		  to consult several excellent publications that deal with the topic in 
		  much greater detail (Duchowsky, 2002; Jacob, Karn, 2003 /in press/).</p>
		<h2 class="SubHeader">Eye and Gaze Tracking in a Museum Context</h2>
		<p class=normal>The use of eye and gaze tracking in a museum context extends 
		  beyond interactions with the digital medium. Eye tracking data can prove 
		  to be extremely useful in revealing how humans observe real artifacts 
		  in a museum setting. The sample data and the methodology from a recent 
		  experiment conducted in the National Gallery in London (in conjunction 
		  with the Institute for Behavioural Studies) can be seen on the <a
href="http://ibs.derby.ac.uk/gallery/updates.shtml">Web</a>. Although some of 
		  my proposed gaze-based interaction solutions can be applied to the viewing 
		  of real artifacts (for example, to get more information about particular 
		  detail that a viewer is interested in), the main focus of my discussion 
		  will be on the development of affordable and intuitive gaze-based interaction 
		  mechanisms with(in) the digital medium. The main reason for this decision 
		  is the issue of accessibility to cultural heritage information. Although 
		  an impressive 4000 people participated in the National Gallery experiment, 
		  they <i>all had to be there</i> at certain time. I am not disputing 
		  the value of experiencing the real artifact, but the introduction of 
		  the digital medium has dramatically shifted the role of museums from 
		  <i>collection &amp; preservation</i> to <i>dissemination &amp; exploration</i>. 
		  Recent advancements in Web-based technologies make it possible for museums 
		  to develop tools (and social contexts) that allow them to serve as centers 
		  of knowledge transfer for both local and virtual communities. My proposal 
		  will focus on three issues: </p>
	  </div>
	  <ol>
		<li class="normal"> <span class="normal"> problems associated with use of gaze 
		  tracking data as interaction mechanism; </span></li>
		<li class="normal"> <span class="normal"> conceptual framework for the development 
		  of gaze-based interface; </span></li>
		<li class="normal"> <span class="normal">currently existing (and affordable) technologies 
		  that could support non-intrusive eye and gaze tracking in a museum context.</span></li>
	  </ol>
	  <div class=Section1> 
		<h2 class="SubHeader">Problems associated with gaze tracking input as 
		  an interaction mechanism</h2>
		<p class=normal>The main problem associated with use of eye movements 
		  and gaze direction as an interaction mechanism is known in the literature 
		  as "Midas touch" or "the clutch" problem (Jacob, 1993). In simple terms, 
		  the problem is that if looking at something should trigger an action, 
		  one would be triggering this action even by just observing a particular 
		  element on the display (or projection). The problem has been addressed 
		  numerous times in literature, and there are many proposed technical 
		  solutions. Detailed analysis and overview of these solutions is beyond 
		  the scope of this paper. I will present here only a few illustrative 
		  examples.</p>
		<p class=normal>One of the solutions to the <i>Midas Touch</i> problem, 
		  one developed by Ris&#248; National Research Laboratory, was to separate 
		  the gaze-responsive area from the observed object. The switch (aptly 
		  named EyeCon) is a square button placed next to the object that one 
		  wants to interact with. When the button is focused (ordinarily for half 
		  a second), it 'acknowledges' the viewer's intent to interact with 
		  an animated sequence depicting a gradually closing eye. The completely 
		  closed eye is equivalent to the pressing of a button (see Figure 3).</p>
		<p class=normal align="center"><img src="milekicFig03.jpg" width="379" height="124"></p>
		<p class=MsoCaption>Figure 3. An EyeCon activation sequence. Separating 
		  the control mechanism from interactive objects allows natural observation 
		  of the object (image reproduced from Glenstrup, A.J., Engell-Nielsen, 
		  T., 1995)</p>
		<p class=normal>One of the problems with this technique comes from the 
		  very solution -- it is the <i>separation of selection and action</i>. 
		  The other problem is the <i>interruption of the flow of interaction</i> 
		  &#8211; in order to select (interact with) an object, the user has to 
		  focus on the action button for a period of time. This undermines the 
		  unique quality of gaze direction as the fastest and natural way of pointing 
		  and selection (focus). </p>
		<p class=normal>Another solution to the same problem (with very promising 
		  results) was to provide the 'clutch' for interaction through another 
		  modality - voice (Glenn, Iavecchia, Ross, Stokes, Weiland, Weiss, Zakland 
		  1986) or manual (Zhai, Morimoto, Ihde 1999) input.</p>
		<p class=normal>The second major problem with eye movement input is the 
		  sheer volume of data collected during eye-tracking and its meaningful 
		  analysis. Since individual fixations carry very little meaning on their 
		  own, a wide range of eye tracking metrics has been developed in the 
		  past 50 years. An excellent and very detailed overview of these metrics 
		  can be found in Jacob (2003/in print). Here, I will mention only a few 
		  that may be used to infer viewer's interest or intent:</p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> <i>number of fixations</i>: 
		  a concentration of a large number of fixations in a certain area may 
		  be related to a user's interest in the object or detail presented in 
		  that area when viewing a scene (or a painting). Repeated, retrograde 
		  fixations on a certain word while reading text are taken to be indicators 
		  of increased processing load (Just, Carpenter 1976).</span></li>
		<li class="normal"> <span class="normal"> <i>gaze duration</i>: gaze is 
		  defined as a number of <i>consecutive</i> fixations in an area of interest. 
		  Gaze duration is the total of fixation durations in a particular area.</span></li>
		<li class="normal"> <span class="normal"> <i>number of gazes</i>: this 
		  is probably a more meaningful metric than the number of fixations. Combined 
		  with gaze duration, it may be indicative of a viewer's interest.</span></li>
		<li class="normal"> <span class="normal"> <i>scan path</i>: the scan path 
		  is a line connecting consecutive fixations (see Figure 2, for example). 
		  It can be revealing of a viewer's visual exploration strategies and 
		  is often very different in experts and novices. </span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>The problem of finding the right metric for interpretation 
		  of eye movements in a gallery/museum setting is more difficult than 
		  in a conventional research setting because of the complexity of the 
		  visual stimuli and the wide individual differences of users. However, 
		  the problem may be made easier to solve by dramatically constraining 
		  the number of interactions offered by a particular application and making 
		  them correspond to the user's expectations. For example, one of the 
		  applications of the interface I will propose is a simple gaze-based 
		  browsing mechanism that allows the viewer to quickly and effortlessly 
		  leaf through a museum collection (even if he/she is a quadriplegic and 
		  has retained only the ability to move the eyes).</p>
		<h1 class="SubHeader">Gaze-based interface for museum content</h1>
		<p class=normal>Needless to say, even a gaze-based interface that is specifically 
		  designed for museum use has to provide a solution for general problems 
		  associated with the use of eye movement-based interactions. I will approach 
		  this issue by analyzing three different strategies that may lead to 
		  the solution of the <i>Midas touch</i> problem. These strategies differ 
		  in terms of the of the interaction mechanism, as it relates to:</p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> time</span></li>
		<li class="normal"> <span class="normal"> location, and</span></li>
		<li>user action</li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>It is clear that <i>any</i> interaction involves time, 
		  space and actions, so the above classification should be taken to refer 
		  to the key component of the interface solution. Each of these solutions 
		  has to accommodate two modes of operation: </p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> the observation mode, and</span></li>
		<li class="normal"> <span class="normal"> the action (command) mode</span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>The viewer should have a clear indication as to which 
		  mode is currently active, and the interaction mechanism should provide 
		  a way to switch between the modes quickly and effortlessly. </p>
		<h2 class="SubHeaderSmall">Time-based interfaces</h2>
		<p class=normal> At first glance, a time-based interface seems like a 
		  good choice (evident even for myself when choosing the title of this 
		  paper). An ideal setup (for which I will provide more details in the 
		  following sections) for this type of interface would be a high-resolution 
		  <i>projection</i> of a painting on the screen with an eye-tracking system 
		  concealed in a small barrier in front of the user. An illustration of 
		  a time-based interaction mechanism is provided in Figure 4. The gaze 
		  location is indicated by a traditional cursor as long as it remains 
		  in a non-active (in this case, outside of the painting) area. When the 
		  user shifts the gaze to the gaze-sensitive object (painting), the cursor 
		  changes its shape to a faint circle, indicating that the observed object 
		  is aware of the user's <i>attention</i>. I have chosen the circle shape 
		  because it does not interfere with the viewer's observation, even though 
		  it clearly indicates potential interaction. As long as the viewer continues 
		  visual exploration of the painting there is no change in status. However, 
		  if the viewer decides to focus on a certain area for a predetermined 
		  period of time (600 ms), the cursor/circle starts to shrink (zoom), 
		  indicating the beginning of the <i>focusing</i> procedure. </p>
		<p class=normal align="center"><img src="milekicFig04small.jpg" width="397" height="321"></p>
		<p class=MsoCaption>Figure 4. The cursor changes at position (A) into 
		  focus area indicating that the object is 'hot'. </p>
		<p class=normal>Position (B) marks the period of relative immobility of 
		  the cursor and the beginning of the focusing procedure. Relative change 
		  in the size of the focus area (C) indicates that focusing is taking 
		  place. The appearance of concentric circles at time (D) indicates imminent 
		  action. The viewer can exit the focusing sequence at any time by moving 
		  the point of observation outside of the current focus area. </p>
		<p class=normal><span class="normal">If the viewer continues to fixate 
		  on the area of interest, the focusing procedure continues for the next 
		  400 milliseconds, ending with a 200 millisecond long signal of <i>imminent 
		  action</i>. At any time during the focusing sequence (including the 
		  imminent action signal), the viewer can return to observation mode by 
		  moving the gaze away from the current fixation point. In the scenario 
		  depicted above (and in general, for time-based interactions) it is desirable 
		  to have <i>only one</i> pre-specified action relevant to the context 
		  of viewing. For example, the action can be that of zooming-in to the 
		  observed detail of the painting (see Figure 6), or proceeding to the 
		  next item in the museum collection. The drawbacks of time-based interaction 
		  solutions triggered by focusing on th</span>e object/area of interest 
		  areas follows:</p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> the problem of <i>going back</i> 
		  to observation mode. This means that the action triggered by focusing 
		  on a certain area has to be either <i>self-terminating</i> (as is the 
		  case with the 'display the next artifact' action, where the application 
		  switches automatically back to the observation mode) ,or one has to 
		  provide a simple mechanism that would allow the viewer to return to 
		  the observation mode (for example, by moving the gaze focus outside 
		  of the object boundary);</span></li>
		<li class="normal"> <span class="normal"> the problem of choice between 
		  <i>multiple actions</i>. Using the time-based mechanism, it is possible 
		  to trigger different actions. By changing the cursor/focus shape, one 
		  can also indicate to the viewer which action is going to take place. 
		  However, since the actions are tied to the objects themselves, the viewer 
		  essentially has no choice but to accept the pre-specified action. This 
		  may not be a problem in a context where pre-specified actions are meaningful 
		  and correspond to the viewer's expectations. However, it does limit 
		  the number of actions one can 'pack' into an application and can create 
		  confusion in cases where two instances of focusing on the same object 
		  may trigger off different actions.</span></li>
		<li class="normal"> <span class="normal"> the problem of <i>interrupted 
		  flow</i> or <i>waiting</i>. Inherent to time-based solutions is the 
		  problem that the viewer always has to wait for an action to be executed. 
		  In my experience, after getting acquainted with the interaction mechanism, 
		  the waiting time becomes subjectively longer (because the users know 
		  what to expect) and often leads to frustration. The problem can be diminished 
		  to some extent by progressively shortening the duration of focusing 
		  necessary to trigger the action. However, at some point it can lead 
		  to another source of frustration since the viewer may be forced to constantly 
		  shift the gaze around in order to stay in the observation mode.</span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>Inspite of the above mentioned problems, time-based gaze 
		  interactions can be an effective solution for museum use where longer 
		  observation of an area of interest provides the viewer with more information. 
		  Another useful approach is to use the gaze direction as input for the 
		  delivery of additional information through another modality. In this 
		  case, the viewer does not need to get visual feedback related to his/her 
		  eye movements (which can be distracting on its own). Instead, focusing 
		  to an area of interest may trigger voice narration related to viewer's 
		  interest. For an example of this technique in the creation of a gaze-guided 
		  interactive narrative, see Starker &amp; Bolt (1990). </p>
		<h2 class="SubHeader">Location-based interfaces</h2>
		<p class=normal>Another traditional way of solving the "clutch" problem 
		  in gaze-based interfaces is by separating the modes of observation and 
		  action by using controls that are in the proximity of the area of interest 
		  but do not interfere with visual inspection. I have already described 
		  EyeCons (Figure 3) designed by the Ris&#248; National Research Laboratory 
		  in Denmark (for a detailed description see Glenstrup and Engell-Nielsen, 
		  1995). In the following section I will first expand on EyeCons design 
		  and then propose another location-based interaction mechanism. The first 
		  approach is illustrated in Figure 5. </p>
		<p class=normal align="center"><img src="milekicFig05small.jpg" width="398" height="304"></p>
		<p class=MsoCaption>Figure 5. Movement of the cursor (A) into the gaze-sensitive 
		  area (B) slides into view the action palette (C). </p>
		<p class=normalItalics>Fixating any of the buttons is equivalent to a 
		  button press and chooses the specified action which is executed without 
		  delay when the gaze returns to the object of interest. The viewer can 
		  also return to observation mode by choosing no action button. The action 
		  palette slides out of view as soon as the gaze moves out of the area 
		  (B).</p>
		<p class=normal>The observation area (the drawing) and the controls (buttons) 
		  are separated. At first glance, the design seems very similar to that 
		  of the EyeCons, but there are some enhancements that make the interactions 
		  more efficient. First, the controls (buttons) are located on a configurable 
		  'sliding palette', a mechanism that was adopted by the most widely used 
		  operating system (Windows) in order to provide users with more 'screen 
		  real estate'. The reason for doing this in a museum context is also 
		  to minimize the level of distraction while observing the artifact. Shifting 
		  the gaze to the side of the projection space (B) slides the action palette 
		  into the view. The button that is currently focused becomes <i>immediately</i> 
		  active (D) signaling the change of mode by displaying the focus ring 
		  and changing the color. This is a significant difference compared to 
		  the EyeCons design, which combines both location- and time-based mechanisms 
		  to initiate action. Moving the gaze back to the object leads to the 
		  execution of specified action (selection, moving, etc.). Figure 6 illustrates 
		  the outcome of choosing the 'zoom' action from the palette. The eye-guided 
		  cursor becomes a magnifying glass allowing close inspection of the artifact. 
		</p>
		<p class=normal align="center"><img src="milekicFig06.jpg" width="245" height="205"></p>
		<p class=MsoCaption>Figure 6. After choosing the desired action (see Figure 
		  5), returning the gaze to the object executes the action without delay. 
		  The detail above shows the 'zoom-in' tool, which becomes 'tied' to the 
		  viewer's gaze and allows close inspection of the artifact.</p>
		<p class=normal>One can conceptually expand location-based interactions 
		  by introducing the concept of an <i>active surface</i>. Buttons can 
		  be viewed as being essentially single-action locations (switches). It 
		  really does not matter which part of the button one is focusing on (or 
		  physically pressing) &#8211; the outcome is always the same. In contrast, 
		  a surface affords assigning meaning to a series of locations (fixations) 
		  and makes possible <i>incremental</i> manipulation of an object.</p>
		<p class=normal>Figure 7 provides an example of a surface-based interaction 
		  mechanism. Interactive surfaces are discretely marked on the area surrounding 
		  the object. For the purpose of illustration, a viewer's scan path (A) 
		  is shown superimposed over the object and indicates gaze movement towards 
		  the interactive surface. Entering the active area is marked by the appearance 
		  of a cursor in a shape that is indicative of the possible action (D). 
		  The appearance of the cursor is followed by a brief latency period (200-300 
		  ms) during which the viewer can return to the observation mode by moving 
		  the gaze outside of the active area. If the focus remains in the active 
		  area (see Figure 8), any movement of the cursor along the longest axis 
		  of the area will be <i>incrementally mapped</i> onto an action sequence 
		  &#8211; in this case, rotation of the object.</p>
		<p class=MsoCaption><img src="milekicFig07small.jpg" width="396" height="294"></p>
		<p class=MsoCaption>Figure 7. Surface-based interaction mechanism. Viewer's 
		  scanpath is visible at (A). Two interactive surfaces (B and C) are discretely 
		  marked on the projection. Moving the gaze into the area of interactive 
		  surface is marked by appearance of cursor with the shape indicative 
		  of possible action (D).</p>
		<p class=MsoCaption><img src="milekicFig08.jpg" width="469" height="349"></p>
		<p class=MsoCaption>Figure 8. If the viewer's gaze (as indicated by cursor 
		  position at A) remains within interactive surface (B), any gaze movement 
		  within the surface will lead to incremental action &#8211; in this case 
		  rotation of the object (C). </p>
		<p class=normal>The advantages of surface-based interaction mechanisms 
		  are the introduction of more complex, incremental action sequences into 
		  eye movement input and the possibility of rapid shifts between the observation 
		  and action modes. The drawback is that the number of actions is limited 
		  and that the surfaces, although visually non-intrusive, still claim 
		  a substantial portion of the display. </p>
		<h2 class="SubHeader">Action-based interfaces </h2>
		<p class=normal>Building on the previous two models, one can further expand 
		  the conceptual framework for gaze-based interfaces. This time I will 
		  focus on the <i>gaze action</i> as a mechanism for switching between 
		  the observation and the active (command) mode. Analysis of the previously 
		  described surface-based model reveals that it can be described as an 
		  intermediary step between the surface- and action-based interfaces. 
		  In this model, although the shift between the observation and action 
		  mode is dependent on <i>the location</i> of gaze focus, the control 
		  of interaction is based on gaze action (moving the focus/cursor over 
		  gaze-sensitive surface). Thus, the last step in our analysis is to explore 
		  the possibility of using predominantly <i>gaze-based actions</i> as 
		  a control mechanism. This may seem like slippery ground because physiologically 
		  our visual behavior is mostly geared towards collecting information 
		  and not acting upon the world. The exception of a kind is in the domain 
		  of sexual and social behaviors where gaze direction and duration may 
		  literally have physical consequences by signaling attraction, dominance, 
		  submissiveness, etc. Fine literature abounds with examples describing 
		  gazes as having a tangible effect ("his piercing gaze," "he felt her 
		  gaze boring two little holes at the back of his neck&#8230;," "her angry 
		  gaze was whipping across the room trying to find out who did this to 
		  her.." to mention a few). Our ability to transfer knowledge from one 
		  sensory domain to another modality will be the key component in the 
		  proposed outline of an action-based gaze interface.</p>
		<p class=normal>In eye-tracking literature, a gaze is most often defined 
		  as a number of consecutive fixations in a certain area. This metric 
		  emphasizes the <i>location</i> and the <i>duration</i> characteristics 
		  of the gaze and can be extremely useful in inferring the viewer's interest 
		  or gauging the complexity of the stimulus. However, in my proposal I 
		  would like to focus on two, often neglected, characteristics of a <i>moving 
		  gaze</i> that can be consciously used by a viewer to indicate his/her 
		  <i>intention</i>. These are:</p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> the direction of gaze movement, 
		  and</span></li>
		<li class="normal"> <span class="normal"> the speed of gaze movement.</span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>For technical purposes a moving gaze can be defined as 
		  a number of consecutive fixations progressing in the same direction. 
		  It corresponds roughly to longer, straight parts of a scan path and 
		  is occasionally referred to as a <i>sweep</i> (Altonen et al. 1998). 
		  The reason for choosing these characteristics is twofold. First, eyes 
		  can move much faster than the hand (and there is evidence from literature 
		  that eye-pointing is significantly faster than mouse pointing, see Sibert 
		  and Jacob 2000). Second, as mentioned before, directional gaze movement 
		  is often used in social communication. For example, we often indicate 
		  in a conversation exactly 'who' we are talking about by repeatedly shifting 
		  the gaze in the direction of the person in question. </p>
		<p class=normal>In order to create an efficient gaze-based interface, 
		  one has to be able to replicate the basic mouse-based actions used in 
		  the traditional graphical user interface (GUI). These are: <b>pointing</b> 
		  (cursor over), <b>selection</b> (mouse down), <b>dragging</b> (mouse 
		  down + move) and <b>dropping</b> (mouse up). I will also propose the 
		  inclusion of yet another non-traditional action, which I introduced 
		  in interface design a while ago (Milekic, 2000) and which proved to 
		  work extremely well as an intuitive browsing mechanism. This is the 
		  action of <i>throwing </i>which is dependent on the speed of movement 
		  of a selected object. Compared to the traditional interface, the throwing 
		  action is an expansion of the action of dragging an object. As long 
		  as the speed of dragging remains within a certain limit, one can move 
		  an object anywhere on the screen and drop it at desired location. However, 
		  if one 'flicks' the object in any direction, the object is released 
		  and literally 'flies away' (most often, to be replaced by another object). 
		  I have implemented this mechanism in a variety of mouse-, touchscreen- 
		  and gesture-based installations in museums and it has been successfully 
		  used by widely diverse audiences, including very young children. Subjectively, 
		  the action is very intuitive and natural, and the feeling can be best 
		  compared to that of sliding a glass on a polished surface (a skill that 
		  many bar tenders hone to perfection). In the following sections I will 
		  describe each of the gaze-based actions.</p>
		<p class=normal><i>Gaze-pointing </i>(Figure 9) is the easiest function 
		  to replicate in a gaze-based interface. It essentially consists of a 
		  visual clue that indicates to the viewer which area of the display is 
		  currently observed. Although one can use the traditional cursor for 
		  this purpose, it is desirable to design a cursor that will not interfere 
		  with observation. Dynamic change of cursor shape when moving over different 
		  objects can also be used to indicate whether an object is gaze-sensitive 
		  and to specify the type of action one can initiate (this technique is 
		  used in surface-based interface, described above; see Figure 4, for 
		  example). I have chosen a simple dashed circle as an indicator of the 
		  current gaze location. Pointing action is maintained as long as there 
		  are no sudden substantial changes in a specific gaze direction. If such 
		  a change occurs, the tracking algorithm determines the <i>direction</i> 
		  of gaze movement and, if necessary, initiates appropriate action. </p>
		<p class=normal align="center"><img src="milekicFig09.jpg" width="493" height="271"></p>
		<p class=MsoCaption>Figure 9. Gaze-pointing. The viewer can observe the 
		  artifact with the pointing cursor (dashed circle) indicating the current 
		  gaze location. Sweeping gazes across the scene are possible as long 
		  as they are not in upward direction and end in the 30&#176; angle strip.</p>
		<p class=normal>This does not mean that the viewer is limited to slow 
		  (and unnatural) observation. In fact, switching from observation to 
		  action mode (selection) occurs <i>only</i> if movement of sufficient 
		  amplitude occurs in an upward direction and ends up in a fairly narrow 
		  area spanning approximately 30&#176; above the current focus area. This means 
		  that viewers can, more or less, maintain a normal observation pattern, 
		  even if it includes sweeping gaze shifts, as long as they don't end 
		  up in the critical area.</p>
		<p class=normal><i>Gaze-selection</i> (Figure 10) is an action initiated 
		  by a sudden upward gaze shift. The action is best described (and subjectively 
		  feels like) the act of upward stabbing, or 'hooking' of the object. 
		  In a mouse-based interface the selection is a separate action &#8211; 
		  that is, one can just select an object, or select-drag-drop it somewhere 
		  else, or de-select it. In a gaze-based interface, what happens after 
		  the selection of an object will depend on the context of viewing. When 
		  multiple objects are displayed, the selection mechanism can act as a 
		  self-terminating action, making it possible for the viewer to select 
		  a subset of objects. In this case, highlighting the object would indicate 
		  the selection. However, in the museum context (assuming that the viewers 
		  will most often engage in observation of a single artifact) object selection 
		  may just be a prelude to the action of moving (dragging). In this case 
		  the object becomes, figuratively speaking, 'hooked' to the end of the 
		  viewer's gaze, as indicated by a change of the cursor's shape to that 
		  of a target. </p>
		<p class=normal align="center"><img src="milekicFig10small.jpg" width="398" height="218"></p>
		<p class=MsoCaption>Figure 10. Gaze-selection. Shifting the gaze rapidly 
		  upwards within the 30&#176; triggers of the selection process. The cursor 
		  changes the shape to that of a target and positions itself at the center 
		  of the object as a prelude to the action of gaze-dragging.</p>
		<p class=normal><i>Gaze-dragging</i> (Figure 12). Once the object has 
		  been selected ('hooked' to the viewer's gaze), it will follow the viewer's 
		  gaze until it is 'dropped' at another location. This action is meaningful 
		  in cases when the activity involves the repositioning of multiple objects 
		  (for example, assembling a puzzle). In the scenario depicted above, 
		  the viewer can 'throw away' the current object and get a new one.</p>
		<p class=normal align="center"><img src="milekicFig11small.jpg" width="395" height="231"></p>
		<p class=MsoCaption>Figure 11. Gaze-dragging. The painting is 'hooked' 
		  to viewer's gaze and follows its direction. At this stage the viewer 
		  can decide either to 'drop' the painting at another location (see Figure 
		  12) or, 'throw' away the current one and get a new artifact.</p>
		<p class=normal align="center"><img src="milekicFig12small.jpg" width="398" height="218"></p>
		<p class=MsoCaption>Figure 12. Gaze-dropping. The action of dropping an 
		  object is the opposite of 'hooking' it. A quick downward gaze movement 
		  releases the object and switches the application into observation mode.</p>
		<p class=normal><i>Gaze-throwing</i> (Figure 13) is a new interaction 
		  mechanism that allows efficient browsing of visual data bases with a 
		  variety of input devices, including gaze input. An object that has been 
		  previously selected ("hooked") will follow the viewer's gaze as long 
		  as the speed of movement does not exceed a certain threshold. A quick 
		  glance to the left or the right will release the object and it will 
		  'fly away' from the display to be replaced by a new artifact. </p>
		<p class=normal align="center"><img src="milekicFig13small.jpg" width="399" height="200"></p>
		<p class=MsoCaption>Figure 13. Gaze-throwing. 'Throwing' an object away 
		  is accomplished by moving the gaze rapidly to the left or to the right. 
		  Once the object reaches threshold speed it is released and 'flies away'. 
		  A new artifact floats to the center of display. </p>
		<p class=normal>The objects appear in a sequential order, so if a viewer 
		  accidentally throws an object away, it can be recovered by throwing 
		  the next object in the opposite direction.</p>
		<p class=normal>To summarize, action-based gaze input mechanisms have 
		  the advantage of allowing the viewer to <i>act upon the object</i> at 
		  will, without time or location constraints. The mechanism is simple 
		  and intuitive because it is analogous to natural actions in other modalities. 
		  The best way to think about action-based gaze input is as a kind of 
		  <i>eye-graffiti</i>. <i></i>The vocabulary of suggested gaze-gestures 
		  for eye input is presented in Figure 14. It is similar to the text input 
		  mechanism used for Palm personal organizers where the letters of the 
		  alphabet are reduced to corresponding simplified gestures. The fact 
		  that millions of users were able to adopt this quick and efficient text 
		  input mechanism is an indication that the development of <i>eye-graffiti</i> 
		  has significant potential for gaze based interfaces. </p>
		<p class=normal align="center"><img src="milekicFig14small.jpg" width="395" height="185"></p>
		<p class=MsoCaption>Figure 14. Eye-graffiti. Top row presents graffiti 
		  used for text input (letters A,B,C,D,E,F respectively) in Palm OS based 
		  personal organizers. Bottom row outlines suggested gaze-gestures that 
		  trigger different actions once the object has been selected. </p>
		<p class=normal>The dashed circle in the illustration above does not represent 
		  the visual representation of the cursor, but rather the area used to 
		  calculate the direction and the velocity of gaze movement by the tracking 
		  algorithm. The heavy dot indicates the starting point of a gesture. 
		  However, while action-based gaze input mechanism may seem best suited 
		  for museum applications, the ideal interface is probably a measured 
		  combination of all three approaches. </p>
		<h1 class="SubHeader">Current Technologies for Non-Intrusive Eye Tracking</h1>
		<p class=normal>Unlike in the laboratory environments, the eye-tracking 
		  technology used in a museum setting has to meet additional specific 
		  requirements. Some of the most obvious ones are:</p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> it should be non-intrusive. 
		  This excludes all eye-tracking devices that use goggles, head-straps, 
		  chin-rests or such.</span></li>
		<li class="normal"> <span class="normal"> it should allow natural head 
		  movements that occur during viewing.</span></li>
		<li class="normal"> <span class="normal"> it should not require individual 
		  calibration.</span></li>
		<li class="normal"> <span class="normal"> it should be able to perform 
		  with a wide variety of eye shapes, contact lenses or glasses.</span></li>
		<li class="normal"> <span class="normal"> it should be portable.</span></li>
		<li class="normal"> <span class="normal"> it should be affordable.</span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>With the ncreasing processor speeds of currently available 
		  personal computers, it seems that the most promising eye-tracking technology 
		  is that based on digital video analysis of eye movements. The most commonly 
		  used approach in video-based eye tracking is to calculate the angle 
		  of the visual axis (and the location of the fixation point on the display 
		  surface) by tracking the relative position of the pupil and a speck 
		  of light reflected from the cornea, technically known as the "glint" 
		  (see Figure 15). The accuracy of the system can be further enhanced 
		  by illuminating the eye(s) with low-level infra-red lightto produce 
		  the "bright pupil" effect and make the video image easier to process 
		  (B in Figure 15). Infrared light is harmless and invisible to the user.</p>
		<p class=normal align="center"><img src="milekicFig15.jpg" width="364" height="206"></p>
		<p class=MsoCaption>Figure 15. Gaze direction can be calculated by comparing 
		  the relative position and the relationship between the pupil (A) and 
		  corneal reflection &#8211; the glint (C). Infra-red illumination of 
		  the eye produces the 'bright pupil' effect (B) and makes the tracking 
		  easier.</p>
		<p class=normal align="center"><img src="milekicFig16.jpg" width="413" height="220"></p>
		<p class=MsoCaption>Figure 16. Several manufacturers produce portable 
		  eye-tracking systems similar to the one depicted above. While the camera 
		  position is most often bellow the eye level (eyelids interfere with 
		  tracking from above), the shape and position of infrared illuminators 
		  vary from manufacturer to manufacturer.</p>
		<p class=normal>A typical and portable eye-tracking system similar to 
		  the ones commercially available is depicted in Figure 16. Since the 
		  purpose of this paper is not to endorse any particular manufacturer, 
		  I urge interested readers to consult the large Eye Movement Equipment 
		  Database (<a
href="http://ibs.derby.ac.uk/emed/">EMED</a>) available on the World Wide Web. 
		  Keeping in mind that many museums and galleries have very modest budgets, 
		  I will specifically address the issue of <i>affordable</i> eye-tracking 
		  systems.</p>
		<p class=normal>The price range of most commercially available eye-trackers 
		  is between $5000 and $60.000, often with additional costs for custom 
		  software development, setup etc. Although there are some exceptions, 
		  the quality and the precision of the system tend to correlate with the 
		  price. However, with the increasing speed of computer processors, greater 
		  availability of cheap digital video cameras (like the ones used for 
		  Web-based video conferencing) and, most importantly, the development 
		  of sophisticated software for video signal analysis, it is becoming 
		  possible to build eye-trackers within a price range comparable to that 
		  of a new personal computer. Even though the cheaper systems have lower 
		  spatial and temporal resolution when compared to the research equipment, 
		  in a museum/gallery setting they may be used for different applications; 
		  for example, for browsing a museum collection with additional information 
		  provided by voice-overs. A more significant use would be providing access 
		  to the museum content to visitors with special needs. An example of 
		  a cost-effective solution based on a personal computer and a Web-cam 
		  for eye-gaze assistive technology was recently described (Corno, Farinetti 
		  and Signorile, 2002). </p>
		<p class=normal>Most commercially available eye-tracking systems (including 
		  the high-end ones) have two characteristics that make them less than 
		  ideal for use in museums. These are:</p>
	  </div>
	  <ul>
		<li class="normal"> <span class="normal"> the system has to be calibrated 
		  for each individual user</span></li>
		<li class="normal"> <span class="normal"> even remote eye-trackers have 
		  very low tolerance for head movements and require the viewer to hold 
		  the head unnaturally still, or to use external support like head- or 
		  chin-rests. </span></li>
	  </ul>
	  <div class=Section1> 
		<p class=normal>The solution lies in the development of software able 
		  to perform eye-tracking data analysis in more natural viewing circumstances. 
		  A recent report by Quiang and Zhiwei (2002) seems to be a step in the 
		  right direction. Instead of using conventional approaches to gaze calibration, 
		  they introduced a procedure based on neural networks that incorporates 
		  natural head movements into gaze estimation and eliminates the need 
		  for individual calibration. </p>
		<p class=normal>The emergence of eye-tracking technologies based on a 
		  personal computer equipped with a Web-cam and the development of software 
		  that allows gaze tracking in natural circumstances open up a whole new 
		  area for museum applications. The described technologies make Web-based 
		  delivery of gaze-sensitive applications possible. This not only presents 
		  an opportunity for a novel method of content delivery (and reaching 
		  different groups of users with special needs) but also offers an incredible 
		  possibility to collect, on a massive scale, data related to visual analysis 
		  of museum artifacts. However, a word of caution is in order here. One 
		  cannot overemphasize the importance of <i>context</i> in an eye-tracking 
		  application (or, for that matter, in any application). In an appropriate 
		  context, even a fairly simple setup can produce magical results, and 
		  the use of the most expensive equipment can lead to viewer frustration 
		  in a flawed application. </p>
		<h1 class="SubHeader">Conclusion</h1>
		<p class=normal>I have outlined a conceptual framework for the development 
		  of a gaze-based interface for use in a museum context. The major component 
		  of this interface is the introduction of <b><i>gaze gestures</i></b> 
		  as a mechanism for performing intentional actions on observed objects. 
		  In conjunction, an overview of suitable eye-tracking technologies was 
		  presented with an emphasis on low cost solutions. The proposed mechanism 
		  allows the development of novel and creative ways for content delivery 
		  both in a museum setting and via the World Wide Web. An important benefit 
		  of this approach is that it makes museum <i>content</i> (and not just 
		  the building or the restrooms) accessible to a wide variety of populations 
		  with special needs. It also offers the possibility of data-logging related 
		  to visual observation on a massive scale. These records can be used 
		  to further refine the content delivery mechanism and to promote our 
		  understanding of both the psychological and the neurophysiological underpinnings 
		  of our relationship with the Art.</p>
		<h2 class="SubHeaderSmall">References</h2>
		<p class=ReferencesText>Altonen, A., A.Hyrskykari, K. Raiha. (1998). 101 
		  Spots, or how do users read menus? in Proceedings of CHI 98 Human Factors 
		  in Computing Systems, ACM Press, pp 132-139.</p>
		<p class=ReferencesText>Bever, T.G., (1970). The cognitive basis for linguistic 
		  structure, in J.R. Hayes, ed., <i>Cognitive development of language</i>, 
		  Wiley, New York.</p>
		<p class=ReferencesText>Card, S.K. (1984). Visual search of computer command 
		  menus, in H. Bouma, D.G. Bouwhuis (Eds.) <i>Attention and Performance 
		  X, Control of Language Processes</i>, Hillsdale, NJ, LEA.</p>
		<p class=ReferencesText>Corno, F., L.Farinetti, I. Signorile (2002). A 
		  Cost-Effective Solution for Eye-Gaze Assistive Technology, ICME 2002: 
		  IEEE International Conference on Multimedia and Expo, Lausanne, Switzerland.</p>
		<p class=ReferencesText>Corkum V., Moore C. (1998). The origins of joint 
		  visual attention in infants. Developmental Psychology, 34, pp 28&#8211;38.</p>
		<p class=ReferencesText>Dodge, R., T.S. Cline (1901). The angle velocity 
		  of eye-movements. Psychological Review, 8, 145&#8211;57.</p>
		<p class=ReferencesText>Duchowski, A.T. (2002). Eye Tracking Methodology: 
		  Theory and Practice, Springer Verlag.</p>
		<p class=ReferencesText>Glenn III, F.A., H.P.Iavecchia, L.V.Ross, J.M. 
		  Stokes, W.J. Weiland, D. Weiss, A.L. Zakland, (1986). Eye-voice-controlled 
		  interface, Proceedings of the Human Factors Society, 322-326.</p>
		<p class=ReferencesText>Glenstrup, A.J., T. Engell-Nielsen, (1995). Eye 
		  Controlled Media: Present and Future State. Minor Subject Thesis, DIKU, 
		  University of Copenhagen, available at: <a
href="http://www.diku.dk/~panic/eyegaze/article.html#contents" target="_blank">http://www.diku.dk/~panic/eyegaze/article.html#contents</a>.</p>
		<p class=ReferencesText>Hartridge, H., L.C. Thompson, (1948). Methods 
		  of investigating eye movements, British Journal of Ophthalmology, 32, 
		  pp 581-591.</p>
		<p class=ReferencesText>Hutchinson, T.E., K.P. White, W.N. Martin, K.C. 
		  Reichert, L.A. Frey, (1989). Human-Computer Interaction Using Eye-Gaze 
		  Input, IEEE Transactions on Systems, Man, and Cybernetics, 19, pp 1527-1534.</p>
		<p class=ReferencesText>Jacob, R. J. K. (1993). Eye-movement-based human-computer 
		  interaction techniques: Toward non-command interfaces, in H. R. Hartson 
		  &amp; D. Hix, (eds.) <i>Advances in Human-Computer Interaction</i>, 
		  Vol. 4, pp 151-190, Ablex Publishing Corporation, Norwood, New Jersey.</p>
		<p class=ReferencesText>Jacob, R.J.K., K.S. Karn, (2003). Eye Tracking 
		  in Human-Computer Interaction and Usability Research: Ready to Deliver 
		  the Promises (Section Commentary), in <i>The Mind's Eyes: Cognitive 
		  and Applied Aspects of Eye Movements</i>, J. Hyona, R. Radach, H. Deubel 
		  (Eds.), Oxford, Elsevier Science (in press).</p>
		<p class=ReferencesText>Judd, C.H., C.N. McAllister, W.M. Steel, (1905). 
		  General introduction to a series of studies of eye movements by means 
		  of kinetoscopic photographs, in J.M. Baldwin, H.C. Warren &amp; C.H. 
		  Judd (Eds.) Psychological Review, Monograph Supplements, 7, pp1-16, 
		  The Review Publishing Company, Baltimore.</p>
		<p class=ReferencesText>Just, M.A., P.A. Carpenter, (1976). The role of 
		  eye-fixation research in cognitive psychology, Behavior Research Methods 
		  &amp; Instrumentation, 8, pp 139-143.</p>
		<p class=ReferencesText>Levine, J.L. (1981) An Eye-Controlled Computer, 
		  Research Report RC-8857, IBM Thomas J. Watson Research Center, Yorktown 
		  Heights, N.Y.</p>
		<p class=ReferencesText>Milekic, S. (2000) Designing Digital Environments 
		  for Art Education / Exploration, Journal of the American Society for 
		  Information Science, Vol. 51-1, 49-56, Wiley.</p>
		<p class=ReferencesText>Moore, C. (1999) Gaze following and the control 
		  of attention, in P. Rochat (ed.), <i>Early social cognition: understanding 
		  others in the first months of life</i>, pp 241&#8211;256. </p>
		<p class=ReferencesText>National Gallery, London "Telling Time Exhibition", 
		  Web source consulted 1/24/03, available at: <a href="http://ibs.derby.ac.uk/gallery/updates.shtml" target="_blank">http://ibs.derby.ac.uk/gallery/updates.shtml</a></p>
		<p class=ReferencesText>Scaife M., J. S. Bruner (1975). The capacity for 
		  joint visual attention in the infant. Nature, 253, pp 265&#8211;266.</p>
		<p class=ReferencesText>Sibert, L. E., R.J.K. Jacob (2000). Evaluation 
		  of Eye Gaze Interaction, Proceedings of the CHI 2000, ACM, New York, 
		  281-288, available at: <a
href="http://citeseer.nj.nec.com/article/sibert00evaluation.html" target="_blank">http://citeseer.nj.nec.com/article/sibert00evaluation.html</a></p>
		<p class=ReferencesText>Starker, I., R.A. Bolt, (1990). A gaze-responsive 
		  self-disclosing display, in <i>CHI '90 Proceedings</i>, ACM, pp. 3-9.</p>
		<p class=ReferencesText>Stewart, J., &amp; C. Logan, (1998). <i>Together: 
		  Communicating Interpersonally (5th Ed.)</i>. Boston: McGraw Hill, 90-100.</p>
		<p class=ReferencesText>Qiang, J., Z. Zhiwei, (2002). Eye and Gaze Tracking 
		  for Interactive Graphic Display, International Symposium on Smart Graphics, 
		  June 11-13, 2002, Hawthorne, NYVertegaal, R. (1999). The GAZE groupware 
		  system: mediating joint attention in multiparty communication and collaboration, 
		  in <i>Proceedings of the ACM CHI'99 Human Factors in Computing Systems</i>, 
		  ACM Press, New York, pp 294-301.</p>
		<p class=ReferencesText>Ware,C., H.H. Mikaelian, (1987). An evaluation 
		  of an eye tracker as a device for computer input, Proceedings of the 
		  CHI 1987, ACM, New York, 183-188.</p>
		<p class=ReferencesText>Yarbus, A.L. (1967) Eye movements during perception 
		  of complex objects, in L. A. Riggs, (ed.), <i>Eye Movements and Vision</i>, 
		  Plenum Press, New York, 171-196.</p>
		<p class=ReferencesText>Zhai, S., C. Morimoto, S. Ihde, (1999). Manual 
		  and Gaze Input Cascaded (MAGIC) Pointing, Proceedings of the CHI'87, 
		  ACM New York, 246-253.</p>
	  </div>
	  <!-- #EndEditable --></td>
  </tr>
</table>

<p class="smallPurple">&nbsp;</p>
<!--htdig_noindex-->

<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script> 
<script type="text/javascript" src="http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript></noscript>

<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>

<!--/htdig_noindex-->

</body>
<!-- #EndTemplate -->
<!-- Mirrored from www.museumsandtheweb.com/mw2003/papers/milekic/milekic.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 20:33:35 GMT -->
</html>
