<html><!-- #BeginTemplate "/Templates/mw2003-papers.dwt" --><!-- DW6 -->

<!-- Mirrored from www.museumsandtheweb.com/mw2003/papers/haley/haley.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:46:46 GMT -->
<head>
<!-- #BeginEditable "doctitle" --> 
<title>Museums and the Web 2003: Papers: Haley Goldman and Bendoly</title>
<!-- #EndEditable --> 
<meta name="keywords" content="Museums and the Web 2003, Archives & Museum Informatics, museums online, on-line, cultural heritage online, museum digitization, internet, conference, symposium, workshop, meeting, international, papers, presentations, multimedia, interactive, education, exhibits, evaluation, virtual reality, digitization, information architecture, information design, interface design, digital library, digital libraries ">
<!-- #BeginEditable "page keywords" --> 
<meta name="keywords" content="Evaluation, Heuristic Evaluation, Usability">
<!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=">
<!-- #BeginEditable "script" --><!-- #EndEditable --> 
<link rel="stylesheet" href="../../Library/mw2003.css" type="text/css">
<script language="JavaScript">
<!--
function MM_swapImgRestore() { //v3.0
  var i,x,a=document.MM_sr; for(i=0;a&&i<a.length&&(x=a[i])&&x.oSrc;i++) x.src=x.oSrc;
}

function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.0
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && document.getElementById) x=document.getElementById(n); return x;
}

function MM_swapImage() { //v3.0
  var i,j=0,x,a=MM_swapImage.arguments; document.MM_sr=new Array; for(i=0;i<(a.length-2);i+=3)
   if ((x=MM_findObj(a[i]))!=null){document.MM_sr[j++]=x; if(!x.oSrc) x.oSrc=x.src; x.src=a[i+2];}
}
//-->
</script>
</head>

<body bgcolor="#FFFFFF" background="../../../mw2002/images/mw2002.bg.gif" text="#000000" link="#660099" vlink="#000066" onLoad="MM_preloadImages('../../images/register_on.gif','../../images/workshops_on.gif','../../images/sessions_on.gif','../../images/speakers_on.gif','../../images/interact_on.gif','../../images/demos_on.gif','../../images/exhibits_on.gif','../../images/events_on.gif','../../images/best_on.gif','../../images/dates_on.gif','../../images/charlotte_on.gif','../../images/sponsors_on.gif')">
<table width="600" border="0" cellspacing="2" cellpadding="5">
  <tr> 
    <td width="145" align="LEFT" valign="TOP"> 
      <p><a href="../../index.html"><img src="../../images/mw.gif" width="112" height="155" border="0" alt="/mw/"></a></p>
      <p> <a href="../../register/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('register','','../../images/register_on.gif',1)"><img name="register" border="0" src="../../images/register_off.gif" width="112" height="18"></a><br>
        <a href="../../workshops/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('workshops','','../../images/workshops_on.gif',1)"><img name="workshops" border="0" src="../../images/workshops_off.gif" width="112" height="18"></a><br>
        <a href="../../sessions/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('sessions','','../../images/sessions_on.gif',1)"><img name="sessions" border="0" src="../../images/sessions_off.gif" width="112" height="18"></a><br>
        <a href="../../speakers/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('speakers','','../../images/speakers_on.gif',1)"><img name="speakers" border="0" src="../../images/speakers_off.gif" width="112" height="18"></a><br>
        <a href="../../interact/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('interactions','','../../images/interact_on.gif',1)"><img name="interactions" border="0" src="../../images/interact_off.gif" width="112" height="18"></a><br>
        <a href="../../demos/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('demonstrations','','../../images/demos_on.gif',1)"><img name="demonstrations" border="0" src="../../images/demos_off.gif" width="112" height="16"></a><br>
        <a href="../../exhibit/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('exhibits','','../../images/exhibits_on.gif',1)"><img name="exhibits" border="0" src="../../images/exhibits_off.gif" width="112" height="19"></a><br>
        <a href="../../events/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('events','','../../images/events_on.gif',1)"><img name="events" border="0" src="../../images/events_off.gif" width="112" height="18"></a><br>
        <a href="../../best/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('best','','../../images/best_on.gif',1)"><img name="best" border="0" src="../../images/best_off.gif" width="112" height="18"></a><br>
        <a href="../../dates/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('dates','','../../images/dates_on.gif',1)"><img name="dates" border="0" src="../../images/dates_off.gif" width="112" height="18"></a><br>
        <a href="../../charlotte/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('charlotte','','../../images/charlotte_on.gif',1)"><img name="charlotte" border="0" src="../../images/charlotte_off.gif" width="112" height="18"></a><br>
        <a href="../../sponsor/index.html" onMouseOut="MM_swapImgRestore()" onMouseOver="MM_swapImage('sponsors','','../../images/sponsors_on.gif',1)"><img name="sponsors" border="0" src="../../images/sponsors_off.gif" width="112" height="21"></a> 
        <br>
        <br>
        <a href="http://www.archimuse.com/" target="_top"><img src="../../images/nav_ami.gif" width="135" height="25" border="0" alt="A&amp;MI home"></a> 
        <br>
        <span class="small">Archives & Museum Informatics<br>
        158 Lee Avenue<br>
        Toronto Ontario<br>
        M4E 2P3 Canada</span></p>
      <p class="small">ph: +1 416-691-2516<br>
        fx: +1 416-352-6025</p>
      <p><span class="small">info @ archimuse.com</span><span class="small"><br>
		<a href="http://www.archimuse.com/">www.archimuse.com</a></span></p>
      <table width="74">
        <tr> 
          <td> <a href="http://search.museumsandtheweb.com/search" target="_top"> <img src="../../images/search.gif" width="24" height="25" alt="Search" border="0" name="Search"></a> 
          </td>
          <td valign="MIDDLE"> <a href="http://search.museumsandtheweb.com/search"> 
            <span class="verysmall">Search<br></span></a> </td>
        </tr>
      </table>
      <p><font face="Arial, Helvetica, sans-serif" class="verysmall"><span class="small">Join 
        our <a href="http://search.museumsandtheweb.com/mailinglist/"> Mailing List</a>. 
        <br>
        <a href="http://search.museumsandtheweb.com/terms-of-use-privacy/"> Privacy</a>.</span></font> 
      </p>
      <p>&nbsp; </p>
 <p><span class="verysmall">published: March 2004<br>
        analytic scripts updated:<br>
		  <!-- #BeginDate format:Am1 -->October 28, 2010<!-- #EndDate -->
        </span>
         </p>
     <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img src="http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0  License" width="88" height="31" style="border-width:0" /></a>
     </td>
    <td width="455" align="LEFT" valign="TOP" class="normal"><a href="../../speakers/index.html"><img src="../../images/PAPERS.gif" width="390" height="55" border="0" alt="Museums and the Web 2003 Papers"></a> 
	  <br>
	  <p>&nbsp;</p>
	 <!-- #BeginEditable "Body of Page" --> 
	  <div class=Section1> 
		<p class=PaperTitle>Investigating Heuristic Evaluation: A Case Study</p>
		<p class=Author>Kate Haley Goldman, Institute for Learning Innovation, 
		  and Laura Bendoly, Atlanta History Center, USA</p>
		<p class=URL><a href="http://%20www.ilinet.org/" target="_blank">http:// 
		  www.ilinet.org</a></p>
		<p class=AbstractTitle>Abstract</p>
		<p class=AbstractText>When museum professionals speak of evaluating a 
		  web site, they primarily mean formative evaluation, and by that they 
		  primarily mean testing the usability of the site. In the for-profit 
		  world, usability testing is a multi-million dollar industry, while in 
		  non-profits we often rely on far too few dollars to do too much. Hence, 
		  heuristic evaluation is one of the most popular methods of usability 
		  testing in museums. </p>
		<p class=AbstractText>Previous research has shown that the ideal usability 
		  evaluation is a mixed-methods approach, using both qualitative and quantitative, 
		  expert-focused and user-focused methods. But some within the online 
		  museum field have hypothesized that heuristic evaluation alone is sufficient 
		  to recognize most usability issues. To date there has been no studies 
		  on how reliable or valid heuristic evaluation is for museum web sites. 
		  This is critical if heuristic evaluation is to be used alone rather 
		  than in tandem with other methods.</p>
		<p class=AbstractText>This paper will focus on work being done at the 
		  Atlanta History Center as a case study for the effectiveness of heuristic 
		  evaluation in a museum web site setting. It is a project currently in 
		  the beginning stages of development. The Center is applying a thorough 
		  mixed-methods approach to evaluation, including heuristic evaluation. 
		  The results of this project will assess how complete and how useful 
		  a rigorous heuristic evaluation is alone and in conjunction with other 
		  methods in the development and implementation of an online educational 
		  resource.</p>
		<p class=keywords>Keywords<b>:</b>Evaluation, Heuristic Evaluation, Usability</p>
		<h1 class="SubHeader">The Atlanta History Center as a Case Study</h1>
		<p class=normal>The Atlanta History Center (AHC) has begun a three-year 
		  education outreach initiative funded by the Goizueta Foundation to enhance 
		  their existing outreach program and web site and help to develop a first-rate 
		  distance learning program. The site in question focuses on the online 
		  publication of educational materials and resources developed by the 
		  Center for a target population of educators in schools, both classroom 
		  teachers and media specialists. Since this population is narrowly defined, 
		  yet of prime importance to museums, the project makes an ideal forum 
		  for testing heuristic evaluation in a museum setting. The Institute 
		  for Learning Innovation has been serving as the evaluator for the Goizueta 
		  Foundation distance learning project. </p>
		<p class=normal>The project has three primary educational objectives:</p>
	  </div>
	  <ol>
		<li class="normal"> <span class = normal> Improve their existing web site 
		  and develop new content and features so that the site is more easily 
		  accessible to educators,</span> </li>
		<li class="normal"> <span class = normal> Ensure that the content better 
		  reflects Georgia's Quality Core Curriculum, and</span> </li>
		<li class="normal"> <span class = normal> Increase the number of teachers 
		  and students who use the web-based educational materials. </span> </li>
	  </ol>
	  <div class=Section1> 
		<p class=normal>A secondary objective, common to many museums, is to build 
		  such strong ties to the educational community that the number of school 
		  group visits to the physical site of the Atlanta History Center increases.</p>
		<p class=normal>While creating the study design for the above-mentioned 
		  project, we became concerned about one of the most commonly used techniques 
		  in web site evaluation, known as heuristic evaluation. Heuristic evaluation 
		  is a usability engineering methodology where experts who are trained 
		  in usability but who are not the end users of the proposed technology 
		  project compare the proposed technology against established usability 
		  principals known as heuristics. The training time for this technique 
		  is relatively short- as little as a half-day workshop, and the cost 
		  is often lower than other possible usability techniques. Due to this 
		  accessibility, heuristic evaluation has been frequently used by museums. 
		</p>
		<p class=normal>While there are many factors to consider when selecting 
		  a research methodology, such as cost, sample size, and personnel, it 
		  is assumed that the techniques used must be fundamentally sound. Heuristic 
		  evaluation has become hotly debated within the human-computer interaction 
		  field due to concerns about the reliability and validity of the results 
		  that it produces. Some specialists claim that heuristic evaluation both 
		  overlooks usability problems that may cripple the ability of a person 
		  to use the program in question, while highlighting issues that the user 
		  never encounters. Previous research, such as that by Harm and Schweibenz 
		  (2001), has shown that the ideal usability evaluation is a mixed-methods 
		  approach, using both qualitative and quantitative, expert-focused and 
		  user-focused methods. But some within the online museum field have hypothesized 
		  that heuristic evaluation alone is sufficient to recognize most usability 
		  issues. To date there have been no studies on how reliable or valid 
		  heuristic evaluation is for museum web sites. This is critical if heuristic 
		  evaluation is to be used alone rather than in tandem with other methods.</p>
		<p class=normal>Using the current project at the Atlanta History Center 
		  as a case study, we saw an opportunity to further investigate the issue 
		  of reliability and validity in using heuristic evaluation for museum 
		  web sites. This paper will outline our proposed techniques and current 
		  thinking; as the project develops we expect these techniques to evolve. 
		</p>
		<h1 class="SubHeader">Why evaluate at all?</h1>
		<p class=normal>As a point of reference, it is useful to step back from 
		  the AHC project and review the goals and methodologies of both traditional 
		  museum evaluation and the developing field of museum web site evaluation. 
		  Evaluation is used to urges us to clarify our goals and accomplish our 
		  objectives. If we are able to define what we intend to do, we are more 
		  likely to achieve our goals, increase the museum's responsiveness to 
		  the community, avoid false assumptions about our visitors, and save 
		  time and money. Evaluation can be scary, because a project with unclear 
		  objectives and no evaluation can always be described as successful. 
		  This is perhaps best stated by the Flying Karamazov Brothers who said, 
		  "If you don't know where you're going, any road will get you there."</p>
		<p class=normal>A quick review and comparison of traditional museum evaluation 
		  and museum web site evaluation is covered in Table 1. Audience research 
		  is done by some institutions on a regular cyclical basis, by some others 
		  who have done no other research and need a starting point or by those 
		  are beginning a new initiative or strategic plan. Audience research 
		  provides demographic information and other basic visitor information 
		  and is often done on the internet through log files analysis and surveys. 
		</p>
		<p class=normal>Traditional museum evaluation is made up of four types, 
		  not including the above mentioned audience research. Front-end evaluation 
		  typically occurs during the initial planning phase of project development 
		  and provides information about visitors' interest, expectations, and 
		  understanding of proposed topics for a program. Formative evaluation 
		  takes place while a project is in development and construction. It provides 
		  feedback on the effectiveness of a project, and its components -- feedback 
		  which allows developers to make informed decisions as they continue 
		  to build the project. Remedial evaluation is generally conducted after 
		  a project is available to public. This type of evaluation focuses on 
		  determining changes which need to be made to the program to improve 
		  it. Summative evaluation is conducted after an exhibit or program is 
		  completed, and it seeks to determine the extent to which exhibit or 
		  program goals were met.</p>
		<table border=1 cellspacing=0 cellpadding=3 align="center" width="85%">
		  <tr> 
			<td width=108 valign=top class="SubHeaderSmall"> 
			  <p class=normal><b>Museum Evaluation</b></p>
			</td>
			<td width=108 valign=top class="SubHeaderSmall"> 
			  <p class=normal><b>Museum Web Site Evaluation</b></p>
			</td>
			<td width=185 valign=top class="SubHeaderSmall"> 
			  <p class=normal><b>Web Site Methodologies</b></p>
			</td>
		  </tr>
		  <tr> 
			<td width=108 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Audience Research</p>
			</td>
			<td width=108 valign=top class="normal"> 
			  <p class=normal>Audience Research</p>
			</td>
			<td width=185 valign=top class="normal"> 
			  <p class=normal>Log Files Analysis, Surveys</p>
			</td>
		  </tr>
		  <tr> 
			<td width=108 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Front-End</p>
			</td>
			<td width=108 valign=top class="normal"> 
			  <p class=normal>Usability</p>
			</td>
			<td width=185 valign=top class="normal"> 
			  <p class=normal>User Testing and Think-aloud Protocols, Heuristic 
				Evaluation, Surveys</p>
			</td>
		  </tr>
		  <tr> 
			<td width=108 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Formative</p>
			</td>
			<td width=108 valign=top class="normal"> 
			  <p class=normal>Usability</p>
			</td>
			<td width=185 valign=top class="normal"> 
			  <p class=normal>User Testing and Think-aloud Protocols, Heuristic 
				Evaluation, Surveys</p>
			</td>
		  </tr>
		  <tr> 
			<td width=108 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Remedial</p>
			</td>
			<td width=108 valign=top class="normal"> 
			  <p class=normal>Usability</p>
			</td>
			<td width=185 valign=top class="normal"> 
			  <p class=normal>User Testing and Think-aloud Protocols, Heuristic 
				Evaluation, Surveys</p>
			</td>
		  </tr>
		  <tr> 
			<td width=108 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Summative</p>
			</td>
			<td width=108 valign=top class="normal"> 
			  <p class=normal>??????</p>
			</td>
			<td width=185 valign=top class="normal"> 
			  <p class=normal>??????</p>
			</td>
		  </tr>
		</table>
		<p class=MsoCaption>Table 1: Evaluation Types and Methods</p>
		<p class=normal>Usability testing is a standard piece of the larger development 
		  lifecycle throughout the technology industry and has been carried over 
		  into the field of museum technology development. Usability is currently 
		  the main focus for formative, remedial and even front-end evaluation. 
		  Although usability is extremely important and is the focus of this current 
		  project, the fact that a project or program is usable does not make 
		  it de facto valuable, or even used. The logistical and methodological 
		  difficulties of assessing the value of a project when the users are 
		  geographically scattered means that summative evaluation of museum web 
		  sites being rarely undertaken. </p>
		<h1 class="SubHeader">Background on Usability Engineering Techniques</h1>
		<p class=normal>The human-computer interaction field has developed a wide 
		  range of techniques to evaluate usability of technology projects. Techniques 
		  that are expert-based are known as usability inspection techniques. 
		  For-profit companies often choose expert-based methods over user-based 
		  methods because of the high costs of doing laboratory tests with end-users. 
		</p>
		<p class=normal>Heuristic evaluation is one of the most informal methods 
		  of usability inspection, meaning it is based on rules of thumb and the 
		  skills of the evaluators. In heuristic evaluation, the evaluators may 
		  be non-experts who have received some training in usability principles. 
		  Since this is a less formal method which avoids using a full set of 
		  controls or specified personnel lower costs are incur than in formal 
		  testing. To quote Mack and Neilsen, </p>
	  </div>
	  <blockquote> 
		<div class=Section1>Usability engineering activities are often difficult 
		  to justify and carry out in a timely way, but many activities can be 
		  done quickly and cheaply, and produce useful results. The methodology 
		  decision ...turn less on what is "correct" than on what can be done 
		  within development constraints. After all, with sufficient resources 
		  we would likely simply aim for rapid prototyping and end-user testing.<br>
		  <span class="BlockQuoteAuthor"><i>(Mack and Nielsen, 1994)</i></span></div>
	  </blockquote>
	  <div class=Section1> 
		<p class=normal>Although other usability inspection techniques are rarely 
		  used in the museum field, we will briefly describe them below in order 
		  to give a sense of what could be used or adapted as a technique for 
		  our field. The majority of these are designed for designers and developers 
		  in the formative development period of a project, rather than the front-end 
		  or remedial stage.</p>
		<h2 class="SubHeader">Possible Usability Inspection Methods:</h2>
		<h3 class="SubHeaderSmall">1. Guideline Review</h3>
		<p class=normal>Project is checked to determine conformity to a list of 
		  usabi<span class="normal">l</span>ity guidelines. Comprehensive sets 
		  can contain more than a thousand guidelines, and require skilled expertise. 
		  They are considered a mix of heuristic evaluation and standards inspections.</p>
		<h3 class="SubHeaderSmall">2. Standards Inspections</h3>
		<p class=normal>An expert in a particular type of interface inspects the 
		  product based on guidelines for that specific product range.</p>
		<h3 class="SubHeaderSmall">3. Cognitive Walkthroughs</h3>
		<p class=normal>Exploration focused inspection focused on one feature 
		  of usability- the ease of learning. This might be a useful goal for 
		  a complex software product, but for a public web-site a more common 
		  goal is ease of use. Ease of use would mean a first-time user could 
		  navigate and accomplish his or her objective easily, as opposed to finding 
		  it easy<span class="normal"> </span>to become an expert of a more complex 
		  system.</p>
		<h3 class="SubHeaderSmall">4. Pluralistic Walkthroughs</h3>
		<p class=normal>Group meetings with users, developers and human interaction 
		  personnel walk through user scenarios, documenting each step of the 
		  scenario and discussing implications.</p>
		<h3 class="SubHeaderSmall">5. Consistency Inspections</h3>
		<p class=normal>Inspections by designers and developers across multiple 
		  projects, ensuring that the projects have consistent design elements 
		  and usability. For instance, as multiple designers may work on separate 
		  functions of a museum web site, a consistency review would evaluate 
		  the congruity of the different sections or how well each section complies 
		  with ADA guidelines.</p>
		<h3 class="SubHeaderSmall">6. Formal Usability Inspections</h3>
		<p class=normal>Inspection method similar to software code inspections. 
		  designed to discover and report a large amount of data efficiently. 
		  Inspectors take on user roles and work through prescribed scenarios.</p>
		<h3 class="SubHeaderSmall">7. Feature Inspections </h3>
		<p class=normal>Focuses on whether the project functions as developed 
		  meet the needs of the intended end users. In traditional evaluation, 
		  this would be a part of summative evaluation. </p>
		<h1 class="SubHeader">Reliability and Validity Issues in Heuristic Evaluation</h1>
		<p class=normal>Reliability is the consistency or stability of a measure 
		  from one test to the next. Repeated measures of a static item using 
		  a reliable measure should end in identical or similar results. Validity 
		  is a term used to describe whether a measure accurately measures what 
		  it is supposed to measure. For instance, it is hotly debated whether 
		  SAT scores accurately assess college achievement. If SATs did accurately 
		  assess achievement, they would be a valid measure. </p>
		<p class=normal>Studies that bring the reliability of inspection methods 
		  include two studies by Rolf Molich. In the first study, he asked four 
		  commercial usability laboratories to carry out usability tests on a 
		  calendar program that was commercially available. One laboratory found 
		  as few as 4 problems, another found as many as 98. The biggest concern, 
		  however, is that only one problem was found by all four team and over 
		  90% of the problems found by each team were found by that team alone. 
		  The second follow-up study had similar results- there was little inter-rater 
		  reliability.</p>
		<p class=normal>The validity of usability inspection methods should be 
		  easier t<span class="normal">o</span> address- the pertinent question 
		  asks how predictive are these methods of end-user problems? Studies 
		  on that question have been completed outside of the museum field. Karat 
		  (1994) reports on the results of several such studies. A study by Desurvire 
		  (1994) compared heuristic evaluation and an automated cognitive walkthrough 
		  to laboratory tests with end users. The system in question was not a 
		  web site, but a telephone system that completed six basic tasks. Table 
		  2 below contrasts the results of the laboratory data with end users 
		  and the data collected using inspection methods.</p>
		<table border=1 cellspacing=0 cellpadding=3 align="center" width="85%">
		  <tr> 
			<td width=89 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Method</p>
			</td>
			<td width=119 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Evaluators</p>
			</td>
			<td width=72 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Problems That Did Occur</p>
			</td>
			<td width=72 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Potential Problems</p>
			</td>
			<td width=72 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Improve-ments</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 valign=top class="SubHeaderSmaller"> Lab </td>
			<td width=119 valign=top class="normal"> 
			  <p class=MsoFooter>Observed with users</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>25</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>29</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>31</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 rowspan=3 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Heuristic Evaluation</p>
			</td>
			<td width=119 valign=top class="normal"> 
			  <p class=MsoFooter>Experts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>44%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>31%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>77%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Software developers</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>16%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>24%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>3%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Nonexperts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>8%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>3%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>6%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 rowspan=3 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Cognitive Walkthrough</p>
			</td>
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Experts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>28%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>31%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>16%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Software developers</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>16%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>21%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>3%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Nonexperts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>8%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>7%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>6%</p>
			</td>
		  </tr>
		</table>
		<p class=MsoCaption>Table 2: Prediction Rate of End-User Problems </p>
		<p class=normalItalics>The top line in this table indicated the number 
		  of usability problems and interface improvement ideas that were observed 
		  during user testing in the laboratory. The remaining part of the table 
		  shows the percentage of these problems and improvement ideas found by 
		  the evaluators using either heuristic evaluation or cognitive walkthrough. 
		  (Source: Desurvire 1994)</p>
		<p class=normal>In the study above, experts were able to predict at best 
		  44 percent of the usability problems identified by the end users. The 
		  table above does not express variance in the problems that occur. Some 
		  problems users encounter are relatively minor and others prevent the 
		  user from completing major tasks. Desurvire dealt with this issues by 
		  asking each participant to assign Problem Severity Codes to the problems 
		  uncovered. The table displaying these results is reproduced below. Note 
		  that experts were able to detect 80% of the minor problems or annoyances 
		  but only 29% of the problems that caused task failure.</p>
		<table border=1 cellspacing=0 cellpadding=3 align="center" width="85%">
		  <tr> 
			<td width=89 valign=top class="SubHeaderSmall"> 
			  <p> 
			</td>
			<td width=119 valign=top class="SubHeaderSmall"> 
			  <p> 
			</td>
			<td width=216 colspan=3 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Problem Severity Code (PSC)</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Method</p>
			</td>
			<td width=119 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Evaluators</p>
			</td>
			<td width=72 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Minor Annoyance/ Confusion</p>
			</td>
			<td width=72 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center> Problem Caused Error</p>
			</td>
			<td width=72 valign=top class="SubHeaderSmall"> 
			  <p class=normal align=center>Problem Caused Task Failure</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 valign=top class="SubHeaderSmaller"> Lab </td>
			<td width=119 valign=top class="normal"> 
			  <p class=MsoFooter>Observed with users</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>5</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>3</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>17</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 rowspan=3 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Heuristic Evaluation</p>
			</td>
			<td width=119 valign=top class="normal"> 
			  <p class=MsoFooter>Experts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>80%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>67%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>29%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Software developers</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>40%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>0%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>12%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Nonexperts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>20%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>0%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>6%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=89 rowspan=3 valign=top class="SubHeaderSmaller"> 
			  <p class=normal>Cognitive Walkthrough</p>
			</td>
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Experts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>40%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>67%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>18%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Software developers</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>0%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>0%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>12%</p>
			</td>
		  </tr>
		  <tr> 
			<td width=119 valign=top class="normal"> 
			  <p class=normal>Nonexperts</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>20%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>0%</p>
			</td>
			<td width=72 valign=top class="normal"> 
			  <p class=normal align=center>6%</p>
			</td>
		  </tr>
		</table>
		<p class=MsoCaption>Table 3: Prediction Rate of End-User Problems by Severity 
		  of Problem</p>
		<p class=normalItalics>The Top line in this table indicated the number 
		  of usability problems in three severity categories that was observed 
		  during user testing in the laboratory. The remaining part of the table 
		  shows the percentage of the problems in each of the three categories 
		  found by evaluators using either heuristic evaluation or cognitive walkthrough. 
		  (Source: Desurvire 1994)</p>
		<p class=normal>These results raise serious questions about the validity 
		  of heuristic evaluation- about the ability of the technique to predict 
		  end-user errors. Missing any error that regularly leads to task failure 
		  is highly problematic. Worse yet, using heuristic evaluation as the 
		  sole usability technique would result in 70% of the errors that cause 
		  task failure going undetected in this example. In addition, many interface 
		  errors found by the experts using heuristic evaluation are false positives- 
		  meaning they find errors that don't actually impact the end-user, wasting 
		  development resources on what might not really be a problem.</p>
		<p class=normal>Still, these results were gathered by a system unlike 
		  tha<span class="normal">t</span> used to evaluate museum web site. Perhaps 
		  the nature of the medium (museum web sites) allows us to use heuristic 
		  evaluation to detect a higher rate of error. Our study aims to replicate 
		  this experiment with the AHC web site.</p>
		<h1 class="SubHeader">Research Design for AHC Project</h1>
		<p class=normal>In order to test the reliability of the heuristic evaluation 
		  methodology, we will use multiple methodologies, including both heuristic 
		  evaluation as well as user testing with think-aloud protocols. These 
		  two types of methodology are quite different. Think-alouds are a user-focused 
		  methodology where we ask the user to talk-aloud while interacting with 
		  the technology, therefore hopefully revealing the conscious cognitive 
		  processes of the user. With this technique, the interplay between thought 
		  and action is revealed by the user, rather than assumed by the researcher.</p>
		<p class=normal>Within usability engineering, an iterative design structure 
		  is critical, and the most complete designs incorporate a cyclical process 
		  of inspection methods and user testing at different point within the 
		  evaluation process. This allows a set of checks so that the solution 
		  to a interface problem does not create increased errors in other functions. 
		  For the purposes of this experiment, each technique will be performed 
		  on the exact same version of the web site. (In a typical design structure, 
		  end-user testing would occur after changes from the heuristic evaluation 
		  had already been incorporated into the web site.) For AHC project itself, 
		  there will be several iterations of evaluation that are not a part of 
		  this experiment. </p>
		<p class=normal>In each of the methodologies used, we will develop scenarios 
		  or tasks for the experts or end-users to complete. There are advantages 
		  and disadvantages to using the scenario approach. If carefully constructed, 
		  the scenarios can assist participants in focusing their efforts on specific 
		  interface elements. On the other hand facilitating a more open-ended 
		  inquiry will emulate the way most users experience a site- through intuitive 
		  exploration. Testers will usually then form their own scenarios with 
		  which to make sense of a site. Given that the AHC project is only one 
		  piece of a much larger site, we opt to control the scenarios. Complexity 
		  of the scenario can at times change the usability issues found, but 
		  as the interface here will be fairly straightforwardly task oriented 
		  we do not anticipate this to be a mitigating factor. </p>
		<p class=normal>Below we will lay out the specific processes for each 
		  methodology.</p>
		<h2 class="SubHeaderSmall">Heuristic evaluation</h2>
		<p class=normal>The first step in heuristic evaluation is to decide which 
		  set of heuristic principles to use. There are many different types of 
		  usability principles. Some of the standard ones were developed by Neilsen 
		  and others in the early 1990s. (See Tables 4 &amp; 5) By combining the 
		  principles from several different sets, we will develop a set of usability 
		  heuristics for the AHC project.</p>
		<table border=1 cellspacing=0 cellpadding=3 class="normal" align="center">
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Simple and natural dialogue</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Speak the users' language</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Minimize the users' memory load</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Consistency</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Feedback</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Clearly marked exits</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Shortcuts</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Precise and constructive error messages</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Prevent errors</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=normal>Help and documentation</p>
			</td>
		  </tr>
		</table>
		<p class=MsoCaption>Table 4: Example of Usability Principles by Molich 
		  and Neilsen (1990) </p>
		<p class=MsoCaption><b>&nbsp;</b></p>
		<table border=1 cellspacing=0 cellpadding=3 class="normal" align="center">
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Visibility of system status</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Match between system and the real world</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Use control and freedom</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Consistency and standards</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Error prevention</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Recognition rather than recall</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Flexibility and efficiency of use</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Aesthetic and minimalist design</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Help users recognize, diagnose and recover from 
				errors</p>
			</td>
		  </tr>
		  <tr> 
			<td width=252 valign=top> 
			  <p class=MsoNormal>Help and documentation</p>
			</td>
		  </tr>
		</table>
		<p class=MsoCaption>Table 5: Example of Usability principles by Neilsen 
		  (1994) </p>
		<p class=normal>For the actual process we will recruit 6 evaluators. Some 
		  studies show a benefit to evaluators working in teams, while other studies 
		  show a concern that teams "filter out" valid issues. To reap the most 
		  benefit, two evaluators will work together while the rest will work 
		  individually. Evaluators will be museum professionals who are unrelated 
		  to the project at the Atlanta History Center. In order to test the "quick-to-learn" 
		  claim of heuristic evaluation, we will not be usability experts. (There 
		  is no certification for the usability profession at this time. Within 
		  the field, the expert status normally is seen as obtained after 7 years 
		  in the field.)</p>
		<p class=normal>Since the evaluators will not be usability experts, but 
		  museum professionals, training will first be given on heuristic evaluation, 
		  including both the process and the specific principles for this evaluation. 
		  Evaluators will not be familiar with the system itself and may or may 
		  not be familiar with the proposed types of users (generally classroom 
		  teachers, but also possibly media coordinators and students), types 
		  of tasks that system users will be trying, and the contexts involved. 
		  Training will be provided to try to set the evaluator into the users' 
		  shoes. Evaluators will then be ask to imagine several scenarios while 
		  using the site. All scenarios will be described without screen-shots 
		  or specificities that would bias the evaluator in how they might approach 
		  the site. Evaluators will have an hour or more to complete the evaluation, 
		  and will be asked to resist discussing their results with others while 
		  moving through the scenarios. We will suggest that evaluators complete 
		  each scenario twice, once to gather a rough idea of the problems, and 
		  then revisit the scenario to link those problems specifically to the 
		  defined heuristic principles. Evaluators will be asked to describe in 
		  writing each of the specific issues that arise.</p>
		<p class=normal>After the formal evaluation, a debriefing session will 
		  be held to discuss the characteristics of the site, and identify any 
		  possible alternate approaches if critical issues arise. After the brainstorming 
		  session, evaluators will be asked to rate the severity of the problems 
		  they encounter. Severity rating assists developers to prioritize the 
		  changes needed in a project. </p>
		<p class=normal>Neilsen's severity rating is made up of three factors:</p>
	  </div>
	  <ol>
		<li class="normal"> <span class=normal>The frequency with which the problem 
		  occurs: Is it common or rare?</span> </li>
		<li class="normal"> <span class=normal>The impact of the problem if it 
		  occurs: Will it be easy or difficult for the users to overcome?</span> 
		</li>
		<li class="normal"> <span class=normal>The persistence of the problem: 
		  Is it a one-time problem that users can overcome once they know about 
		  it or will users repeatedly be bothered by the problem?</span> </li>
	  </ol>
	  <div class=Section1> 
		<p class=normal>Neilsen also mentions a fourth factor which he does not 
		  directly add to the others- one of market impact. He points out that 
		  certain types of usability problems can have a &#8216;devastating effect" 
		  on the usage of a project, even if the problem is supposedly easy to 
		  overcome. </p>
		<p class=normal>We will use an alternative system by Desurvire (1994) 
		  for severity ratings, which splits the ratings phase into two different 
		  three point scales. The first scale, the Problem Severity Code (PSC) 
		  rates the error severity as follows:</p>
	  </div>
	  <ol>
		<li class="normal"> 
		  <span class=normal>Minor annoyance or confusion</span>
		</li>
		<li class="normal"> 
		  <span class=normal>Problem caused error</span>
		</li>
		<li class="normal"> 
		  <span class=normal>Problem caused task failure</span>
		</li>
	  </ol>
	  <div class=Section1> 
		<p class=normal>The second scale measures the attitude of the user towards 
		  the system, an extremely important variable in the likelihood of a user 
		  to continue with a system once errors have occurred. The ratings for 
		  this scale are below:</p>
	  </div>
	  <ol>
		<li class="normal"> 
		  <span class=normal>Content with the system</span>
		</li>
		<li class="normal"> 
		  <span class=normal>Frustrated with the system</span>
		</li>
		<li class="normal"> 
		  <span class=normal>Exasperated with the system </span>
		</li>
	  </ol>
	  <div class=Section1> 
		<p class=normal>At times it is difficult to get useful severity estimates 
		  from evaluators during the actual session, when they are mostly focused 
		  on the finding of problems, rather than on the severity of the problem 
		  and how that particular problem impedes the overall purpose of the project. 
		  His suggestion is to ask the evaluators to revisit their list of problems 
		  after the debriefing session, despite the fact that the evaluators would 
		  generally not have access to the system in question. </p>
		<p class=normal>After gathering the severity ratings, we would do several 
		  tests of inter-rater reliability, including calculating the average 
		  correlation between the severity rating provided by any two evaluators, 
		  using Kendall's coefficient of concordance, and we would also 
		  estimate the reliability of the combined judgements by using the Spearman-Brown 
		  formula.</p>
		<h1 class="SubHeaderSmall">End-User Testing</h1>
		<p class=normal>To contrast with the Heuristic evaluation, we will also 
		  complete a round of user testing at the same point in the formative 
		  development process of the web site. We will attempt to have a minimum 
		  of 15-20 user-testing sessions. Unlike in the heuristic evaluation 
		  phase, users will work separately under the assumption that most end-users 
		  of the AHC site will be working on their own. Sessions will take place 
		  either in the History Center classrooms or within a usability laboratory. 
		  Users will be recruited through the large teacher network that has worked 
		  previously with the Atlanta History Center. </p>
		<p class=normal>Users will be given a series of tasks and asked to work 
		  through each of them while articulating their thoughts out loud in a 
		  stream-of consciousness fashion. As with the heuristic evaluation phase, 
		  users will interact directly with the interface. With each user will 
		  be an observer/facilitator who will record users' thoughts and actions 
		  as well as use appropriate prompts to probe for further information. 
		  Sessions will be audio taped and /or videotaped for further analysis. 
		</p>
		<h1 class="SubHeaderSmall">Analysis </h1>
		<p class=normal>During both phases of testing, data will be collected 
		  on variables task completion, error data, time to complete task, error 
		  severity, and user's attitude (the PSC and PAS scales mentioned 
		  above) based on the observation of and discussion with the end user. 
		  We will provide analysis similar to Desurvire's, doing a comparison 
		  of heuristic evaluation and end-user testing on each variable. We will 
		  also present analysis on which heuristics are cited most often. If 
		  possible, we will present a comparison on the use of evaluators individually 
		  and in teams. Finally, we will present recommendations for the use 
		  of heuristic evaluation to inspect museum web sites and suggestions 
		  for future research in this field.</p>
		<h2 class="SubHeaderSmall">References</h2>
		<p class=ReferencesText>Bailey, B. (2001) How reliable is usability performance 
		  testing? Last updated Sept 2001. Consulted August 27,2001. <a href="http://www.humanfactors.com/downloads/sep012.htm" target="_blank"> 
		 http://www.humanfactors.com/downloads/sep012.htm</a></p>
		<p class=ReferencesText>Desurvire, H. (1994). Faster, Cheaper!! Are Usability 
		  Inspection Methods as Effective as Empirical Testing?. In J. Nielsen 
		  and R. Mack (Ed.) <i>Usability Inspection Methods.</i> New York: Wiley 
		  &amp; Sons, Inc, 173-199</p>
		<p class=ReferencesText>Di Blas, N., Pai Guermand, M., &amp; P. Paolini 
		  (2002) Evaluating the Features of Museum Websites. In D. Bearman &amp; 
		  J. Trant (Eds.) <i>Museums and the Web 2002 Proceedings</i>. CD ROM. 
		  Archives &amp; Museum Informatics, 2002. <a href="http://www.archimuse.com/mw2002/papers/diblas/diblas.html" target="_blank">http://www.archimuse.com/mw2002/papers/diblas/diblas.html</a></p>
		<p class=ReferencesText>Harm, I. &amp; W. Schweibenz (2001) Evaluating 
		  the Usability of a Museum Web Site. In D. Bearman &amp; J. Trant (Eds.) 
		  <i>Museums and the Web 2001 Proceedings</i>. CD ROM. Archives &amp; 
		  Museum Informatics, 2001. <a href="http://www.archimuse.com/mw2001/papers/schweibenz/schweibenz.html">http://www.archimuse.com/mw2001/papers/schweibenz/schweibenz.html</a></p>
		<p class=ReferencesText>Karat, C., (1994). A Comparison of User Interface 
		  Evaluation Methods. In J. Nielsen and R. Mack (Ed.) <i>Usability Inspection 
		  Methods.</i> New York: Wiley &amp; Sons, Inc, 203-230</p>
		<p class=ReferencesText>Mack, R. &amp; J. Nielsen, (1994). Executive Summary. 
		  In J. Nielsen and R. Mack (Ed.) <i>Usability Inspection Methods.</i>. 
		  New York: Wiley &amp; Sons, Inc, 1-23</p>
		<p class=ReferencesText>Nielsen, J., (1994). Heuristic Evaluation. In 
		  J. Nielsen and R. Mack (Ed.) <i>Usability Inspection Methods.</i>. New 
		  York: Wiley &amp; Sons, Inc, 25-61</p>
		<p class=ReferencesText>Wharton, C., Rieman, J. Lewis, C. &amp;P. Polson, 
		  (1994). The Cognitive Walkthrough Method: A Practioner's Guide. 
		  In J. Nielsen and R. Mack (Ed.) <i>Usability Inspection Methods.</i>. 
		  New York: Wiley &amp; Sons, Inc, 105-139</p>
	  </div>
	  <!-- #EndEditable --></td>
  </tr>
</table>

<p class="smallPurple">&nbsp;</p>
<!--htdig_noindex-->

<!-- analytics scripts -->
<!-- tynt script -->
<script type="text/javascript">tyntVariables = {"ap":"Read more: "};</script> 
<script type="text/javascript" src="http://tcr.tynt.com/javascripts/Tracer.js?user=aTNeQ-tzOr36a6adbiUzgI&amp;s=130&amp;cc=6&amp;st=1"></script>
<noscript></noscript>

<!-- google analytics script -->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-26332456-1']);
  _gaq.push(['_setDomainName', '.archimuse.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--chartbeat script -->
<script type="text/javascript">
var _sf_async_config={uid:3385,domain:"archimuse.com"};
(function(){
  function loadChartbeat() {
    window._sf_endpt=(new Date()).getTime();
    var e = document.createElement('script');
    e.setAttribute('language', 'javascript');
    e.setAttribute('type', 'text/javascript');
    e.setAttribute('src',
       (("https:" == document.location.protocol) ? "https://s3.amazonaws.com/" : "http://") +
       "static.chartbeat.com/js/chartbeat.js");
    document.body.appendChild(e);
  }
  var oldonload = window.onload;
  window.onload = (typeof window.onload != 'function') ?
     loadChartbeat : function() { oldonload(); loadChartbeat(); };
})();

</script>

<!--/htdig_noindex-->

</body>
<!-- #EndTemplate -->
<!-- Mirrored from www.museumsandtheweb.com/mw2003/papers/haley/haley.html by HTTrack Website Copier/3.x [XR&CO'2008], Mon, 22 Jul 2013 17:46:46 GMT -->
</html>
