<div id="main-content">
          
                      <div id="breadcrumb">
              <h2 class="element-invisible">You are here:</h2>
              <a href="../../mw2011.html">MW2011</a> » <a href="../sessions.html">Sessions</a> » <a href="../programs/cultural_data_sculpting_omni_spatial_interac.html">Cultural Data Sculpting: Omni-spatial interactive visualization for large scale heterogeneous datasets</a> »             </div> <!-- /breadcrumb -->
              
                      <div id="main-content-header" class="clear-block">
              <h1 id="page-title">Cultural Data Sculpting: Omni-spatial Visualization for Large Scale Heterogeneous Datasets</h1>                          </div>
          
          
          <div id="content"><div id="article-14333" class="article article-promoted paper-article clear-block">

    
    <div class="article-detail clear-block">
  <div class="node-edit-link" id="node-edit-link-14333"><ul class="links"><li class="0 first last active"><a href="cultural_data_sculpting_omni_spatial_visualiza.html" class="active">[View]</a></li>
</ul>
</div><div id="intro-paragraph">
<h2 class="Author">Sarah Kenderdine, Applied Laboratory of Interactive Visualization and Embodiment, CityU Hong Kong, Special Projects, Museum Victoria, Australia; and Tim Hart, Information, Multimedia and Technology, Museum Victoria, Australia</h2>
<h4 class="AbstractTitle">Abstract</h4>
<p class="AbstractText">The rapid growth in participant culture (embodied by Web 2.0) has seen creative production overtake data access as the primary motive for interaction with museum databases and online collections. The meaning of diverse bodies of data is increasingly expressed through the user&#39;s creative exploration and re-application of data, rather than through the simple access to information, and presents the museum with theoretical, experimental and technological challenges. In addition, the opportunities offered by interactive 3D technologies in combination with visual analytics for enhanced cognitive exploration and interrogation of rich multimedia data still need to be realized within the domain of the digital humanities.</p>
<p class="AbstractText">This paper presents four research projects currently underway to develop new omni-spatial visualization strategies.</p>
<p class="AbstractText">Keywords: 3D, immersive, information visualization, interactive narrative, museum collection, archaeology, corpora,</p>
</div>
<p></p>
<div id="body-text">
<h2>1. Introduction</h2>
<p>This paper presents four research projects currently underway to develop new omni-spatial visualization strategies for the collaborative interrogation of large-scale heterogeneous cultural datasets using the worlds&#39; first 360-degree stereoscopic visualization environment (Advanced Visualization and Interaction Environment – AVIE). The AVIE system enables visualization modalities through full body immersion, stereoscopy, spatialized sound and camera-based tracking. The research integrates groundbreaking work by a renowned group of international investigators in virtual environment design, immersive interactivity, information visualization, museology, visual analytics and computational linguistics. The work is being implemented at the newly established world-leading research facility, City University&#39;s Applied Laboratory for Interactive Visualization and Embodiment – ALIVE) in association with partners Museum Victoria (Melbourne), iCinema Centre, UNSW (Sydney), ZKM Centre for Art and Media (Karlsruhe), UC Berkeley (USA), UC Merced (USA) and the Dharma Drum Buddhist College (Taiwan). The applications are intended for museum visitors and for humanities researchers. They are: 1) <em>Data Sculpture Museum</em>; 2) <em>Rhizome of the Western Han</em>; 3) <em>Blue Dots</em> (Tripitaka Koreana) and, 4) the <em>Social Networks of Eminent Buddhists</em> (Biographies from Gaoseng Zhuan). The research establishes new paradigms for interaction with future web-based content as situated embodied experiences.</p>
<p>The rapid growth in participant culture embodied by Web2.0 has seen creative production overtake basic access as the primary motive for interaction with databases, archives and search engines (Manovich 2008). The meaning of diverse bodies of data is increasingly expressed through the user&#39;s intuitive exploration and re-application of that data, rather than simply access to information (NSF 2007). This demand for creative engagement poses significant experimental and theoretical challenges for the memory institutions and the storehouse of cultural archives (Del Favero et al. 2009). The structural model that has emerged from the Internet exemplifies a database paradigm where accessibility and engagement is constrained to point and click techniques where each link is the node of interactivity. Indeed, the possibility of more expressive potentials for interactivity and alternative modalities for exploring and representing data has been passed by, largely ignored (Kenderdine 2010). In considering alternatives, this paper explores situated experiments emerging from the expanded cinematic that articulate for cultural archives a reformulation of database interaction, narrative recombination and analytic visualization.</p>
<p>The challenges of what can be described as cultural data sculpting following from Zhao &amp; Van Moere&#39;s &#39;data sculpting&#39; (2008) are currently being explored at a new research facility, the Applied Laboratory of Interactive Visualization and Embodiment (ALiVE), co-directed by Dr. Sarah Kenderdine and Prof. Jeffrey Shaw (<a href="http://www.cityu.edu.hk/alive">http://www.cityu.edu.hk/alive</a>). It has been established under the auspices of CityU Hong Kong and is located at the Hong Kong Science Park (http://www.hkstp.org). ALiVE builds on creative innovations that have been made over the last ten years at the UNSW iCinema Research Centre, at the ZKM Centre for Art and Media, and at Museum Victoria. ALiVE has a transdisciplinary research strategy. Core themes include aesthetics of interactive narrative; kinaesthetic and enactive dimensions of tangible and intangible cultural heritage; visual analytics for mass heterogeneous datasets; and immersive multi-player exertion-based gaming. Embedded in the title of ALiVE are the physicality of data and the importance of sensory and phenomenological engagement in an <em>irreducible ensemble</em> with the world.</p>
<p>Laboratories such as ALiVE can act as nodes of the cultural imaginary of our time. Throughout the arts and sciences, new media technologies are allowing practitioners the opportunity for cultural innovation and knowledge transformation. As media archaeologist Siegfried Zielinski says, the challenge for contemporary practitioners engaged with these technologies is not to produce &#34;more cleverly packaged information of what we know already, what fills us with ennui, or tries to harmonize what is not yet harmonious&#34; (Zielinski 2006, p.280). Instead, Zielinski celebrates those who, inside the laboratories of current media praxis, &#34;understand the invitation to experiment and to continue working on the impossibility of the perfect interface&#34; (Zielinski 2006, p.259). To research the &#39;perfect interface&#39; is to work across heterogeneity and to encourage &#34;dramaturgies of difference&#34; (Zielinski 2006, p.259). Zielinski also calls for ruptures in the bureaucratization of the creative and cultural industries of which museums are key stakeholders.</p>
<p>At the intersections of culture and new technologies, Zielinski observes that the media interface is both: &#34;…poetry and techne capable of rendering accessible expressions of being in the world, oscillating between formalization and computation, and intuition and imagination&#34; (Zielinski 2006, p.277). And as philosopher Gaston Bachelard describes in <em>L&#39;Invitation au Voyage</em>, imagination is not simply about the forming of images; rather:</p>
<blockquote>
<p>… the faculty of <em>deforming</em> the images, of freeing ourselves from the immediate images; it is especially the faculty of <em>changing</em> images. If there is not a changing of images, an unexpected union of images, there is no imagination, no <em>imaginative action</em>. If a <em>present</em> image does not recall an <em>absent</em> one, if an occasional image does not give rise to a swarm of deviant images, to an explosion of images, there is no imagination… The fundamental work corresponding to imagination is not <em>image</em> but the <em>imaginary</em>. The value of an image is measured by the extent of its <em>imaginary</em> radiance or luminance. (Bachelard 1971/1988, p.19)</p>
</blockquote>
<p>This paper documents the challenge of designing forms of new cultural expression in the cultural imaginary using databases of cultural materials. It also investigates new forms of scholarly access to large-scale heterogeneous data. It describes this research as it has been realized in the world&#39;s first 360-degree interactive stereographic interface, the Applied Visualization Interaction Environment (AVIE). The four experimental projects included in this paper draw upon disciplines such as multimedia analysis, visual analytics, interaction design, embodied cognition, stereographics and immersive display systems, computer graphics, semantics and intelligent search and, computational linguistics. The research also investigates media histories, recombinatory narrative, new media aesthetics, socialization and presence in situated virtual environments, and the potential for a psycho geography of data terrains. The datasets used in these four works are:</p>
<p>1. <em>Data Sculpture Museum</em>: over 100,000 multimedia rich heterogeneous museological collections covering arts and sciences derived from the collections of Museum Victoria, Melbourne and ZKM Centre for Art and Media, Karlsruhe. For general public use in a museum contexts.</p>
<p>2. <em>Rhizome of the Western Han</em>: laser-scan archaeological datasets from two tombs and archaeological collections of the Western Han, Xian, China culminating in a metabrowser and interpretive cybermap. For general public use in a museum contexts.</p>
<p>3. <em>Blue Dots</em>: Chinese Buddhist Canon, Koryo version (Tripitaka Koreana) in classical Chinese, the largest single corpus with 52 million glyphs carved on 83,000 printing blocks in 13th century Korea. The digitized Canon contains metadata that link to geospatial positions, to contextual images of locations referenced in the text, and to the original rubbings of the wooden blocks. Each character has been abstracted to a &#39;blue dot&#39; to enable rapid search and pattern visualization. For scholarly use and interrogation.</p>
<p>4. <em>Social Networks of Eminent Chinese Buddhists</em>: in which the visualization is based on the Gaoseng zhuang corpus. For scholarly use and interrogation.</p>
<p>To contextualize these projects, this paper begins by briefly introducing the rationale for the use of large-scale immersive display systems for data visualization, together with a description of the AVIE display system. Several related research demonstrators are described as background for the aforementioned projects currently underway at ALiVE. The paper also includes brief accounts of multimedia analysis, visual analytics and text visualization as emerging and fast developing fields applied to large-scale datasets and heterogeneous media formats, as techniques applicable to cultural datasets.</p>
<h2>2. Culture@Large</h2>
<h3>Advanced Visualization and Interaction Environment</h3>
<p>Applied Visualization Interaction Environment (AVIE) is the UNSW iCinema Research Centre&#39;s landmark 360-degree stereoscopic interactive visualization environment. The base configuration is a cylindrical projection screen 4 meters high and 10 meters in diameter, a 12-channel stereoscopic projection system and a 14.2 surround sound audio system. AVIE&#39;s immersive mixed reality capability articulates an embodied interactive relationship between the viewers and the projected information spaces (Figure 1). It uses active stereo projection solution and camera tracking. (For a full technical description, see McGinity et al. 2007).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_s.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig1.jpg" border="0" alt="Fig 1: Advanced Visualization and Interaction Environment (AVIE) © UNSW iCinema Research Centre. Image: ALiVE 2010" title="Fig 1: Advanced Visualization and Interaction Environment (AVIE) © UNSW iCinema Research Centre. Image: ALiVE 2010" width="500" height="500"/></a><span class="caption"><strong>Fig 1: Advanced Visualization and Interaction Environment (AVIE) © UNSW iCinema Research Centre. Image: ALiVE 2010</strong></span></span></p>
<h3>Embodied Experiences</h3>
<p>The research projects under discussion in this paper are predicated on the future use of real-time Internet delivered datasets in large-scale virtual environments, facilitating new modalities for interactive narrative and new forms of scholarship. The situated spaces of immersive systems provide a counterpoint to the small-scale desktop delivery and distributed consumption of internet-deployed cultural content. New museology has laid the foundations for many of the museums we see today as &#39;zones of contact&#39;, places of &#39;civic seeing&#39; (Bennett 2006, pp. 263-281), and engagements with poly-narratives and dialogic experience. The immersive display system AVIE encourages physical proximity, allowing new narrative paradigms to emerge from interactivity and physical relationships. Situated installations allow for human-to-human collaborative engagements in the interrogation of cultural material and mediations of virtual worlds. The physical proximity of the participants has a significant influence on the experiences of such installations (Kenderdine &amp; Schettino 2010, Kenderdine &amp; Shaw 2009, Kenderdine et al. 2009).</p>
<p>For online activity, shared socialization of searching cultural data such as social media involving non-specialist tagging of cultural collection data (Chan 2007; Springer et al. 2008) is still largely contained within a 2D flat computer screen. In the case of social tagging, the impersonal, invisible characteristics of the medium deny the inhibiting aspect of physical distance. Studies of human experiences demonstrate that perception implies action, or rather interaction, between subjects (Maturana &amp; Varela 1980). Knowledge is a process that is inherently interactive, communicative and social (Manghi 2004). Elena Bonini, researcher in digital cultural heritage, describes the process:</p>
<p class="BlockQuote">All art works or archaeological findings are not only poetic, historical and social expressions of some peculiar human contexts in time and space, but are also [the] world&#39;s founding acts. Every work of art represents an original, novel re-organization of the worlds or a Weltanschauung… aesthetic experience creates worlds of meaning (Bonini 2008, p.115).</p>
<p>Research into new modalities of visualizing data is essential for a world producing and consuming digital data at unprecedented rates (Keim et al., 2006; McCandless, 2010). Existing techniques for interaction design in visual analytics rely upon visual metaphors developed more than a decade ago (Keim et al. 2008), such as dynamic graphs, charts, maps, and plots. Currently, interactive, immersive and collaborative techniques to explore large-scale datasets lack adequate experimental development essential to the construction of knowledge in analytic discourse (Pike et al. 2009). Recent visualization research remains constrained to 2D small-screen-based analysis and advances interactive techniques of &#34;clicking&#34;, &#34;dragging&#34; and &#34;rotating&#34; (Lee et al. 2009, Speer et al. 2010, p.9). Furthermore, the number of pixels available to the user remains a critical limiting factor in human cognition of data visualizations (Kasik et al., 2009). The increasing trend towards research requiring &#39;unlimited&#39; screen resolution has resulted in the recent growth of gigapixel displays. <span class="text">Visualization systems for large-scale data sets are increasingly focused on effectively representing their many levels of complexity. This includes tiled displays such as HIPerSpace at Calit2 (<a href="http://vis.ucsd.edu/mediawiki/index.php/Research_Projects:_HIPerSpace">http://vis.ucsd.edu/mediawiki/index.php/Research_Projects:_HIPerSpace</a>) and, next generation immersive virtual reality systems such as StarCAVE (UC San Diego, De Fanti et al. 2009) and Allosphere at UC Santa Barbara (</span><a href="http://www.allosphere.ucsb.edu/">http://www.allosphere.ucsb.edu/</a><span class="text">).</span></p>
<p><span class="text">In general, however,</span> the opportunities offered by interactive and 3D technologies for enhanced cognitive exploration and interrogation of high dimensional data still need to be realized within the domain of visual analytics for digital humanities (Kenderdine, 2010). The projects described in this paper take on these core challenges of visual analytics inside AVIE to provide powerful modalities for an omni-directional (3D, 360-degree) exploration of multiple heterogeneous datasets responding to the need for embodied interaction; knowledge-based interfaces, collaboration, cognition and perception (as identified in Pike et al., 2009). A framework for &#39;enhanced human higher cognition&#39; (Green, Ribarsky &amp; Fisher 2008) is being developed that extends familiar perceptual models common in visual analytics to facilitate the flow of human reasoning. Immersion in three-dimensionality representing infinite data space is recognized as a pre-requisite for higher consciousness, autopoesis (Maturana &amp; Varela, 1980) and promotes non-vertical and lateral thinking (see Nechvatal, 2009). Thus, a combination of algorithmic and human mixed-initiative interaction in an omni-spatial environment lies at the core of the collaborative knowledge creation model proposed.</p>
<p>The four projects discussed also leverage the potential inherent in a combination of &#39;unlimited screen real-estate&#39;, ultra-high stereoscopic resolution and 360-degree immersion to resolve problems of data occlusion and distribute the mass of data analysis in networked sequences revealing patterns, hierarchies and interconnectedness. The omni-directional interface prioritizes &#39;users in the loop&#39; in an egocentric model (Kasik, et al. 2009). The projects also expose what it means to have embodied spherical (allocentric) relations to the respective datasets. These hybrid approaches to data representation also allow for the development of sonification strategies to help augment the interpretation of the results. The tactility of data is enhanced in 3D and embodied spaces by attaching audio to its abstract visual elements and has been well defined by researchers since Chion and others (1994). Sonification reinforces spatial and temporal relationships between data (e.g. the object&#39;s location in 360-degrees/infinite 3D space and its interactive behavior; for example, see West et al., 2008). The multi-channel spatial array of the AVIE platform offers opportunities for creating a real-time sonic engine designed specifically to enhance cognitive and perceptual interaction, and immersion in 3D. It also can play a significant role in narrative coherence across the network of relationships evidenced in the datasets.</p>
<h2>3. Techniques for Data Analysis and Visualization</h2>
<h3>Multimedia Analysis</h3>
<p>This short section introduces the intersection of key disciplines related to the projects in this paper. Multimedia analysis in the recent past has generally focused on video, images, and, to some extent, audio. An excellent review of the state of the art appeared in <em>IEEE Computer Graphics and Applications</em> (Chinchor et al. 2010). Multimedia Information Retrieval is at the heart of computer vision, and as early as the 1980s, image analysis included things such as edge finding, boundary and curve detection, region growing, shape identification, feature extraction and so on. Content based information retrieval in the 1990s for images and video became a prolific area of research, directed mainly at scholarly use. It was with the mass growth of Internet based multimedia for general public use that the research began to focus onf human-centric tools for content analysis and retrieval (Chinchor et al. 2010, p.52). From the mid-1990s, image and video analysis included colour, texture, shape and spatial similarities. Video parsing topics include segmentation, object motion analysis framing, and scene analysis. Video abstraction techniques include skimming, key frame extraction, content-based retrieval of clips, indexing and annotation (e.g. Aigrain et al. 1996). Other researchers attempt to get semantic analysis from multimedia content. Michel Lew and colleagues try to address the semantic gap by &#34;translating the easily computable low-level content-based media feature to high level concepts or terms intuitive to users&#34; (cited in Chinchor et al. 2010, p.54). Other topics such as similarity matching, aesthetics, security, and storytelling have been the focus of research into web-based multimedia collections (Datta et al. 2008). Also, shot boundary detection, face detection and content-based 3D shape retrieval have been the focus of recent research (respectively Leinhart 2001; Yang et al. 2002; Tangelder &amp; Veltkamp 2004). A notable project for multimedia analysis for the humanities is Carnegie Mellon University&#39;s Informedia project (Christel 2009 (<a href="http://www.informedia.cs.cmu.edu/">www.informedia.cs.cmu.edu</a><em>))</em> that uses speech, image and language processing to improve the navigation of a video and audio corpora (The HistoryMakers African-American oral history archive (<a href="http://www.idvl.org/thehistorymakers">http://www.idvl.org/thehistorymakers</a>/) &amp; NIST TRECVIC broadcast news archive). However, it is generally agreed that research in this area is application specific and robust, and automatic solutions for the whole domain of media remain largely undiscovered (Chinchor et al. 2010, p.55).</p>
<h3>Visual analytics</h3>
<p>In the last five years, the visual analytics field has grown enormously, and its core challenges for the upcoming five years are clearly articulated (Thomas &amp; Kielman 2009; Pike et al. 2009). Visual Analytics includes associated fields of Scientific Visualization, Information Visualization and Knowledge Engineering, Data Management and Data Mining, as well as Human Perception and Cognition. The research agenda of Visual Analytics addresses the science of analytics reasoning, visual representation and interaction techniques, data representation and transformations, presentation, production and dissemination (Thomas &amp; Kielman 2009, p.309). Core areas of development have included business intelligence, market analysis, strategic controlling, security and risk management, health care and biotechnology, automotive industry, environment and climate research, as well as other disciplines of natural, social, and economic sciences.</p>
<p><span class="text">In the humanities, <em>Cultural Analytics</em> as developed by UC San Diego uses computer-based techniques for quantitative analysis and interactive visualization employed in sciences to analyze massive multi-modal cultural data sets on gigapixels screens (Manovich 2009). The project draws upon cutting-edge cyberinfrastructure and visualization research at Calit2.</span> Visual analytics was referred to as one of the upcoming key technologies for research with adoption of 4-5 years in the Horizon Report (2010). While visual data analysis was not mentioned in the Museum edition of the Horizon report (2010), I would argue that it represents techniques fundamental to the re-representation of museological collections (online and offline).</p>
<h3>Text visualization</h3>
<p>Research into new modalities of visualizing data is essential for a world producing and consuming digital data (which is predominantly textual data) at unprecedented scales (McCandless 2010; Keim 2006). Computational linguistics is providing many of the analytics tools required for the mining of digital texts (e.g. Speer et al. 2010; Thai &amp; Handschuh 2010) The first international workshop for intelligent interface to text visualization only recently took place in Hong Kong, 2010 (Lui et al. 2010). Most previous work in text visualization focused on one of two areas: visualizing repetitions, and visualizing collocations. The former shows how frequently, and where, particular words are repeated, and the latter describes the characteristics of the linguistic &#34;neighborhood&#34; in which these words occur. Word clouds are a popular visualization technique whereby words are shown in font sizes corresponding to their frequencies in the document. It can also show changes in frequencies of words through time (Havre et al. 2000) and in different organizations (Collins et al. 2009), and emotions in different geographical locations (Harris &amp; Kamvar 2009). The significance of a word also lies in the locations at which it occurs. Tools such as <em>TextArc</em> (Paley 2002), Blue Dots (Lancaster 2007, 2008a, 2008b) and <em>Arc Diagrams</em> (Wattenberg 2002) visualize these &#34;word clusters&#34; but are constrained by the small window size of a desktop monitor. In a concordance or &#34;keyword-in-context&#34; search, the user supplies one or more query words, and the search engine returns a list of sentence segments in which those words occur. IBM&#39;s <em>Many Eyes</em> (<a href="http://www.manyeyes.alphaworks.ibm.com/">http://www.manyeyes.alphaworks.ibm.com</a>) displays the context with suffix trees, thereby visualizing the most frequent n-grams following a particular word. In the digital humanities, use of words and text strings is the typical mode of representation of mass corpora. However, new modes of lexical visualization such as <em>Visnomad</em> (http://<a href="http://www.visnomad.org/">www.visnomad.org</a>) are emerging as dynamic visualization tools for comparing one text with another. In another example, the <em>Visualization of the Bible</em> by Chris Harrison, each of the 63,779 cross references found in the Bible is depicted by a single arc whose color corresponds to the distance between the two chapters (Harrison &amp; Romhild 2008).</p>
<h2>4. Cultural Data Sculpting</h2>
<p>This section details prior work by the researcher partners dealing with large-scale video archives (iCinema Centre, <em>T_Visionarium I</em> &amp; <em>II</em> 2003; 2008; <em>Open City</em> 2009) and the real-time delivery of Wikipedia pages and image retrieval from the Internet (ZKM, <em>Crowdbrowing</em>, 2008). All projects were delivered in large-scale museum-situated panoramic or spherical projection systems. The paper then goes on to describe the current works under production at ALiVE.</p>
<h3>Previous work</h3>
<h4>T_Visionarium</h4>
<p><em>T_Visionarium I</em> was developed by iCinema Centre, UNSW in 2003. It takes place in the Jeffrey Shaw&#39;s EVE dome (Figure 2), an inflatable (12 meters by 9 meters). Upon entering the dome, viewers place position-tracking devices on their heads. The projection system is fixed on a motorized pan tilt apparatus mounted on a tripod. The database used here was recorded during a week-long period from 80 satellite television channels across Europe. Each channel plays simultaneously across the dome; however, the user directs or reveals any particular channel at any one time. The matrix of &#39;feeds&#39; is tagged with different parameters – keywords such as phrases, color, pattern, and ambience. Using a remote control, each viewer selects options from a recombinatory search matrix. On selection of a parameter, the matrix then extracts and distributes all the corresponding broadcast items of that parameter over the entire projection surface of the dome. For example, selecting the keyword &#34;dialogue&#34; causes all the broadcast data to be reassembled according to this descriptor. By head turning, the viewer changes the position of the projected image, and shifts from one channel&#39;s embodiment of the selected parameter to the next. In this way, the viewer experiences a revealing synchronicity between all the channels linked by the occurrence of keyword tagged images. All these options become the recombinatory tableau in which the original data is given new and emergent fields of meaning (Figure 3).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_0.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig2.jpg" border="0" alt="Fig 2: EVE © Jeffrey Shaw" title="Fig 2: EVE © Jeffrey Shaw" width="500" height="350"/></a><span class="caption"><strong>Fig 2: EVE © Jeffrey Shaw</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_1.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig3.jpg" border="0" alt="Fig 3: T_Visionarium I © UNSW iCinema Research Centre" title="Fig 3: T_Visionarium I © UNSW iCinema Research Centre" width="500" height="278"/></a><span class="caption"><strong>Fig 3: T_Visionarium I © UNSW iCinema Research Centre</strong></span></span></p>
<p><em>T_Visionarium II</em> (produced as part of the ARC Discovery, &#39;Interactive Narrative as a Form of Recombinatory Search in the Cinematic Transcription of Televisual Information&#39;) uses 24 hours of free-to-air broadcast TV footage from 7 Australian channels as its source material. This footage was analyzed by software for changes of camera angle, and at every change in a particular movie (whether it be a dramatic film or a sitcom), a cut was made, resulting in a database of 24,000 clips of approx. 4 seconds each. Four researchers were employed to hand tag each 4 second clip with somewhat idiosyncratic metadata related to the images shown, including emotion; expression; physicality; scene structure; with metatags including speed; gender; colour; and so on. The result is 500 simultaneous video streams looping each 4 seconds, and responsive to a user&#39;s search (<a href="http://www.icinema.unsw.edu.au/projects/prj_tvis_II_2.html">http://www.icinema.unsw.edu.au/projects/prj_tvis_II_2.html</a>) (Figures 4 &amp; 5).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_2.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig4.jpg" border="0" alt="Fig 4: T_Visionarium II in AVIE © UNSW iCinema Research Centre" title="Fig 4: T_Visionarium II in AVIE © UNSW iCinema Research Centre" width="500" height="375"/></a><span class="caption"><strong>Fig 4: T_Visionarium II in AVIE © UNSW iCinema Research Centre</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_3.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig5.jpg" border="0" alt="Fig 5: Close of the dataspace, T_Visionarium II © UNSW iCinema Research Centre" title="Fig 5: Close of the dataspace, T_Visionarium II © UNSW iCinema Research Centre" width="500" height="359"/></a><span class="caption"><strong>Fig 5: Close of the dataspace, T_Visionarium II © UNSW iCinema Research Centre</strong></span></span></p>
<p>An antecedent of the <em>T_Visionarium</em> projects can be found in Aby Warburg&#39;s, <em>Mnemosyne</em>, a visual cultural atlas, a means of studying the internal dynamics of imagery at the level of its medium rather than its content, performing image analysis through montage and recombination. <em>T_Visionarium</em> can be framed by the concept of aesthetic transcription; that is, the way new meaning can be produced is based on how content moves from one expressive medium to another. The digital allows the transcription of televisual data, decontextualising the original and reconstituting it within a new artifact. As the archiving abilities of the digital allow data to be changed from its original conception, new narrative relationships are generated between the multitudes of clips, and meaningful narrative events emerge because of viewer interaction in a transnarrative experience where gesture is all-defining. The segmentation of the video reveals something about the predominance of close-ups, the lack of panoramic shots, the heavy reliance on dialogue in TV footage. These aesthetic features come strikingly to the fore in this hybrid environment. The spatial contiguity gives rise to news ways of seeing, and of reconceptualising in a spatial montage (Bennett 2008). In <em>T_Visionarium</em> the material screen no longer exists (Figure 6). The boundary of the cinematic frame has been violated, hinting at the endless permutations that exist for the user. Nor does the user enter a seamless unified space; rather, he or she is confronted with the spectacle of hundreds of individual streams. Pannini&#39;s picture galleries also hint at this infinitely large and diverse collection, marvels to be continued beyond the limits of the picture itself.</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_4.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig6.jpg" border="0" alt="Fig 6: Datasphere, T_Visionarium II © UNSW iCinema Research Centre" title="Fig 6: Datasphere, T_Visionarium II © UNSW iCinema Research Centre" width="500" height="500"/></a><span class="caption"><strong>Fig 6: Datasphere, T_Visionarium II © UNSW iCinema Research Centre</strong></span></span></p>
<h4>Open City</h4>
<p>The <em>T&#39;Visionarium</em> paradigm was also recently employed for <em>OPEN CITY</em>, as part of the 4th International Architecture Biennale in Rotterdam &amp; the International Documentary Film Festival of Amsterdam, in 2009. Via the program Scene-Detector, all 450 documentary films were segmented into 15,000 clips. Categories and keywords here were related in some way to the thematic of the Biennial and included tags such as rich/poor, torn city, alternative lifestyles, religion, aerial views, cityscapes, highrise, green, public/open, transport, slums. Also, time, location, and cinematographic parameters such as riders, close-up, black and white, color, day, night, were used to describe the footage (Figure 7).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_5.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig7.jpg" border="0" alt="Fig 7: Open City © UNSW iCinema Research Centre. Image: VPRO" title="Fig 7: Open City © UNSW iCinema Research Centre. Image: VPRO" width="500" height="333"/></a><span class="caption"><strong>Fig 7: Open City © UNSW iCinema Research Centre. Image: VPRO</strong></span></span></p>
<h4><em>Crowdbrowsing</em></h4>
<p>The interactive installation <em>CloudBrowsing</em> (2008-09) was one of the first works to be developed and shown in ZKM&#39;s recently established PanoramaLab (<em>Crowdbrowsing</em> ZKM 2008b). The project lets users experience Web-based information retrieval in a new way:</p>
<p class="BlockQuote">Whereas our computer monitor only provides a restricted frame, a small window through which we experience the multilayered information landscape of the Net only partially and in a rather linear mode, the installation turns browsing the Web into a spatial experience: Search queries and results are not displayed as text-based lists of links, but as a dynamic collage of sounds and images. (<em>Crowdbrowsing ZKM 2008b)</em></p>
<p>In the current version of the project (Figures 8, 9 &amp; 10), the user browses the free online encyclopedia <em>Wikipedia</em>. A filter mechanism ensures that only open content is displayed in the installation. The cylindrical surface of the 306-degree PanoramaScreen becomes a large-scale browser surrounding the user, who can thus experience a panorama of his movements in the virtual information space.</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_6.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig8.jpg" border="0" alt="Fig 8: Wikipedia pages, Crowdbrowsing © ZKM" title="Fig 8: Wikipedia pages, Crowdbrowsing © ZKM" width="500" height="357"/></a><span class="caption"><strong>Fig 8: Wikipedia pages, Crowdbrowsing © ZKM</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_7.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig9.jpg" border="0" alt="Fig 9: Crowdbrowsing © ZKM" title="Fig 9: Crowdbrowsing © ZKM" width="500" height="309"/></a><span class="caption"><strong>Fig 9: Crowdbrowsing © ZKM</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_8.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig10.jpg" border="0" alt="Figure 10: Crowdbrowsing: interface control © ZKM" title="Figure 10: Crowdbrowsing: interface control © ZKM" width="500" height="376"/></a><span class="caption"><strong>Figure 10: Crowdbrowsing: interface control © ZKM</strong></span></span></p>
<h3>Current work</h3>
<h4><em>Data Sculpture Museum</em></h4>
<p>This project is being developed as part of the Australian Research Council Linkage Grant (2011 – 2014) for &#34;The narrative reformulation of multiple forms of databases using a recombinatory model of cinematic interactivity&#34; (UNSW iCinema Research Centre, Museum Victoria, ALiVE, City University, ZKM Centre for Built Media). The aim of this research is to investigate re-combinatory search, transcriptive narrative and multimodal analytics for heterogeneous datasets through their visualization in a 360° stereoscopic space (Del Favero et al. 2009). Specifically, the exploration of re-combinatory search of cultural data (as a cultural artefact) as an interrogative, manipulable and transformative narrative, responsive to and exposing multiple narrations that can be arranged and projected momentarily (Deleuze, 1989) over that which is purposefully embedded and recorded in the architecture of data archive and metadata, and witnessed (Ricoeur 2004). This project builds upon the exploration and gains made in the development of <em>T_Visionarium</em>.</p>
<p>The datasets used include over 100,000 multimedia rich records (including audio files, video files, high resolution monoscopic and stereoscopic images, panoramic images/movies, and text files) from Museum Victoria and the media art history database of the ZKM (<a href="http://on1.zkm.de/zkm/e/institute/mediathek/">http://on1.zkm.de/zkm/e/institute/mediathek/</a>) that include diverse subject areas from the arts and sciences collections. The data are collated from collection management systems and from web-based and exhibition-based projects. Additional metadata and multimedia analysis will be used to allow for intelligent searching across datasets. Annotation tools will provide users with the ability to make their own pathways through the data terrain, a psycho geography of the museum collections. Gesture-based interaction will allow users to combine searches, using both image-based and text input methods. Search parameters include:</p>
<ul>
<li>Explicit (keyword search based on collections data and extra metadata tags added using the project),</li>
<li>Multimedia (e.g. show me all faces like this face; show me all videos on Australia, show me everything pink!),</li>
<li>Dynamic (e.g. show me the most popular search items; join my search to another co-user; record my search for others to see; add tags).</li>
</ul>
<p>This project seeks understanding in the developments of media aesthetics. Problems of meaningful use of information are related to the way users integrate the outcomes of their navigational process into coherent narrative forms. In contrast to the interactive screen based approaches conventionally used by museums, this study examines the exploratory strategies enacted by users in making sense of large-scale databases when experienced immersively in a manner similar to that experienced in real displays (Latour 1988). In particular, evaluation studies will ask:</p>
<ol>
<li>How do museum users interact with an immersive 360-degree data browser that enables navigational and editorial choice in the re-composition of multi-layered digital information?</li>
<li>Do the outcomes of choices that underpin editorial re-composition of data call upon aesthetic as well as conceptual processes and in what form are they expressed? (Del Favero et al. 2009)</li>
</ol>
<p>The recent advent of large-scale immersive systems has significantly altered the way information can be archived, accessed and sorted. There is significant difference between museum 2D displays that bring pre-recorded static data into the presence of the user, and immersive systems that enable museum visitors to actively explore dynamic data in real-time. The experimental study into the meaningful use of data involves the development of an experimental browser capable of engaging users by enveloping them in an immersive setting that delivers information in a way that can be sorted, integrated and represented interactively. Specifications of the proposed experimental data browser include:</p>
<ul>
<li>immersive 360-degree data browser presenting multi-layered and heterogeneous data</li>
<li>re-compositional system enabling the re-organization and authoring of data</li>
<li>scalable navigational systems incorporating Internet functions</li>
<li>collaborative exploration of data in a shared immersive space by multiple users</li>
<li>intelligent interactive system able to analyze and respond to users&#39; transactions.</li>
</ul>
<h4><em>Rhizome of the Western Han</em></h4>
<p>This project investigates the integration of high resolution archaeological laser scan and GIS data inside AVIE. This project represents a process of archaeological recontextualization, bringing together remote sensing data from the two tombs (M27 &amp; The Bamboo Garden) with laser scans of funerary objects, in a spatial context (Figure 11, 12 &amp; 13). This prototype builds an interactive narrative based on spatial dynamics, and cultural aesthetics and philosophies embedded in the archaeological remains. The study of Han Dynasties (206 BC-220 A.D.) imperial tombs has always been an important field of Chinese archaeology. However, only a few tombs of the West Han Dynasty have been scientifically surveyed and reconstructed. Further, the project investigates a reformulation of narrative based on the application of cyber mapping principles in archaeology (Forte 2010; Kurillo et al. 2010).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omni_9.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig11.jpg" border="0" alt="Fig 11: Rhizome of the Western Han: inhabiting the tombs at 1:1 scale © ALiVE, CityU" title="Fig 11: Rhizome of the Western Han: inhabiting the tombs at 1:1 scale © ALiVE, CityU" width="500" height="280"/></a><span class="caption"><strong>Fig 11: Rhizome of the Western Han: inhabiting the tombs at 1:1 scale © ALiVE, CityU</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_10.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig12.jpg" border="0" alt="Fig 12: Rhizome of the Western Han: iconographic hotspots © ALiVE, CityU  " title="Fig 12: Rhizome of the Western Han: iconographic hotspots © ALiVE, CityU  " width="500" height="280"/></a><span class="caption"><strong>Fig 12: Rhizome of the Western Han: iconographic hotspots © ALiVE, CityU </strong></span></span></p>
<p><span class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_11.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig13.jpg" border="0" alt="Fig 13: Rhizome of the Western Han: image browser © ALiVE, CityU" title="Fig 13: Rhizome of the Western Han: image browser © ALiVE, CityU" width="500" height="280"/></a><span class="caption"><strong>Fig 13: Rhizome of the Western Han: image browser © ALiVE, CityU</strong></span></span></span></p>
<p>There is ample discourse to situate the body at the forefront of interpretive archaeology research as a space of phenomenological encounter. Post-processual frameworks for interpretive archaeology advance a phenomenological understanding of the experience of landscape. In his book, <em>Body and Image: Explorations in Landscape Phenomenology</em>, archaeologist Christopher Tilley, for example, usefully contrasts iconographic approaches to the study of representation with those of kinaesthetic enquiry. Tilley&#39;s line of reasoning provides grounding for the research into narrative agency in large-scale, immersive and sensorial, cognitively provocative environments (Kenderdine, 2010). This project examines a philosophical discussion of what it means to inhabit archaeological data &#39;at scale&#39; (1:1). It also re-situates the theatre of archaeology in a fully immersive display system, as &#34;the (re)articulation of fragments of the past as real-time event&#34; (Pearson &amp; Shanks 2001).</p>
<h4><em>Blue Dots</em></h4>
<p>This project integrates the Chinese Buddhist Canon, Koryo version Tripitaka Koreana, into the AVIE system (a project between ALiVE, City University Hong Kong and UC Berkeley). This version of the Buddhist Canon is inscribed as UNESCO World Heritage enshrined in Haeinsa, Korea. The 166,000 pages of rubbings from the wooden printing blocks constitute the oldest complete set of the corpus in print format (Figures 14 &amp; 15). Divided into 1,514 individual texts, the version has a complexity that is challenging since the texts represent translations from Indic languages into Chinese over a 1000-year period (2nd-11th centuries). This is the world&#39;s largest single corpus containing over 50 million glyphs, and it was digitized and encoded by Prof Lew Lancaster and his team in a project that started in the 70s (Lancaster 2007, 2008a, 2008b).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_12.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig14.jpg" border="0" alt=" Fig 14: Tripitaka Koreana © Korean Times (http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)" title=" Fig 14: Tripitaka Koreana © Korean Times (http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)" width="500" height="384"/></a><span class="caption"><strong> Fig 14: Tripitaka Koreana © Korean Times (http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_13.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig15.jpg" border="0" alt="Fig 15: Tripitaka Koreana © Korean Times (http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)" title="Fig 15: Tripitaka Koreana © Korean Times (http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)" width="500" height="398"/></a><span class="caption"><strong>Fig 15: Tripitaka Koreana © Korean Times (http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)</strong></span></span></p>
<p>The <em>Blue Dots</em> (<a href="http://ecai.org/textpatternanalysis/">http://ecai.org/textpatternanalysis/</a>) project undertaken at Berkeley as part of the Electronic Cultural Atlas Initiative (ECAI) abstracted each glyph from the Canon into a blue dot, and gave metadata to each of these Blue Dots, allowing vast searches to take place in minutes rather than scholarly years. In the search function, each blue dot also references an original plate photograph for verification. The shape of these wooden plates gives the blue dot array its form (Figure 16). As a searchable database, it exists in a prototype form on the Internet (currently unavailable). Results are displayed in a dimensional array where users can view and navigate within the image. The image uses both the abstracted form of a &#39;dot&#39; as well as color to inform the user of the information being retrieved. Each blue dot represents one glyph of the dataset. Alternate colors indicate the position of search results. The use of color, form, and dimension for fast understanding of the information is essential for large data sets where thousands of occurrences of a target word/phrase may be seen. Analysis across this vast text retrieves visual representations of word strings, clustering of terms, automatic analysis of ring construction, viewing results by time, creator, and place. The <em>Blue Dots</em> method of visualization is a breakthrough for corpora visualization and lies at the basis of the visualization strategies of abstraction undertaken in this project. The application of an omni-spatial distribution of this text solves problems of data occlusion, and enhances network analysis techniques to reveal patterns, hierarchies and interconnectedness (Figures 17 &amp; 18). Using a hybrid approach to data representation, audification strategies will be incorporated to augment interaction coherence and interpretation. The data browser is designed to function in two modes: the Corpus Analytics mode for text only, and the Cultural Atlas mode that incorporates original texts, contextual images and geospatial data. Search results can be saved and annotated.</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_14.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig16.jpg" border="0" alt="Fig 16: Blue Dots: abstraction of characters to dots and pattern arrays © ECAI, Berkeley" title="Fig 16: Blue Dots: abstraction of characters to dots and pattern arrays © ECAI, Berkeley" width="500" height="282"/></a><span class="caption"><strong>Fig 16: Blue Dots: abstraction of characters to dots and pattern arrays © ECAI, Berkeley</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_15.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig17.jpg" border="0" alt="Fig 17: Prof Lew Lancaster interrogates the prototype of Blue Dots AVIE © ALiVE, CityU. Image: Howie Lan" title="Fig 17: Prof Lew Lancaster interrogates the prototype of Blue Dots AVIE © ALiVE, CityU. Image: Howie Lan" width="500" height="375"/></a><span class="caption"><strong>Fig 17: Prof Lew Lancaster interrogates the prototype of Blue Dots AVIE © ALiVE, CityU. Image: Howie Lan</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_16.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig18.jpg" border="0" alt="Fig 18: Close up of blue dots &amp; corresponding texts, prototype of Blue Dots AVIE © ALiVE, CityU. Image: Howie Lan" title="Fig 18: Close up of blue dots &amp; corresponding texts, prototype of Blue Dots AVIE © ALiVE, CityU. Image: Howie Lan" width="500" height="369"/></a><span class="caption"><strong>Fig 18: Close up of blue dots &amp; corresponding texts, prototype of Blue Dots AVIE © ALiVE, CityU. Image: Howie Lan</strong></span></span></p>
<p>The current search functionality ranges from visualizing word distribution and frequency, to revealing other structural patterns such as the chiastic structure and ring compositions. In the <em>Blue Dots</em> AVIE version, the text is also visualized as a matrix of simplified graphic elements representing each of the words. This will enable users to identify new linguistic patterns and relationships within the matrix, as well as access the words themselves and related contextual materials. The search queries will be applied across multiple languages, accessed collaboratively by researchers, extracted and saved for later re-analysis.</p>
<p>The data provide an excellent resource for the study of dissemination of documents over geographic and temporal spheres. Additional metadata, such as present day images of the monasteries where the translation took place, will be included in the data array. The data will also be sonified. The project will design new omni-directional metaphors for interrogation and the graphical representation of complex relationships between these textual datasets to solve the significant challenges of visualizing both abstract forms and close-up readings of this rich data. In this way, we hope to set benchmarks in visual analytics, scholarly analysis in the digital humanities, and the interpretation of classical texts</p>
<h4><em>Social Networks of Eminent Buddhists</em></h4>
<p>This visualization of social networks of Chinese Buddhists is based on the Gaoseng zhuang corpus produced at the Digital Archives Section of Dharma Drum Buddhist College, Taiwan (<a href="http://buddhistinformatics.ddbc.edu.tw/biographies/socialnetworks/interface/">http://buddhistinformatics.ddbc.edu.tw/biographies/socialnetworks/interface/</a>). The Gaoseng zhuan corpus contains the biographies of eminent Buddhists between the 2nd and 17th century. The collections of hagio-biographies of eminent Buddhist monks and nuns are one of the most interesting sources for the study of Chinese Buddhism. These biographies offer a fascinating glance into the lives of religious professionals in China between c. 200 and 1600 CE. In contrast to similar genres in India or Europe, the Chinese hagio-biographies do not, in the main, emphasize legend, but, following Confucian models of biographical literature, are replete with datable historical facts. The project uses <a href="http://www.tei-c.org/index.xml">TEI</a> to markup the four most important of these collections, which together contain more than 1300 hagio-biographies. The markup identifies person and place names as well as dates (<a href="http://buddhistinformatics.ddbc.edu.tw/biographies/gis/">http://buddhistinformatics.ddbc.edu.tw/biographies/gis/</a><em>).</em></p>
<p>It further combines these into nexus points to describe events of the form: one or more person(s) were at a certain time at a certain place (Hung et al. 2009; Bingenheimer et al. 2009; Bingenheimer et al. 2011).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_17.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig19.jpg" border="0" alt="Fig 19: Social Networks of Eminent Buddhists JavaScript applet © Digital Archives, Dharma Drum Buddhist College" title="Fig 19: Social Networks of Eminent Buddhists JavaScript applet © Digital Archives, Dharma Drum Buddhist College" width="500" height="422"/></a><span class="caption"><strong>Fig 19: Social Networks of Eminent Buddhists JavaScript applet © Digital Archives, Dharma Drum Buddhist College</strong></span></span></p>
<p>On the Web, the social networks are displayed as spring loaded nodal points (JavaScript applet; Figure 19). However, for immersive architectures such as AVIE, there is no intuitive way to visualize such a social network as a 2D relationship graph in the virtual 3D space provided by the architecture itself. Therefore, this project proposes an effective mapping that projects the 2D relationship graph into 3D virtual space and provides an immersive visualization and interaction system for intuitive visualizing and exploring of social network using the immersive architecture AVIE. The basic concept is to map the center of the graph (the current focusing person) to the center of the virtual 3D world and project the graph horizontally on the ground of the virtual space. This mapping provides an immersive visualization such that the more related nodes will be closer to the center of the virtual world (where the user is standing and operating). However, this will put all the nodes and connections at the same horizontal level, and therefore they will obstruct each other. To resolve this, nodes have been raised according to their distances from the world center; i.e. the graph is mapped on to a paraboloid such that farther nodes will be at a higher horizontal level. This mapping optimizes the usage of 3D space and gives an intuitive image to the user – smaller and higher objects are farther from the user. In general, there are no overlapped nodes, as nodes with different distances from the world center will have different heights; thus the user views them with different pitch angles. The connections between nodes are also projected on to the paraboloid such that they become curve segments; viewers can follow and trace the connections easily since the nodes are already arranged with different distances and horizontal levels (Figures 20 &amp; 21).</p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_18.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig20.jpg" border="0" alt="Fig 20: Prototype of Social Networks of Eminent Buddhists, AVIE © ALiVE, CityU" title="Fig 20: Prototype of Social Networks of Eminent Buddhists, AVIE © ALiVE, CityU" width="500" height="375"/></a><span class="caption"><strong>Fig 20: Prototype of Social Networks of Eminent Buddhists, AVIE © ALiVE, CityU</strong></span></span></p>
<p class="caption"><span class="inline inline-center"><a href="../../image/kenderdine_s_and_t_hart_cultural_data_sculpting_omn_19.html"><img class="image image-_original " src="http://museumsandtheweb.com/files/images/kenderdine_fig21.jpg" border="0" alt="Fig 21: Researcher using Prototype of Social Networks of Eminent Buddhists, AVIE © ALiVE, CityU  " title="Fig 21: Researcher using Prototype of Social Networks of Eminent Buddhists, AVIE © ALiVE, CityU  " width="500" height="375"/></a><span class="caption"><strong>Fig 21: Researcher using Prototype of Social Networks of Eminent Buddhists, AVIE © ALiVE, CityU </strong></span></span></p>
<p>To reduce the total number of nodes to be displayed and emphasize the local relationship of the current focusing person, only the first two levels of people related to the current focusing person are shown to the user. Users can change their focus to another person by selecting the new focusing person with the 3D pointer. The relationship graph updates dynamically such that only the first two levels of relational are displayed. Users can also check the biography of any displayed person by holding the 3D pointer on the corresponding node. The biography of the selected person will be displayed in a separated floating window. The geospatial referencing and a dynamic GoogleEarth map are soon to be implemented.</p>
<h2>5. Conclusion</h2>
<p>The projects described begin to take on core challenges of visual analytics, multimedia analysis, text analysis and visualization inside AVIE to provide powerful modalities for an omni-directional exploration of museum collections, archaeological laser scan data and multiple textual datasets. The research is responding to the need for embodied interaction and knowledge-based interfaces that enhance collaboration, cognition and perception, and narrative coherence. Through AVIE, museum users and scholars are investigating the quality of narrative coherence brought to interactive navigation and re-organization of information in 360-degree 3D space. There will be ongoing reporting related to the Data Sculpture Museum, which has recently commenced as part of a three -ear project, and the Blue Dots. Upcoming projects in AVIE include a real-time visualization of the Israel Museum of Jerusalem Europeana dataset (5000 records (<a href="http://www.europeana.eu/">http://www.europeana.eu</a>)) that is looking for new ways to access museum collections with existing (and constrained) metadata, and an interactive installation using laser scan data from the UNESCO World Heritage site of the Dunhuang Caves, Gobi Desert, China.</p>
<h2>6. Acknowledgements:</h2>
<p class="AcknowedgementsText">The author would like to acknowledge the contribution of colleagues at ALiVE: Prof Jeffrey Shaw, William Wong and Dr Oscar <span class="rwrro">Kin Chung Au</span>. Also the contributions of members of the Department of Chinese, Translation and Linguistics, CityU, in relation to textual analytics, Prof Jonathan Webster and Dr John Lee. The title of this paper, Cultural Data Sculpting, is inspired Zhao &amp; Vande Moere (2008).</p>
<p class="AcknowedgementsText"><em>Data Sculpture Museum: The narrative reformulation of multiple forms of databases using a recombinatory model of cinematic interactivity</em>. Partners: UNSW iCinema Research Centre, Museum Victoria, ZKM, City University of Hong Kong. Researchers: Assoc Prof Dr Dennis Del Favero, Prof Dr. Horace Ip, Mr Tim Hart, Assoc Prof Dr Sarah Kenderdine, Prof Jeffrey Shaw, Prof Dr Peter Wiebel. This project is funded by the Australian Research Council 2011-2014.</p>
<p class="AcknowedgementsText"><em>Rhizome of the Western Han</em>. Partners: ALiVE, City University of Hong Kong, UC Merced, Researchers: Assoc Prof Dr Sarah Kenderdine, Prof Maurizio Forte, Carlo Camporesi, Prof Jeffrey Shaw.</p>
<p class="AcknowedgementsText"><em>Blue Dots AVIE</em>: Tripitaka Koreana Partners: ALiVE, City University of Hong Kong, UC Berkeley, Researchers: Assoc Prof Dr Sarah Kenderdine, Prof Lew Lancaster, Howie Lan, Prof Jeffrey Shaw.</p>
<p class="AcknowedgementsText"><em>Social Networks of Eminent Buddhists.</em> Partners: ALiVE, City University of Hong Kong, Dharma Drum Buddhist College<em>.</em> Researchers: Assoc Prof Dr Sarah Kenderdine, Dr Marcus Bingenheimer, Prof Jeffrey Shaw.</p>
<h2>7. References</h2>
<p class="ReferencesText">Allosphere, Available (<a href="http://www.allosphere.ucsb.edu/">http://www.allosphere.ucsb.edu/</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Applied Laboratory for Interactive Visualization and Embodiment – ALiVE, CityU, Hong Kong Available (<a href="http://www.cityu.edu.hk/alive">http://www.cityu.edu.hk/alive</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Bachelard, G. (1998), <em>Poetic imagination and reveries</em> (<em>L&#39;invitation au voyag</em>e 1971), Colette Guadin (trans.), Connecticut: Spring Publications Inc.</p>
<p class="ReferencesText">Bennett, J. (2008), <em>T_Visionarium: a Users Guide</em>, University of New South Wales Press Ltd.</p>
<p class="ReferencesText">Bennett, T. (2006), &#39;Civic seeing: museums and the organization of vision&#39;, in S. MacDonald (ed.), <em>Companion to museum studies</em>, Oxford: Blackwell, pp. 263-81.</p>
<p class="ReferencesText">Bingenheimer, Marcus; Hung, Jen-Jou; Wiles, Simon: &#34;Markup meets GIS – Visualizing the &#39;Biographies of Eminent Buddhist Monks&#39;&#34; in <em>Proceedings of the International Conference on Information Visualization 2009</em> (Published by the IEEE Computer Society). DOI: 10.1109/IV.2009.91</p>
<p class="ReferencesText">Bingenheimer, M., Hung, J-J., Wiles, S. (2011): &#34;Social Network Visualization from TEI Data&#34;, <em>Literary and Linguistic Computing</em> Vol. 26 (forthcoming).</p>
<p class="ReferencesText"><em>Blue Dots</em> (<a href="http://ecai.org/textpatternanalysis/">http://ecai.org/textpatternanalysis/</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Bonini, E. 2008, &#39;Building virtual cultural heritage environments: the embodied mind at the core of the learning processes&#39;, in <em>International Journal of Digital Culture and Electronic Tourism</em>, vol. 2, no. 2, pp. 113-25.</p>
<p class="ReferencesText">Chan, S. 2007, &#39;Tagging and searching: Serendipity and museum collection databases&#39;, in Trant, J. &amp; Bearman, D. (eds), <em>Museums and the Web 2007, proceedings,</em> Toronto: Archives &amp; Museum Information. Available online (<a href="http://www.archimuse.com/mw2007/papers/chan/chan.html">http://www.archimuse.com/mw2007/papers/chan/chan.html</a>), Consulted June 20 2009.</p>
<p class="ReferencesText">Chinchor, N, Thomas, J., Wong, P. Christel, M. &amp; Ribarsky, W. (2010), Multimedia Analysis + Visual Analytics = Multimedia Analytics, September/October 2010, <em>IEEE Computer Graphics</em>, vol. 30 no. 5. pp. 52-60.</p>
<p class="ReferencesText">Chion, M., et al.. (1994), <em>Audio-Vision</em>, Columbia University Press.</p>
<p class="ReferencesText">Christel, M.G. (2009), <em>Automated Metadata in Multimedia Information Systems: Creation, Refinement, Use in Surrogates, and Evaluation</em>. San Rafael, CA: Morgan and Claypool Publishers</p>
<p class="ReferencesText">Collins, C. Carpendale, S. &amp; Penn, G. (2009), DocuBurst: Visualizing Document Content using Language Structure. Computer Graphics Forum (<em>Proceedings of Eurographics/IEEE-VGTC Symposium on Visualization</em> (EuroVis &#39;09)), 28(3): pp.1039-1046.</p>
<p class="ReferencesText">Crowdbrowsing, KZM (2008a) [video] (<a href="http://container.zkm.de/cloudbrowsing/Video.html">http://container.zkm.de/cloudbrowsing/Video.html</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Crowdbrowsing, KZM (2008b) (<a href="http://www02.zkm.de/you/index.php?option=com_content&amp;view=article&amp;id=59:cloudbrowsing&amp;catid=35:werke&amp;Itemid=82&amp;lang=en">http://www02.zkm.de/you/index.php?option=com_content&amp;view=article&amp;id=59:cloudbrowsing&amp;catid=35:werke&amp;Itemid=82&amp;lang=en</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Del Favero, D., Ip, H., Hart, T., Kenderdine, S., Shaw, J., Weibel, P. (2009), Australian Research Council Linkage Grant, &#34;Narrative reformulation of museological data: the coherent representation of information by users in interactive systems&#34;. PROJECT ID: LP100100466</p>
<p class="ReferencesText">DeFanti, T. A., et al.. (2009). The StarCAVE, a third-generation CAVE &amp; virtual reality OptIPortal. <em>Future Generation Computer Systems, 25</em>(2), 169-178.</p>
<p class="ReferencesText">Deleuze, G. (1989) <em>Cinema 2: the Time Image</em>. Translated by Hugh Tomlinson and Robert Galeta, Minnesota: University of Minnesota.</p>
<p class="ReferencesText">Electronic Cultural Atlas Initiative (<a href="http://www.ecai.org/">www.ecai.org</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Forte, M. (2010), Introduction to Cyberarcheology, in Forte, M (ed) <em>Cyber Archaeology</em>, British Archaeological Reports BAR S2177 2010.</p>
<p class="ReferencesText">Green, T. M., Ribarsky &amp; Fisher (2009), Building and Applying a Human Cognition Model for Visual Analytics. <em>Information Visualization, 8</em>(1), pp.1-13.</p>
<p class="ReferencesText">Harris, J. &amp; Kamvar, S. (2009), <em>We feel fine</em>. New York, NY: Scribner.</p>
<p class="ReferencesText">Harrison, C. &amp; Romhild, C. (2008), <em>The Visualization of the Bible</em>. (<a href="http://www.chrisharrison.net/projects/bibleviz/index.html">http://www.chrisharrison.net/projects/bibleviz/index.html</a><em>).</em> Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Havre, S., et al.. (2000), ThemeRiver: Visualizing Theme Changes over Time. <em>Proc. IEEE Symposium on Information Visualization</em>, pp.115-123.</p>
<p class="ReferencesText">HIPerSpace CALIT2. (2010), Research Projects: HIPerSpace. (<a href="http://vis.ucsd.edu/mediawiki/index.php/Research_Projects:_HIPerSpace">http://vis.ucsd.edu/mediawiki/index.php/Research_Projects:_HIPerSpace</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Horizon Report, 2010, Four to Five Years: Visual Data Analysis. Available from (<a href="http://wp.nmc.org/horizon2010/chapters/visual-data-analysis/">http://wp.nmc.org/horizon2010/chapters/visual-data-analysis/</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Horizon Report, Museum edition, (2010). Available from (<a href="http://www.nmc.org/pdf/2010-Horizon-Report-Museum.pdf">www.nmc.org/pdf/2010-Horizon-Report-Museum.pdf</a>.) Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Hung, J,J., Bingenheimer, M. &amp; Wiles, S. (2010), Digital Texts and GIS: The interrogation and coordinated visualization of Classical Chinese texts&#34; in <em>Proceedings of the International Conference on Computer Design and Applications ICCDA 2010</em>.</p>
<p class="ReferencesText">Kasik, D. J., et al.. (2009), Data transformations &amp; representations for computation &amp; visualization. <em>Information Visualization 8</em>(4), pp.275-285.</p>
<p class="ReferencesText">Keim, D. A., et al.. (2006), Challenges in Visual Data Analysis. <em>Proc. Information Visualization (IV 2006)</em>, pp.9-16. London: IEEE.</p>
<p class="ReferencesText">Keim, D. A., et al.. (2008), Visual Analytics: Definition, Process, &amp; Challenges. <em>Information Visualization: Human-Centered Issues and Perspectives</em>, pp<em>.</em>154-175. Berlin, Heidelberg: Springer-Verlag.</p>
<p class="ReferencesText">Schettino, P. &amp; Kenderdine, S. (2010), &#39;PLACE-Hampi: interactive cinema and new narratives of inclusive cultural experience&#39;, International Conference on the Inclusive Museum, Istanbul, June 2010, <em>Inclusive Museums Journal</em> (In press).</p>
<p class="ReferencesText">Kenderdine, S. &amp; Shaw, J. (2009), &#39;New media insitu: The re-socialization of public space&#39;, in Benayoun, M. &amp; Roussou, M. (eds), <em>International Journal of Art and Technology, special issue on</em> <em>Immersive Virtual, Mixed, or Augmented Reality Art,</em> vol 2, no.4, Geneva: Inderscience Publishers, pp.258 – 276.</p>
<p class="ReferencesText">Kenderdine, S. (2010), &#39;Immersive visualization architectures and situated embodiments of culture and heritage&#39; <em>Proceedings of IV10 – 14th International Conference on Information Visualisation</em>, London, July 2010, IEEE, pp. 408-414.</p>
<p class="ReferencesText">Kenderdine, S., Shaw, J. &amp; Kocsis, A. (2009), &#39;Dramaturgies of PLACE: Evaluation, embodiment and performance in PLACE-Hampi&#39;, <em>DIMEA/ACE Conference (5th Advances in Computer Entertainment</em> <em>Technology Conference &amp; 3rd Digital Interactive Media Entertainment and Arts Conference</em>), Athens, November 2008, ACM. Vol 422, pp. 249-56.</p>
<p class="ReferencesText">Kurillo, G. Forte, M. Bajcsy, R. (2010), Cyber-archaeology and Virtual Collaborative Environments, in Forte, M. (ed) 2010, BAR S2177 2010: <em>Cyber-Archaeology</em>.</p>
<p class="ReferencesText">Lancaster, L. (2007), The First Koryo Printed Version of the Buddhist Canon: Its Place in Contemporary Research. <em>Nanzen-ji Collection of Buddhist Scriptures and the History of</em> <em>the Tripitake Tradition in East Asia.</em> Seoul: Tripitaka Koreana Institute.</p>
<p class="ReferencesText">Lancaster, L. (2008a), Buddhism &amp; the New Technology: An Overview. <em>Buddhism in the Digital Age: Electronic Cultural Atlas Initiative</em>. Ho Chi Minh: Vietnam Buddhist U.</p>
<p class="ReferencesText">Lancaster, L. (2008b), Catalogues in the Electronic Era: CBETA and <em>The Korean Buddhist Canon: A Descriptive Catalogue</em>. Taipei: CBETA (electronic publication).</p>
<p class="ReferencesText">Lancaster, L. (2010), Pattern Recognition &amp; Analysis in the Chinese Buddhist Canon: A Study of &#34;Original Enlightenment&#34;. <em>Pacific World</em>.</p>
<p class="ReferencesText">Latour, Bruno. (1988). Visualisation and Social Reproduction. In G.Fyfe and J. Law (Eds.). Picturing Power: Visual Depiction and Social Relations. London: Routledge. 15-38.</p>
<p class="ReferencesText">Liu, S., <em>et al.</em> (eds.) (2010), <em>Proc. 1st Int. Workshop on Intelligent Visual Interfaces for Text Analysis</em>, IUI&#39;10.</p>
<p class="ReferencesText">Manovich, L. (2008), The Practice of Everyday (Media) Life. In R. Frieling (Ed.), The Art of Participation: 1950 to Now. London: Thames and Hudson.</p>
<p class="ReferencesText">Manovich, L. (2009), How to Follow Global Digital Cultures, or Cultural Analytics for Beginners. <em>Deep Search: They Politics of Search beyond Google.</em> Studienverlag (German version) and Transaction Publishers (English version).</p>
<p class="ReferencesText">Many Eyes<em>.</em> <em>(</em><a href="http://www.manyeyes.alphaworks.ibm.com/">http://www.manyeyes.alphaworks.ibm.com</a><em>)</em>. Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Maturana, H. &amp; Varela, F. (1980), <em>Autopoiesis and cognition: The realization of the living</em>, vol. 42, Boston Studies in the Philosophy of Science, Dordrecht: D. Reidel.</p>
<p class="ReferencesText">McCandless, D. (2010.) The beauty of data visualization [Video file]. <em>(</em><a href="http://www.ted.com/talks/lang/eng/david_mccandless_the_beauty_of_data_visualization.html">http://www.ted.com/talks/lang/eng/david_mccandless_the_beauty_of_data_visualization.html</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">McGinity, M., <em>et al.</em> (2007), <em>AVIE</em>: A Versatile Multi-User Stereo 360-Degree Interactive VR Theatre. <em>The 34</em><em>th</em> <em>Int. Conference on Computer Graphics &amp; Interactive Techniques,</em> <em>SIGGRAPH 2007</em>, 5-9 August 2007.</p>
<p class="ReferencesText">National Science Foundation, (2007)<em>, Cyberinfrastructure Vision for 21st Century Discovery</em>. Washington: National Science Foundation.</p>
<p class="ReferencesText">Nechvatal, J. (2009), <em>Towards an Immersive Intelligence: Essays on the Work of Art in the Age of Computer Technology and Virtual</em> Reality (1993-2006) Edgewise Press, New York, NY.</p>
<p class="ReferencesText">Paley, B. 2002. <em>TextArc</em> (<a href="http://www.textarc.org/">http://www.textarc.org</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Pearson, M. &amp; Shanks, M. (2001) <em>Theatre/Archaeology</em>, London: Routledge.</p>
<p class="ReferencesText">Pike, W. A., <em>et al.</em> (2009), The science of interaction. <em>Information Visualization, 8</em>(4), pp.263-274.</p>
<p class="ReferencesText"><a href="http://www.press.uchicago.edu/presssite/author.epl?fullauthor=Paul%20Ricoeur">Ricoeur</a><em>,</em> P. (2004), <em>Memory, History, Forgetting</em>, University of Chicago Press.</p>
<p class="ReferencesText">Speer, R., et al.. (2010), Visualizing Common Sense Connections with Luminoso. <em>Proc. 1st Int. Workshop on Intelligent Visual Interfaces for Text Analysis</em> (IUI&#39;10), pp.9-12.</p>
<p class="ReferencesText">Springer, Michelle et al. (2008), For the Common Good: the Library of Congress Flickr Pilot Project. <span class="s1"><a href="http://www.loc.gov/rr/print/flickr_report_final.pdf">http://www.loc.gov/rr/print/flickr_report_final.pdf</a> Consulted</span>: 8/02/2009.</p>
<p class="ReferencesText"><em>T_Visionarium</em> (2003-2008), (<a href="http://www.icinema.unsw.edu.au/projects/prj_tvis_II_2.html">http://www.icinema.unsw.edu.au/projects/prj_tvis_II_2.html</a>). Consulted Nov 30, 2010.</p>
<p class="ReferencesText">Thai, V. &amp; Handschuh, S. (2010), Visual Abstraction and Ordering in Faceted Browsing of Text Collections. <em>Proc. 1st Int. Workshop on Intelligent Visual Interfaces for Text</em> <em>Analysis</em> (IUI&#39;10), pp.41-44.</p>
<p class="ReferencesText">Thomas, J. &amp; Kielman, J. (2009), Challenges for visual analytics. <em>Information Visualization, 8</em>(4), pp.309-314.</p>
<p class="ReferencesText">Tilley, C. 2008, <em>Body and image: Explorations in landscape phenomenology,</em> Walnut Creek: Left Coast Press.</p>
<p class="ReferencesText"><em>Visnomad</em>. (<a href="http://www.visnomad.org/">www.visnomad.org</a>). Consulted Nov 30, 2010</p>
<p class="ReferencesText"><em>Visual Complexity</em>. (<a href="http://www.visualcomplexity.com/">www.visualcomplexity.com/</a>). Consulted Nov 30, 2010</p>
<p class="ReferencesText">Wattenberg, M. (2002), Arc Diagrams: Visualizing Structure in Strings. <em>Proc. IEEE Symposium on Information Visualization</em>, pp.110-116. Boston, MA.</p>
<p class="ReferencesText">West, R., <em>et al.</em> (2009), Sensate abstraction: hybrid strategies for multi-dimensional data in expressive virtual reality contexts. <em>Proc. 21st Annual SPIE Symposium on Electronic Imaging</em>, vol 7238 (2009), 72380I-72380I-11.</p>
<p class="ReferencesText">ZKM Centre for Art and Media (<a href="http://on1.zkm.de/zkm/e/">http://on1.zkm.de/zkm/e/</a>). Consulted Nov 30, 2010</p>
<p class="ReferencesText">Zhao J. and Vande Moere A. (2008), &#34;Embodiment in Data Sculpture: A Model of the Physical Visualization of Information&#34;, International Conference on Digital Interactive Media in Entertainment and Arts (DIMEA&#39;08), ACM International Conference Proceeding Series Vol.349, Athens, Greece, pp.343-350.</p>
<p class="ReferencesText">Zielinski, S. (2006), <em>Deep time of the media: Toward an archaeology of hearing and seeing by technical means</em>, Custance, G. (trans.), Cambridge, MA: MIT Press.</p>
</div>
<div id="citation">
<h4>Cite as:</h4>
<p class="references">Kenderdine, S., and T. Hart, Cultural Data Sculpting: Omni-spatial Visualization for Large Scale Heterogeneous Datasets. In J. Trant and D. Bearman (eds). <em>Museums and the Web 2011: Proceedings</em>. Toronto: Archives &amp; Museum Informatics. Published March 31, 2011. Consulted
<script type="text/javascript">// <![CDATA[
      // current date - based on http://rainbow.arch.scriptmania.com/scripts
      // Array of day names
      var dayNames = new Array("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday");

      var monthNames = new Array("January","February","March","April","May","June","July",
                           "August","September","October","November","December");

      var dt = new Date();
      var y  = dt.getYear();

      // Y2K compliant
      if (y < 1000) y +=1900;

      document.write(monthNames[dt.getMonth()] + " " + dt.getDate() + ", " + y + ". ");
// ]]></script>
http://conference.archimuse.com/mw2011/papers/cultural_data_sculpting_omnispatial_visualization_large_scale_heterogeneous_datasets</p>
</div>
<div id="copyright"><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img class="c1" src="../../files/images/cc-88x31.png" border="0" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 License" width="88" height="31"/></a>
<p>Copyright © 2011 – Archives &amp; Museum Informatics.</p>
</div><div class="field nodereference field-paper-program-ref">
                                <h3 class="label inline">Program Item Reference: </h3>
          <div class="item inline odd"><a href="../programs/cultural_data_sculpting_omni_spatial_interac.html">Cultural Data Sculpting: Omni-spatial interactive visualization for large scale heterogeneous datasets</a></div>
              </div>
 <!-- /content-field -->
  </div>
  
  <div class="article-meta">
      <p class="submitted">Posted <em class="time">March 16, 2011 - 8:07am</em> by <em class="author"><a href="../../user/sarah_kenderdine.html" class="username" title="View Sarah&#39;s profile.">Sarah Kenderdine</a></em></p>
  
    
  <ul class="taxonomy"><li><a href="../../conferences/mw/mw2011.html" rel="tag" title="Museums and the Web 2011">MW2011</a></li></ul>  </div>
  
  
</div> <!-- /article --></div>

        </div>