<div id="main-content">
          
                      <div id="breadcrumb">
              <h2 class="element-invisible">You are here:</h2>
              <a href="../../mw2011.html">MW2011</a> » <a href="../mini-workshops.html">Mini-Workshops</a> » <a href="../programs/how_to_evaluate_online_success_new_action_re.html">How to Evaluate Online Success – New action research with 24 institutions</a> »             </div> <!-- /breadcrumb -->
              
                      <div id="main-content-header" class="clear-block">
              <h1 id="page-title">How to Evaluate Online Success? A New Piece of Action Research </h1>                          </div>
          
          
          <div id="content"><div id="article-14861" class="article article-promoted paper-article clear-block">

    
    <div class="article-detail clear-block">
  <div class="node-edit-link" id="node-edit-link-14861"><ul class="links"><li class="0 first last active"><a href="how_to_evaluate_online_success_a_new_piece_of_.html" class="active">[View]</a></li>
</ul>
</div><div id="intro-paragraph">
<h2 class="Author">Jane Finnis, Culture24, United Kingdom; Seb Chan, Powerhouse Museum, Australia; and Rachel Clements, Central Saint Martins, United Kingdom</h2>
<h3 class="AbstractTitle">Abstract</h3>
<p class="AbstractText">How to measure and define the success of cultural websites and online services is a problem common to all parts of the cultural sector – museums, archives, galleries, arts organizations, libraries and publishers. Worse, funding agencies and government departments lack the expertise to offer guidelines or set standards. The result is a confusing mixture of statistics and reporting that reveals little about genuine user satisfaction.</p>
<p class="AbstractText">In mid-2010, a group of 24 organizations and agencies from across the cultural heritage and arts sector in the UK began collaborating on a piece of action research addressing the issues around &#39;evaluating online success&#39;. Coordinated by Culture24 and working with Seb Chan, the groups of institutions have been developing a framework and best practice guide for measuring online projects both on organizational websites and those presented through social media channels.</p>
<p class="keywords">Keywords: evaluation, metrics, evaluating cultural websites</p>
</div>
<p></p>
<div id="body-text">
<h2>1.    Introduction</h2>
<p>The project is led centrally by Jane Finnis, Director of Culture24, with strategic input and practical guidance on the use of Google Analytics from Seb Chan, Head of Digital, Social &amp; Emerging Technologies, Powerhouse Museum. Culture24 also brought in a social media strategist from the commercial online works – Jenni Lloyd from Nixon McInnes, a social consultancy company based in Brighton, UK. Rachel Clements, an MA student at Central St Martins studying Innovation Management, carried out the mapping of social media activities across all partners.</p>
<p>Seventeen individual venues are taking part, a mixture of national museums, galleries, performing arts venues and other heritage organizations: British Library, National Gallery, Science Museum, Birmingham Museums &amp; Art Gallery, Tate, British Museum, Kew, National Maritime Museum, Imperial War Museum, National Portrait Gallery, Watershed, Royal Shakespeare Company, National Museums Scotland, National Museum Wales, Design Museum, English Heritage and Roundhouse.</p>
<p>Culture24 have set up a collaborative funding arrangement with all of these partners, each demonstrating their commitment through a contribution of £1,000. Further matched funding has also been secured from a number of cultural sector agencies with vested interested in the research outcomes:</p>
<blockquote>
<p class="ListParagraph">Department of Culture Media and Sport (DCMS): contribution of £5,000</p>
<p class="ListParagraph">Arts Council of England (ACE): contribution of £4,000</p>
<p class="ListParagraph">Heritage Lottery Fund (HLF): contribution of £3,000</p>
</blockquote>
<p>Culture24 are also working with the University of Leicester on a new Scholarship in Digital Heritage. In 2010 the first PhD scholarship was offered to Elena Villaespesa to investigate museums&#39; online performance and approaches for measuring the value and impact of online digital culture.</p>
<p>Basecamp is being used to conduct the majority of the project online with logins for all partners.</p>
<p>The starting point for the research was to:</p>
<ul>
<li>Explore what common ground there may be between organizations in trying to evaluate online activity</li>
<li>Benchmark current activity across the sector in such a way that any future changes can be clearly identified and mapped</li>
<li>Consider what lessons can be learned that might help to shape some advice or recommendations for the wider sector</li>
<li>Highlight examples of good practice, begin to define what they mean and how they can inform good practice</li>
<li>Make recommendations for software and configurations standards</li>
<li>Help to define a better understanding of audience behavior online</li>
<li>Create, test and publish simple guidelines for organizations</li>
<li>Help organizations with the process of clearly defining their target audiences</li>
<li>Improve understanding of user segmentation</li>
</ul>
<h2>2.    Background to this proposal</h2>
<p>Between 2003 and 2008, Culture24 carried out three comparative surveys across UK cultural websites, and assisted in the coordination of two international comparative surveys of cultural portals (http://www.culturemondo.org/documents/surveys). These attempted to collect information about the quantity and behavior of online audiences of those organizations&#39; websites.</p>
<p>This body of work and evidence clearly highlighted the lack of clarity about what to collect, how to count it, and how to interpret what it is telling you. There was also clear inconsistency in the use of tools, with organizations counting and reporting different things using different systems.</p>
<p>In 2009, Culture24 called together a meeting of UK national &amp; regional museums, research &amp; educational organizations, plus strategic public sector agencies, to explore the possibility of a joint action research project. Those attending all responded to a call that was put out using the UK MCG (Museums Computer Group) e-mail list.</p>
<p>The resulting action research project has been directly informed by the outcome of that meeting and by the input and ideas of the group. Further input has come from a series of &#39;web metrics&#39; workshops that Culture24 held in the UK, New Zealand, Taiwan and Luxembourg, led by Seb Chan as part of the international work of the Culturemondo network.</p>
<h2>3.    Face-to-Face <span class="Heading2Char">meetings</span></h2>
<p>The partnering organizations met as a group three times over the course of the project. Each meeting took the shape of a facilitated workshop and each had a different focus.</p>
<ul>
<li>Brighton July 2010: Agree on scoping and priorities for project.</li>
<li>London October 2010: Defining user segmentation and social media priorities for benchmarking: This informed the research work at a more practical level.</li>
<li>London January 2011: Review social media benchmarking and Google Analytics work to date: This ensured a sanity check of the work to date and threw light on any key issues that needed further attention.</li>
</ul>
<p class="listwithbullets">These meetings have been key in providing a platform for shared learning and reflection. They also ensured a genuinely collaborative approach to the project, with each partner able to have a say and express any concerns.</p>
<p>A fourth and final group meeting is being planned for July 2011.</p>
<p><span class="inline inline-center"><a href="../../image/finnis_j_et_al_how_to_evaluate_online_success_figure_1.html"><img class="image image-preview " src="http://museumsandtheweb.com/files/images/fig_1_face-to-face_meeting_in_brighton.preview.jpg" border="0" alt="The first face-to-face meeting in Brighton, July 2010" title="The first face-to-face meeting in Brighton, July 2010" width="520" height="254"/></a><span class="caption"><strong>The first face-to-face meeting in Brighton, July 2010</strong></span></span></p>
<h2>4.    Quantitative site analysis</h2>
<p>In November 2010, each of the participating institutions provided access to its web analytics accounts. Each institution&#39;s installation and basic configuration was subjected to a &#39;health check&#39;, partially to ascertain the degree of compatibility between quantitative data and partially to assess the general Web analytics competence of each institution.</p>
<p>Most institutions use Google Analytics (GA) although this is not usually used for official reporting and it runs in parallel to older log-based systems. Two institutions used Urchin on their own servers instead of Google Analytics. The earliest GA installation dated back to 5 June 2006 whilst the youngest ran from 8 February 2010. On the whole, though, all but three institutions were able to have data collected and compared across all three desired analysis periods (Oct-Dec 2008, Oct-Dec 2009, Oct-Dec 2010).</p>
<p>Underscoring the difficulty of comparative analytics even with the same tools in use, configurations amongst the 14 institutions varied widely. Only one institution had set Google Analytics up in a way consistent with the tool&#39;s own documentation, and less than half had basic filtering in their accounts. Only a handful had successfully segmented out their internal traffic, and only one had set up a filter to remove case-sensitivity on URLs. Some had not integrated their e-mail and other online marketing activities effectively with Google Analytics, and only half had tracking of non-HTML pages such as PDFs.</p>
<p>Following the health check, data collection began using a series of custom segments that were shared with the group. It had been hoped that data collection could be done by each individual institution, but given the wide variance in setup and internal skill levels, the collection was done by Seb Chan externally and then shared on Basecamp for verification and discussion. Internal traffic was segmented out at the report level as a result of the lack of top level filtering.</p>
<p>The data which were collected for the fourth quarters of 2008, 2009, and 2010 were:</p>
<ul>
<li>Internal visits and visitors (collected to identify and exclude internal traffic)</li>
<li>Global visits and visitors</li>
<li>UK visits and visitors</li>
<li>Institutional home city (or major home city for multi-homed institutions like IWM, Tate, etc) visits and visitors</li>
<li>Organic search visits</li>
<li>Paid search visits</li>
<li>All social media referred visits</li>
<li>Facebook referred visits</li>
<li>Twitter referred visits</li>
<li>Flickr referred visits</li>
<li>Wikipedia referred visits</li>
</ul>
<p>Mobile data was collected for the fourth quarter of 2010 only and segmented the following:</p>
<ul>
<li>Global visits</li>
<li>UK visits</li>
<li>Institutional home city (or major home city for multi-homed institutions like IWM, Tate, etc) visits</li>
<li>iPhone/iPod visits</li>
<li>iPad visits</li>
<li>Android visits</li>
<li>Other mobile OS visits</li>
</ul>
<p>This base-level data allows for significant cross-institutional comparisons to be made, and the social media-related quantitative data can be correlated with the specific social media research undertaken in the project.</p>
<p>There are a number of inconsistencies in Google Analytics, especially when using custom segments and custom reports. The worst of these are variances in visit counts between report types – however, by using the same segmentation, the effect of these across institutions is at least consistent.</p>
<p>For counting Twitter, the project used referral traffic from Twitter.com, even though this is known to not contain traffic coming via Twitter apps. Despite this, the overall social media data suggests that even taking this into account the overall patterns and trends are reliable.</p>
<p>These figures could, in the future, be programmatically harvested using the Google Analytics API and a presentation/aggregator like Museumstats.org originally developed by the Walker Art Center.</p>
<h2>5.    Quantitative and qualitative social media approach</h2>
<p>Recent years have seen rapid growth in the usage of social media in cultural organizations, with the medium fast becoming an essential part of most audience communication strategies. The potential of social media for extending audience reach and for engaging audiences in new ways makes it an attractive tool for organizations to explore. However, the enthusiastic uptake of this medium has been accompanied by a new set of challenges for organizations, one of the most pressing of which is how to evaluate success and how to measure return of investment (ROI) in relation to social media, especially as realization of the resource cost of social media becomes apparent.</p>
<p>Initial discussion within the project group during mid- 2010 highlighted a need for cultural organizations to be proactive in setting up meaningful measurement frameworks for social media. Without such a framework, it is difficult to demonstrate the quality and value of social media interactions with cultural audiences. The project group also recognized the need for benchmarking data, in order to compare social media performance across organizations with similar audiences and/or missions.</p>
<h2>6.    Aims</h2>
<p>This part of the project aimed to collect cross-organizational benchmarking data which would provide the basis of a social media framework incorporating the following principles:</p>
<ul>
<li>Establish a baseline for comparison across organizations, creating equivalencies in the data to allow for cross-sector benchmarking.</li>
<li>Move beyond measuring numbers and begin to measure quality / value of engagement and meaning: not just usage but also interaction.</li>
<li>Recognize the importance of qualitative measures as well as quantitative.</li>
<li>Establish a focus on trends over time rather than snapshot numbers.</li>
</ul>
<h2>7.    Methodology</h2>
<p>Data collection took place between October and December of 2010. For each of the 17 partner organizations, data was collected about social media usage, strategy and investment in order to build a more holistic picture of each organization&#39;s approach to social media.</p>
<p>The data collection process was split out into two distinct stages:</p>
<p><strong>Stage 1</strong> involved desk-based research, supplemented by e-mail exchanges, Basecamp discussions and phone calls in order to review each organization&#39;s presence on the main social media channels.</p>
<p><strong>Stage 2</strong> involved each partner organization filling out a series of questionnaires via e-mail and Survey Monkey, plus further discussion via e-mail, phone and Basecamp.</p>
<h3>Stage 1</h3>
<p><em>Aim = Collect qualitative and quantitative data to determine which social media channels each of the partner organizations is using, how many people they are reaching through these channels, and the level of user engagement.</em></p>
<p>The decision was made to focus on the major social media channels only, and therefore data was collected from Twitter, Facebook and YouTube. These platforms were chosen based on an initial review which indicated that these are the most common platforms currently in use. Blogs and Flickr were omitted from the initial study since their function and setup varies significantly between institutions, adding an extra layer of complexity to any comparison.</p>
<p>The aim of this stage was to take a snapshot of the performance of all partners on that platform at a specific point in time (i.e. reviewing all partner organizations on the same day to ensure a fair comparison). All partner organizations provided access to their Twitter and Facebook accounts for a period of 2 months to facilitate data collection. The desk-based research was supplemented by e-mail exchanges, Basecamp discussions and phone calls.</p>
<p>For Twitter, additional statistics for influence and user engagement were gathered using online tools available from Klout and Tweetlevel. The problems associated with the use of automated tools to collect these statistics were readily acknowledged, and issues with the slow rates of data refresh on these platforms were also taken into account. However, it was felt that the inclusion of this data in cross-organizational benchmarking would provide for a richer case study when the results from these tools were compared with the qualitative data.</p>
<p>The full list of data collected was as follows:</p>
<h4>General information</h4>
<ul>
<li>List of social media channels that each organization uses</li>
<li>Strategy for linking to social media from the main website</li>
<li>Size of the organization (based on number of physical visits)</li>
</ul>
<h4>Twitter</h4>
<ul>
<li>Number of followers</li>
<li>Number of mentions</li>
<li>Tweet profile for the organization</li>
<li>Klout stats</li>
<li>Tweetlevel stats</li>
</ul>
<h4>Facebook</h4>
<ul>
<li>Number of fans</li>
<li>Growth rate in fans</li>
<li>Fan demographics</li>
<li>Amount of organizational content</li>
<li>Amount of user-generated content</li>
</ul>
<h4>YouTube</h4>
<ul>
<li>Amount of content</li>
<li>Number of viewers and subscribers</li>
<li>Amount of user-generated content</li>
</ul>
<p>For each channel, qualitative information was also gathered on the tone of content (positive or negative) and the general &#34;feel&#34; of the site.</p>
<h3>Stage 2</h3>
<p><em>Aim = Collect data from each of the partner organizations on a) their social media strategies, and b) the resources they invest in social media.</em></p>
<p>Each of the partner organizations was asked to respond by e-mail to a set of questions about its social media strategy, and also to provide copies of relevant documentation. Additionally, an online survey was administered via SurveyMonkey to collect information about the resources each invest in social media. Additional discussion took place via e-mail, phone and Basecamp.</p>
<p>The questionnaires sought to explore the following themes:</p>
<h4>Strategy</h4>
<ul>
<li>Percentage of organizations that have a social media strategy</li>
<li>Scope of strategies</li>
<li>Success measures in relation to strategy</li>
<li>Impact and effect of strategy</li>
</ul>
<h4>Investment</h4>
<ul>
<li>Staff investment in maintaining, updating and creating content</li>
<li>Staff investment in monitoring and reviewing performance</li>
<li>Direct costs of social medias</li>
<li>Software tools used</li>
<li>Concepts of value in relation to social media</li>
</ul>
<h2>8.    Feedback and analysis</h2>
<p>Once data collection was complete, individual results were made available to each partner organization for checking and feedback, and the results were discussed via e-mail and Basecamp. The aim of this stage was to capture any additional social media activity that may have been missed from the review and also to account for any other circumstances that may have influenced an organization&#39;s figures during the data collection period. Once this initial feedback period was complete, the full dataset was then shared with all partner organizations on Basecamp, for further review and discussion.</p>
<p>The group then met in January, 2011, in London to review and discuss the results of the research. A number of issues and questions for further investigation emerged at this stage:</p>
<h4>Investment in social media</h4>
<ul>
<li>Do those organizations that invest more in social media have more success with their social media channels?</li>
</ul>
<h4>Social media strategy and frameworks</h4>
<ul>
<li>The group recognized a need to provide guidelines to aid cultural organizations in creating effective social media strategies.</li>
<li>The group felt that frameworks for monitoring social media performance in cultural organizations could be developed and trialed through the adaptation of existing frameworks from the business world.</li>
</ul>
<h4>Benchmarking across the main social media channels</h4>
<ul>
<li>Which quantitative statistics are the most useful for benchmarking social media between different cultural organizations?</li>
<li>How can we measure engagement the most effectively?</li>
</ul>
<h4>Qualitative data</h4>
<ul>
<li>What are the uses and pitfalls of sentiment analysis for cultural organizations?</li>
<li>How do different organizations&#39; account styles vary across Facebook and Twitter?</li>
</ul>
<h4>Social Media referrals to websites</h4>
<ul>
<li>What factors drive social media referrals most effectively?</li>
<li>Is there a difference in the demographics and behavior of socially engaged segments of web traffic compared to other segments?</li>
</ul>
<p>Analysis of the results is still ongoing.</p>
<p>However, one of the early outcomes of this project has been the development by James Morley, Website Manager at Kew Gardens, of an automated online tool that harvests publicly available metrics from organizations&#39; Facebook pages, YouTube channels and Twitter profiles and captures them into a table for comparison. The tool is still in its beta version, but the plan is to refine the metrics that it tracks to match the research recommendations for cross-organizational comparison, and to add in an archiving facility to allow historic data to be analyzed and trends to be studied.</p>
<h2>9.    Timescale</h2>
<ul>
<li>April 2011: Museum &amp; the Web workshop – International Peer Review</li>
<li>July 2011: Final face-to-face meeting of all project partners in London</li>
<li>July 2011: Arts Marketing Association keynote on the project: Early outcomes and recommendations</li>
<li>Autumn 2011: Culture24 conference &#34;So What and Who Cares?&#34;: Publication of final report and recommendations.</li>
</ul>
<h2>10.         Conclusion</h2>
<p>The tangible end result of the project will be a published report with a series of recommendations to funding agencies and governments and with a set of guidelines and key issues for sector organizations.</p>
<p>Specifically, these will include shared guidelines on:</p>
<ul>
<li>How to carry out a &#39;Health checks&#39; on your one website and your Google Analytic set up.</li>
<li>Ideas and advice about useful audience segmentation, supported by &#39;shared segments&#39; available for download and use in Google Analytics</li>
<li>The need and arguments for a joined up strategy – not just &#39;digital&#39; or &#39;social media&#39; strategies: Framework for headings and topics.</li>
<li>A working framework for social media evaluation: ways to measure both engagement and popularity.</li>
</ul>
<p>Interest in the project among cultural venues in the UK has been high, and Culture24 intend to look at how the learning might be rolled out and implemented by other groups of venues who are willing to buy into the collaborative funding model.</p>
<p>Culture24 is also in discussion with Seb Chan about the possibility of continuing a Phase 2 project with the same group of venues in order to take the thinking further and consider in more detail the impact of mobile devices and platforms on our understanding of online success.</p>
<!-- InstanceEndEditable --></div>
<div id="citation">
<h4>Cite as:</h4>
<p class="references">Finnis, J. et al., How to Evaluate Online Success? A New Piece of Action Research. In J. Trant and D. Bearman (eds). <em>Museums and the Web 2011: Proceedings</em>. Toronto: Archives &amp; Museum Informatics. Published March 31, 2011. Consulted
<script type="text/javascript">// <![CDATA[
      // current date - based on http://rainbow.arch.scriptmania.com/scripts
      // Array of day names
      var dayNames = new Array("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday");

      var monthNames = new Array("January","February","March","April","May","June","July",
                           "August","September","October","November","December");

      var dt = new Date();
      var y  = dt.getYear();

      // Y2K compliant
      if (y < 1000) y +=1900;

      document.write(monthNames[dt.getMonth()] + " " + dt.getDate() + ", " + y + ". ");
                       // 
// ]]></script>
http://conference.archimuse.com/mw2011/papers/<!-- #BeginEditable "URL" -->how_to_evaluate_online_success<!-- #EndEditable --></p>
</div>
<div id="copyright"><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img class="c1" src="../../files/images/cc-88x31.png" border="0" alt="Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 License" width="88" height="31"/></a>
<p>Copyright © 2011 – Archives &amp; Museum Informatics.</p>
</div><div class="field nodereference field-paper-program-ref">
                                <h3 class="label inline">Program Item Reference: </h3>
          <div class="item inline odd"><a href="../programs/how_to_evaluate_online_success_new_action_re.html">How to Evaluate Online Success – New action research with 24 institutions</a></div>
              </div>
 <!-- /content-field -->
  </div>
  
  <div class="article-meta">
      <p class="submitted">Posted <em class="time">March 29, 2011 - 7:24pm</em> by <em class="author"><a href="../../user/janefinnis.html" class="username" title="View Jane&#39;s profile.">Jane Finnis</a></em></p>
  
    
  <ul class="taxonomy"><li><a href="../../conferences/mw/mw2011.html" rel="tag" title="Museums and the Web 2011">MW2011</a></li><li><a href="../../paper_keywords/evaluating_cultural_websites.html" rel="tag" title="">evaluating cultural websites</a></li><li><a href="../../paper_keywords/evaluation_0.html" rel="tag" title="">evaluation</a></li><li><a href="../../paper_tags/metrics.html" rel="tag" title="">metrics</a></li></ul>  </div>
  
  
</div> <!-- /article --></div>

        </div>